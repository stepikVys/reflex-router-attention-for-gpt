{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zMuv4OWB4x"
      },
      "source": [
        "#**Language Model: Reflex and RouterReflexAttention for long term memory for sequences**\n",
        "S. Vysotsky, Central University\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCcaaCx9WAZU"
      },
      "source": [
        "*abstract:*\n",
        "___\n",
        "The problem of representation collapse and over-squashing in the later layers (as discussed in the paper “Transformers need glasses! Information over-squashing in language tasks”, 1) negatively impacts the accuracy of large language models when solving tasks that require considering long-term context.\n",
        "\n",
        "Decoder layers in language models are trained to predict the next tokens. However, the model loses direct access to early representations (residual) when performing self-attention.\n",
        "\n",
        "The idea of providing the model with a high degree of freedom in choosing the decisive features for prediction is one of the key ideas in improving its quality in deep learning. After all, why should we decide for the model which features it should focus on at any given moment during the forward pass, if it can do it itself without our help? Neural networks and backpropagation allow such representations to be learned. Routers and weight combinations are capable of describing the importance of each representation to the model (weighted sum). This motivation leads to the intuition of adding weighted combinations of hidden states from all layers of the decoder-transformer when building the attention matrix at each new layer. We will call this technique Reflex Router Attention ($\\sim$ CA vs SA). This learnable router gives the model the freedom to choose which hidden state to reuse. The motivation for this idea is borrowed from the paper “Hyper-Connections” (2). Hyper-connections provide freedom and flexibility for the model both between layers and within a single layer. We use this motivation to address the problems of collapse and over-squashing of late-stage hidden representations in the final layers (by adding knowledge from earlier layers).\n",
        "\n",
        "\n",
        "notation:\n",
        "\n",
        "Self-Attention SA\n",
        "\n",
        "Cross-Attention CA\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l22bsYzGDy4k"
      },
      "source": [
        "*reference:*\n",
        "\n",
        "*(1)  Transformers need glasses! |\n",
        "Information over-squashing in language tasks, 2024*\n",
        "https://arxiv.org/pdf/2406.04267\n",
        "\n",
        "*(2) Hyper-Connections, 2024*\n",
        "https://arxiv.org/pdf/2409.19606\n",
        "\n",
        "*code and data from A.Karpathy repo on GitHub*:\n",
        "https://github.com/karpathy/nanoGPT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ZGTfK1R-UUVx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-Udp_RFaWdI"
      },
      "source": [
        "*work:*\n",
        "\n",
        "*Continuing the work, the main experiments are presented here below.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train loss:"
      ],
      "metadata": {
        "id": "vXTg5rwWM4hF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Снимок экрана от 2024-11-26 23-38-28.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "cPjz8jK0M3Gc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "val loss"
      ],
      "metadata": {
        "id": "vCaJ42ofM_Nv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Снимок экрана от 2024-11-26 23-38-52.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "KR3gmjXCNDYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“By decreasing loss”\n",
        "\n",
        "3.Reflex Router Attention on s_d_p CA for all hidden states (first 2 layer SA)\n",
        "\n",
        "2.Redlex Router Attention on all hidden states (linear combinations for future s_d_p) (first 2 layer SA)\n",
        "\n",
        "1.Reflex Attention (const) 6head=cat(CA(prev1), CA(prev2), SA) (first 2 layer SA) and also simple Transformer"
      ],
      "metadata": {
        "id": "LPcT2AB0cQQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "5P7W0UWwNOKN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_35sNJDXlQIf"
      },
      "source": [
        "![Снимок экрана от 2024-11-26 16-02-27.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zFb2uiok8o5"
      },
      "source": [
        "![Снимок экрана от 2024-11-26 16-05-29.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эти графики лосса описывают эксперимент по сравнению трех концепций моделей: Simple Transformer(6head, 6layers, all SA) and Reflex Attention(в двух конфигурациях). Качество стандартного SA показывает результат лучше, однако конфигурация похожая на стандартный SA (добавить две головы CA на два последних скрытых состояния) дает аналогичный результат. При росте модели модель способна учитывать зависимости с прошлых слоев лучше, в том числе учитывать прошлые скрытые состояния и предсказания в них (длинный контекст).\n"
      ],
      "metadata": {
        "id": "8NCEFNiyQ3gL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "eD09ug87VR7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABv4AAAJbCAYAAAAsbeNmAAAABHNCSVQICAgIfAhkiAAAABl0RVh0U29mdHdhcmUAZ25vbWUtc2NyZWVuc2hvdO8Dvz4AAAAuaVRYdENyZWF0aW9uIFRpbWUAAAAAANCh0YAgMjcg0L3QvtGPIDIwMjQgMDA6NDM6NTY/FEL/AAAgAElEQVR4nOzdfVwU1f7A8c/uyiLISoEkqwZqiA+gCZqkN6JELdTSLIzCNEx/mGZmPj9kaplamqn5QJoWqXG18CEfKi/djEqxFFIgQ7oKVwVTVrmLArst/v4AdMEFlgdB8Pt+vSydnZ05e+bMmXPOd+aM4tq1a9cQQgghhBBCCCGEEEIIIYQQQtRryrpOgBBCCCGEEEIIIYQQQgghhBCi+iTwJ4QQQgghhBBCCCGEEEIIIUQDIIE/IYQQQgghhBBCCCGEEEIIIRoACfwJIYQQQgghhBBCCCGEEEII0QBI4E8IIYQQQgghhBBCCCGEEEKIBkACf0IIIYQQQgghhBBCCCGEEEI0AI2OHz9e12kQQgghhBBCCCGEEEIIIYQQQlSTIj8//1pdJ0IIUX/98ccftG/fvq6TIYQQQgghRK0wGo11nYQ7go2NTV0nQQghhBBCiHpJpvoUQgghhBBCCCGEEEIIIYQQogGQwJ8QolquXZOHhoUQQgghhBBCCCGEEEKI24EE/oQQ1SKBPyGEEEIIIYQQQgghhBDi9iCBPyGEEEIIIYQQQgghhBBCCCEaAAn8CSGEEEIIIYQQQgghhBBCCNEASOBPCFEtMtWnEEIIIYQQQgghhBBCCHF7kMCfEEIIIYQQQgghhBBCCCGEEA2ABP6EEEIIIYQQQgghhBBCCCGEaAAk8CeEEEIIIYQQQgghhBBCCCFEAyCBPyGEEEIIIYQQQgghhBBCCCEagEZV/eK1a9eqtWOFQlGt7wshhBBCCCGEEEIIIYQQQgghbqh04O/atWvX/5gvs4Z5sE+hUFz/I4QQQgghhBBCCCGEEEIIIYSonkoF/ooDftnZ2WRlZWEymaq0U5VKhbOzM46OjoA8/SeEEEIIIYQQQgghhBBCCCFEdVkd+CsO+hUUFJCVlUWbNm2ws7Or0k5zc3M5deoUGo0GpbLwNYMS/BNCCCGEEEIIIYQQQgghhBCi6pSVWbk48Gcymaoc9AOws7PDZDJRUFBQ7XcFCiGEEEIIIYQQQgghhBBCCCEqGfgDKCgoqLGd1+S27kxXOZOUwkVDXaejPjNy8Y8kzlyt63QIIYQQQgghhBBCCCGEEEJUT6UDfw1W9j4m9OnD0A/iKRFHM6WybkR/JuzU1VXKymQ6n8LvOrBXF/5b/8f37Ny5s+SfuDNU+k2MuWkc+mYv35/IrukkW3btPEf27eanNGPt7P/vNA7t3s3hcybABnuVjt9PnMd4a/Z2Rwh8rD/9R0xh2e5U9HWdmAYkceUw/B/tc+NP/6G88PpSopNzamX/KRtH0n/oQg7UUlVQd3JIjF7Ky8MGE9inP/2HTWD+5jgyqvYaW2GRgf1z+tN/YVyt7jVj2wQCh0WQeMccS2vyOZ3o1wfTf/o+MmotXQ2NNflc82U+Y9sEAkdsJOWOKc+W6Iie0J+hK5PrOiGiilLWjyRw9GbSrFz/zqvHa1/Gl6/zeNh6kiSPi9RNm+XOI/l82zPFsXhwf6buq52+Z92RtnHtkHP+9pbDwfVTGPpEH/yfGMO6P+o6PXeSatZBhjjmD1vIgVzgj4288NgEos7WcBLrq/LyIzmCF6bvJIsb/Y2GqNJTfda0am/TaKTyka2yZXz7CVHpNbe9WyePM6fO49i6DfZFS4wGI6pmHXmkd296F/3pe78WVWU3baOhmUszXBxtazjNt+f+7d3b4HgxlTO5tbK7Bmn5iiW8EezB6aiV7Lml508mUa/0Z+q3d85jrmqvMDZ9tYO9X+1gx9oZhLgmsWraQvZcqNn9pESMZPC7JRvhzh5+POjXFW3VZ3auFzKi5zF1YzqeoW+yZu0K3h7eA0PMStbFNvRObv1jqZyKynKidTc/Arq641DXSRFCCCGEEKJOWWgbxy1l8B1/s5W4o6TvZH1UGp5jN7B13TuEeNR1guqBGqsnqtc/N8T/SGK7Hvg28HG7mpbyczxOPfxwruuE3GKNKvuFmgz+WbutK//9lZ9+Pcmp/57jL91lLl/O5rL+Ckbzk0utxt7WniZ33cU9Li1oea87rdu2w7uTO3fZWJkgtRd9/bKIWh/DgPmBt/fBz83gzKW7ce9248cZjUaUdo44ajTV23YjJzx8e1Qzgbfx/kuHu2203Ht3EqnnrtLmPnuLXxHl827fCdp3omcQVD7SXAmmHLLutEcKVQ44ODigAXDwYcD4MA5+v5CD8QYG9FPX2G6ysnWUDqc6+4czx7/GdnGbyuTggSTUgQuYGORTuMjDA9/A0FtblkWVWCqnorIc8A2dgW9dJ0MIIYQQQog6d3Pb2JCtQ49TnaVIiFqXk4Ne5c6A7m5oXeo6MfVDzdUT1eufJx5OonWPcKoZCbjDpHPoV1t6vuFa1wm55Sod+Kttl39ewfSPU7nbuztd296PZ/e7uMvZhSs/r2FTXDZGowEaN6FJY3ta9nyGge3g0oULnD2Xyk+f72KjTR/mz32SllYN4ObTekAoOfM2Epngz8SulgfV9akxREXtYv+vJ8nIVdPaZxATZ4Th61j4eca2CQxL6M3ygHTWRcaQeAFa9Arjjdd8SNu8kshvkziHKwGj5jJ9oBvFezGkx7Bu9Wb2xKeT7+BBz0HhTBzuYzEAabxwnuymzWl2Pe5nwvh3ATb2FUc58079xL9ONKZb325oi0tAwXkS9v9CXofHeNBNx5GvfyGv02P8w90GyOP8iQSSTl1Eb1Ji79icNp264OFsA4ZT/PTt7zTu3p9uxefL5SS++/EC2ocfoWNToOAq5/+TQmr6eS7l5IGtI1pPH7q2cbQ8rn7tfIn9m84dZt8vGSUf7GykpUdQD7RKMP0vjaTjKZy5dJUCGw3N3L3o2qE5jYvXNehI/e0Yf/6VjVGp4Z42zTACN54nVHF3Mwf05y9gvM8da+PEwgKzA1p4HjzO8oeSWbU+hpS249n6XhDOOalER6wk6vuTZJkc8PQPZdz4QXg7ACYdR3duJmp3LEfO6sDRg35hM5kY5IY6cxsvF0/1tLA//gvBO3wTa0JcobxtAuQkg0OnOsiQW8kAqElcPYwJyYPY9GEw2qLl++cNZpFqBjGz/Qv/PWcw0e0XMNy0i/W74zmdo6Z1YDhvvxaIllTWjRxDZDrALPz3gbrXJHYsCCJn2wSG7ezE8k/D8VZR+Jj8tEyGv9WJoxHbOHBSB26BTJwbjnt8BMuiYlmzaUfdZUmVOGBrB/rUZNIMPrgXV8jXy7KOPdOHsc7lTbZO8iusrxNWMnhyDL7ztzKnV+EXUtaP5OU/ggvLOJDx80ZWbdzHwVM52Lb0YcCo8Yzzv9GoKO/zjG0TGHawK693zWT3t/Gk6Qw4eAUx7rVwAlrWUrbcKqZM9q+fReTueM4ZnPAMDGX6+KCifDeQEbuNyO0xHDyRTpbKFd9+4Uwf619uOdWQQ+LOCNZtiyUx04CmjR9DwsczvHtRI1wF+vjNzF69k4Nnc3D2GsTE2eH0bMh9+Qryef+cwSyze5O9M/yAits1hvQYVn2wkf3HMsl38qBnv1DGDfdHW3P3HtRP5eazZSXOfVcvAoLDGTvIw6yjVkF5NmP4YyMvT47Bc/Zapvk11Oc3dRyNXMmqnXGcznXAM3AQniUahEXXuE4LCNFtYdXuJDSha/k41K1ku9rOjW79Qhk3KvD68bG2ri3/mGUSNWYk0T4r2BpeX2+NvtFOCMn9gvVF5dk7ZBJz+hmIXh3Bnl8zMTj5MHzmDEI63ShrFZdnyPg+gmUb93EkE5y7BDLwpqKcw9GolayKjiUlW43WK5AR48MZ0KahVTAGYuYFs8NzPsNMX7Fh32+k5ahxf3QUc8c/itbqm41yiN+6mrU7f+Jkthptx0cJHTeK/q3VJfYxNO9LNuz9jQyDE17BrzGrj4EdEevZezQTg9P9DJs6laEdzeoNlYG0b1azYdu/SboAzt6PMea1UfjXu/GQCtq9lbmpS5VDYvRClkXFcjrbgdb9Sm2jqn2a4u3nJBO1cgVRsens2Lu3RnPh1qvBfDanS2ZP1DaiY+M5fcGAxs2PkMkzCOmkRv/tPJ5drWbaphkEFBddUzzLnptH1qhNvN3PoYJjEgcqvxr6/bXjwMLBzM8JL2rvFkndyAvjEhjw6XJCXKnwWgeQ9etmVn2yj0MnM8l39CIgJJyJQzqhsaacAujiiZy3mei4dPQObgQEv8rE4E4NaJC5ZNv44LuDi6Y3jeOlPpvBIYh3d0yip8qKPlyp8Y+Y94Lq9qfd5iTPboUqtOsSVjJ02k4yDLBsaB+W+Yxnx/uDyKhwrOnOVWY9kbqRF2bqGD7Xg4MrNnIg0485X07CM66McQ4V3Nw/r8Q11pTMwXhnHgwppy+Yk0p0RARR3yex9av61d6o8DqoLrvdUK6zcRykB9PLHF8zcPDdYUzdZ+G1b2ovxm1YTkg9GZu77d/xdzntHPgOZ/bE4Tw7qA+P9uqOT3sX0F3gsv4KV/KMXLl8mb8yz3Eq2x6v+7vzUJ8gnh0+hmkvP8xd509ztsDKnZkM5N8VyKhBanav31nGuycMpP18GF2bJ5m+ZANbV43HO2Mb89eXfDegIW4jy5I78fq6HexbG4pT3EpeG72Uo23CWb1jBxuGazm0OoL9xWVIF8uiyRtJ6xzOms93sHVxMJqYeSzenWkxFfpsPTYazY3gFgUYjWDMTGD/3t3s3ref739J4WLezd9trG3F3QXnOXfhxsiJ6UIG503N0GpvDnuZMpM48qcR7QN96P/YI/i0skdVmaKTd4GMLCUtOnQjILA3D7jbcD7pGKeuWPd1lbYbjwX1p39Qfx7r3RVtYxWO97XnHiWQe4Yjh06gb9YF/8D+9PG7D5szRzjyn6vFOUXq4Z85kW3Pfd388ffzwunqRfSlHsVu7KjBRp+NvuZns72zJW1k8QFXxq3bwb4FQTiTSfS8mWzP7c30tZvYsWEufQ3bmP1BbOG7AXXxHDiu5sFRc9nw6QbeG6ThwAdLic4EXINZ89Wb9LVT03PqDmK++Vdh0K+ibZJJ9JyZdZcHNSk7nf0Rmzmo8qGnT+UGeRM3L2WPQzAffL6DHUuC4NuVrPvZACoPRm/YyvTuajRBC4j5Zi8x84PK7sjlxLBqo46HZ29g75dLGMA+Fo0bw7JTPZizrr4F/QAc6BsaTOv/bGTkyFms2h1PRolHypzw7doO/bFkThfVG2nxSeCkJvFIatE6OhKTM2nd1QtnQP/rSqYuTcZ9+Dts/XITa0a5cXTpXCJPFa5d0ecAhmM72Z37EHM2bGXvlysYYRfD/LmbSavn083oY7exn95MW7GBNbODUMcuZUpE0fXTlMnBH9Nx6hvOu2s3sWmGP4Z9S1kVk1NuOU3ZPIsJESdpEfImazasZc4gtxJTgBsyd7JsSw4BE5ewZnEYrdO2sSyqYb8frNx8vklF7ZpMopcu5aBLGB/v2MvWxaF42wENbUy+CiqXz4Xn/oR5+zA8Mp41G9bybqgbKRGTmR19o61XUXm+LjuWZXN34hD6DhMbbNDPQMrGmUyJysB71ALWrHqHkJbpHP3PzTmctm0p0apnWP7lDtaEuIEuhvkTl3LIYRBvrNrAmtmDcIhbyoSFMWSZ76GCutaaY9ZQJG5eyQGXMFZ/uZdN49txMnIeL73xNZqQFez4ahMTO5xk3Yob/SNr8kb/60omvL2PfP/xLF+7gjmPO5B4LLPEOZISOZn53zsQMncDe7euZXrXdNa9EUFiA328OynqA/Y6PM3SyG18segx+NdqNhyy/see3DSdBT84MHR2BLs2r2Ty/elsmLeeJLNNJEWt5odmI1gZtZONY+/j5Oa3+b/53+AwdClfRG9kguefbFj1VYm+riH9G7YdvZvBM1ay8f2x+Od+w4L5n9fcD69lZbZ7K0H/fQSrTnVi3DtrWT7eB/0+821Uo09T/P05M1l3wp3hb66osd9d22oin83pT8Ry0NCJ4TNXsOnTJQx3T2LV4s2kmEDj/zgBqjj2/2w2Ff/xHzmY60NALwcq7hPWPz0D/NHE/8gPZu9bT4uN5VyH3gS4YtW1zpAQwWuztpHROYx3125gzWsPoTUV3QRdYTkFMHAw+guyuoWzfN1a3gvWkhgxl2XfN9xXIvSctJVNYR6oWwazZu9eYouCftb04UqPfwgrSJ7dEpVq13UO5+P5g9CqvRi9bgd7FwTd3jPg3QbKqicAyI5l1aoEvCdvYt+XkwhQlTPOUQ6rrrGpcRy96yF6lvmUZibR8yaz6oQbI95aW92fXesqug6W124oT8bPhzF098O9zDXU9AyfQYhH6UEPJ3qOncSQehL0g3oQ+GsdEMA9v33Opt+umi21xy+oDy0bmy3SdGJgP7Mnekzn2L0phis+AXhZ/QiXAYNJjWdwGAEZ21hn8SRU4z18BtNCA/H1cEXrEcjwIDf0xxKuDwoDqF2CmDg+EHc7ULcJJKCdGro/z7QgDzSocQ98iHamkyT+p3D9tJ2bOeQVxhuhfrg7OeDsEcjY4HYcORBXYoCi6MeRn2fEprH5tJRKmrl74tGhG48EDeSxf3TkLn0Kh46c4mrprzduQSvnAs5nXCwaSzJxKeM8JudWtLAwkFdgMFKgtMG+SWNsbOxp1qYjbZwrcRufvTtd/brQpkUzNE00NPdsSzNlNhcvGq37vkKFjdoGGzVkp6bwl+19+HgWPi2YfTqFi45e9GjfHI2dDY3vcserrSOXzp0nD+DCn/yps6eNbzc8XJ1wdGqOR5f7cCqd/Mb2NDbkcVUCfzXKYGhHyKRQvJ3UqNVAwjaizvgzceogfFs6oXHtxJCwIJzjvuOoAXAJZOKb4Qzp1Ql3Vzd8g5/hQfVJjh4rOhfVRQVUpb7+1wq3mbCNyGP199Eew7GVDH2sP4GP9cF/8EiWxWsZ8dYkBlRy+gVNr3DmDOmERgWaToEEuBlIO1XUq1Oprz/hplary5/iUuXBkNfC6OmqBodO9PVzw+AYyMSx/vX2XYDqzmGsWfsmIzpksf+DKQwdOob52+Kv173a7l1pkZlA4gUAHUeOGwgY5A9J8YUN5Zx4jp50wtfHDdCxf0sMTqEzGO3vgbOjE+7+YQzvnsmB2HQrPi9Kk2sQ40YVPVFl58aAsEG0To/h21TqNc0j4bw9KhBvN1c8e4UyPcSLjJjvOGIAVG4MmTGD0UF+eLq54t4rlIFdDBxNSCr8sqVymhNLZNRJPMPmMm2gD55ubvgODGO4341zXu0YyMQF4fTt7IZn12BCejmRlXrSwrW14Sg3n29SQbvGlIP+Mmhc3HF2UOPcxp+QUP+iuz7vbJXLZx37t+wjx388b4T64+nmhnfQeOaEuHJ08+bC65UV5RkAUybRC1dypMsk5oS4NdwYbE4ckdHpeIbNZWKQD55tPAgICeepdjf/Yr1LEK+P8kNrp0atgpSdmzmgDmLa1EH09HDDs/sgpk8OQh27mWizerT8utaKY9aAaHqFMX2QBxoVaB95CF+VAc/gojvBVU4EBPjAqVRSDGBd3ujY/8995PQaz5xRgXi3ccM7MIxRga5mTzzFEhltYMDk8fTt5IrG0RXf0DACTHHsb6D3Zzg8OIpZgzvioAKHjo/g72Yg/ZSVgeScn9i000j/iWMJ7OiKg6MrPs+N4GHTL8T8br6P4Ux54j4cVKB9+B/4qAy0G1L0hJ/KCf+H74fTf5JqVobVrk8wecZz+Hu6ovV8lDFjHsP51Pc1+ttrU7ntXiupu4fz7qRB+Hq44R0UTF/zbVS3T1PUPwmZOYMhfvX1aeGayefS23v7tWACOruhbdmJIcGBaDMTOHoBsPNjQKADB/fHXm/DJcbGkePXmwcdqPiY1EPq7g/R0y6eA3HF41LpHPg5E++AwnZYxdc6Hbsjd5LlN563wwvrYc9ewYwO7lRYD1dUTgtTQc+wBUwc6IN7Szd8gycxwi+HA/vj6m1AtULFfQ6V2qxvbF0f7qbxD1EhybNbo1LtOpUajUYNqHFwdEBjJweiQhbriWJqfEMnMaS9Q9FnFYxzlMGaa2xaXBzqHn5l980TthEZ78qImeMZ0NWtGj+4blR0HSy33VAmHQcPZ+Hbq4L2l6MP45YsuBH8UznR87UlvD2ofvW/b/upPmkRxCuhycz5aA3fTn+VfvcWRvFsOj3Hove6E/dLKpdVLnj7daf19UdULhO/eQXbrz7MtIn306Sy+3T0Z3ToF7wUuZnEXkHY3jQInkNa7D52HzjM0ZMZ6LN1GFRelAgTljrpbe1ArTIrGnZqNBjINxRuL/GPVPS/LmVw/6Uld9W2Bzlw090WJlMBKpV53FaFppXnjad0mraic6fznDt8hnNX2+BR4tV1NrRo2Yyk5DNcLGhOc8Ulzv1lolnH5hanubRp1ZEOGYc5/u/9nNG6496mDa2sfnFiUXqvXiQ97QznL+rQ5xnJ/9uE498mqMTEmsbzSRw7Z8N9//DEUQlgRKfTY8xK4Js9CSVXdnDBCBh1l8mzdaL53RUEKhupUFKAqYB6EA6vP9RObribjVemJSWRcSGdKYP3lVrTh5xcQA2GC8ns37ePg0dOkpalIyvXQLvcsntrFW0z40QqeiefmvpJtU7dPpTlbwThfGEfs6ftwz18EsO7VuHpDpXa7OKkRm0LhnLytWy2qM1OJ9uiQdb6Tu3mz/DZ/oSEJ7N/cwSrImaRlruCj4d7QJuu+Dpu42hSDkMc4jl6ph19Z3cla9sujlwIxf1sMolqHwZ4AKaTJJ7I4WjSMALXm+3ABM6DcsCUUf7nZWnpipZM0jIM0L4+NTNKU5doJGnbuKHJzSQjG3ABcjM5+O0+fjicQGJaFnqdgXw/Q9HEthakJ5NicGOgXzlzkakd0Jh92cFBDQZDA39XYAX5fJNy2jUqD4aMHcTRha8yNMGfAY8/zsB+PjLNJ1CpfDYlkXgCvMf6lHii2r27D9rN8SRmgm+OFeWZHI5unMu6OOi3qIG/jLz4/O5e8VyDGncP3K9fi3I4+Ucmmi5d8TQ7QOr2PfC220fiiRzwKOM6al7XelhxzOpf/7ls5u0EtUPhtd3s+q62U6MhB4MBUFmRNy1PkngC2o2y/NoCoPAY56Rz8NX+RJkvN6kJuOnOxQaidHvMBgx5Vt4M+d/fSc1JJ+71QWw1X25S459bxj7KOJYO5BT2Q2+a4rxI6/vwUH1lXbpuRzXQ7rW1c7hRvlUOaOxvbKO6fZri/km3+hvzK1Rj/Ysb9H/EsP3bHzlyLI2MHB1ZJif0RU1k74FBtB7zNfszgwhxSWb/zwYeHOuHBiuOSX18OF7tQ19/B2b/GIe+XyCa9FgOnPVigL8TVl3r2hTVw2PLbi9Y1fcuUT840K6NK/ycjs4EmgbQD7RKRX28IqXHP0TFJM9ukcq066RfV7NUWjzblrroVHacA6y4xmZy4DD0nFR2XyktKQm9i0/97bOUex0sVF67wSJdHIf+8iGkvRX7Lwr+MSeCjL4zmTOwfgX9oD4E/oB7Al5l+tUVLFownzPDXuaFh1pgA9g4teOhx9qVXFmXzPaNH7Nb35PXpz+Dp73FTVZIOzCcATtnsu7bHnQrsQ0d++e9yrIzPowaNZ4Pprph2DeFoZFV/HHXqdEEvcneSdbNO69SKjGZyp/DVGVnT2NlNkYLfUkb11Y0O55ExgUTzRud47ypGR1dywjCKR3x8OtLK90Z0k+fIunHVE55/oNenmW8o68UU9bvxMalY+PegY6+XjjZZ5Pw7SH0WDsHK2A8T9KxM6ha98LzruK9mgAVNq0e4DHf5hbTogdQWBHJ+9tEAUpUEvS75dQtg1mzIQxPCwfMkLyRCTNj0AwKY/iM8Xi7JLHsuVklnqat7DbTTPnlP8F2u7N1QtvSFeeWwYwK3MfsjdtI8bP8Wysrv/qbaHDULp0Y8Np7aBnJhG9jSAn1wFPlhW8XNcuOn0TvkEBi265MdPIiq91Sjh7LodvZJAxewXirAVNh0y1g0g7e7mehSWCKLf/zsuQa0APOqvrWzCifoSi/bNVAdhzLXl3I0bahjAp7k4keThyc15/55YXoTA07fFdTSuTzTSpu1zj7hbPm80EcjdlJ9LZ5DNvmz5xlkwiQTnoJ5edzkfLqbivKs+HsPtbF+BEyCKIiNjLAJ7yw7mnIqny9KxmYtWo7lura+tyGuNXKzRtDUVO9nAJqAlRejN5Q+K6qO5XVVzIToOrIyIj3GXqr86sB3iBTE+1e821Up09jKOqfNMTquzr5nBY9hQmbDfQd9SLTw3zQXtjMS2NibqzgFsQQr21Ex6QzxCeWgwY/JvqZDYuWc0wsTpt921Pj+4g/Dm/8yKGcQLxjYzntNYgAl5LrlH2tKzyLbcvYelX73gaDoXAmnjvq+lhBH08IIcpSlXGOMpS4xmbGciTfj3H1NahnlfKvgxW2GyzQ/3qY010ex9vaa5ijD+OW179pUovVkzCHDa2DJrHo/+7n7D/fYMysFWz612+czrzMFSMY9Rc4m3yQ7R8v4tUpKzmseZI3pj+DV3XeNqzuxPDhPqRF7eK0eWE4G0P0z2qemjaJIX5uJZ4iqDoH2nm4ov/1sJXTUKiwtbPBmG/2Ar+CPErfLJqn03FVYY+9peCnjZZ7XUycz7iILuM8xmb3YuH1fiU0dmqFp68//h00ZJ9O49I1QKlERQFGo9n7AvOMGM1ien+dPkWOsxc9vNxxalK8k8q0vE1c/P0YZxq1oWtHJ7P2ZWMcHW0xXszg4t+Wv2nfxB5Vng6d+V3D1+CmmGNePkZ1YxorKpEscbMLmWSVU4a1Hu3QnI3jULrlz4/s3kmKTxhvhwXi7VrGyaWixOBoRdvUurlje6Gez48IgAM9Q4PxPLuT9d/eeMGsWq2Gqzlm763MQa+rwlCNinraIa4uA2nHU0tNVaPGwUFt9pS2Gt9uXuiTEvghIQltZx+ccaJbZ1cSE+I4kpyOe9eipx5U7fBsB0cPljH9TUWfl6Oet0cAACAASURBVEEff5iTFH634TCQciQJvasH7RxA//MuducGMnF2MAEeTqgtNYRLl1M3D1qr0jkS3/Det1VzSubzTaxt19i54jswnLfXzqBfbgzRPzfcd7tUTQX5XHTupyTElzj30+LjyXBoRztXrCvPKjdC5s5g9NhXGWLaybKoMi5+DYGrG1rSSTlhXtasCUYUtauTEkq8J86QnEBiriveFg9QoRJ1rTXHrPju25wGO+GZZdbkjcqd1i0NpCUnlX3M3DzwVJ3kYJzU4Va59z48VH8S98utz6+cxN9I4r5bvp/6qrp9muL+SWK5U1HdYUyp7I5Owj30TcYF+aC1WFU78fAAH7K+j+HAz3EYej1Et6LsreiY1FudAwlwjOfgr+kcOJxJt4Dip/esuNap2uHZFhKPWO53WNX3Ls2UztFjmWjadWrQ076rVbZgMmtzVLEPJ0R9VmNjTQ3UTfVEGawa56iCrLjD6Hz8yn0oQOvRDo0unqNna2SXdaOs66BV7YbScjh6+CTePX0a5M1XltSTwF+hu3yf4Y33FvHKoy5c/jmSt6a/yksvDueFMdOZs2YvKcpODJ21lIX/F0DrxhVvryKawDBCnOPYb/62dDsH1OhI/DWZjAuZJMZsZHFUOR1aK3kOCiXAsI9Fb2/jaLoOvS6dg9Gb2XPK8vqaphqM+myKY315aUf47sAhfj+nQ39Fj+5MEr+cuITtvR60shjQU3FPq+aYLqTy+19Gmre6p4wbtkxkpx7mUOIZsnNNmIx6Lly6Cnb22CuARnfh2MTExdMpnL+cje6/SRz6/XyJcVkblQqT/iLn/3eVq5fPkxqfyLkStymoUClNGHPzLMYdTBeSiE+HVh3aoPnbiNFQ+McEOLX1pLkpjYRfUzj/vzzycvWc//N3ThW9+FPl6k6rxtmkxieRcVlP9sUz/H7kBBdL7cOo15PXRIOjBP6qJCU1mYO7V/Ly6DEsNgtKlabuHkyIVyafvrWQPcczycrWkRK7jcjvCwcwNHYOGP6TwMF0HRmpcUQvjWC/ecNCpaW1q4GUw7GkXcgkQ2eocJvq7o/T16X8ubPrjZaDGN3PgYObN1/v4LX2aIdtegxR3yaTlhpP9MJ5RGVU9hLmgLurA/qkHzlwVkdG5h00oJ8Zw6qZr/LS9AiiY5Ovl+XFOzPxDAq83ojS+HTF82wc0b+Ct0/hbfbu3X1Qx29jz0k3fLsUP/rkyoDnA3GIXcnsjbGkXcgh62wyeyJ3kphrzeeFDJkxREbGkJieTsrPm3lrdSzqwOcZUM+fiNDHbWHZ7njSzqZydOdSFu/OoWfwIDxVYOvggG12KgfjM8nKTOVA1ELWx5lfXS2UU8dAhg90JXH9QlZ9m0xaZuF1ednm5Ab3pEJllJfPN6moXZMdx7I5S4lOyERvMJCVmszpHAdauNwpTeWyVZTPahXk5+iKbohxZcCzgai/X8niqDhSzqaT+O1K5kdl4h0STE81VpVntasfAe3VhTeohQeSFbWUKKs6cvUwsOLkx8Beag6sX0hUXDoZp+LZE7GUqKSKz27PQaEE5O5k8dKdHD2VSVrCThYt2YmhVyhDzKZ1Kb+uteKY4YSnhxNZP+8iKiGdtITSU8w1VNbkjRt9g3zQx0SwaHcyaempHNy5ksU702/UL47+hAwpLPPLvk0lIzuHjD9iiNx2Y2BVrVZDro6sO6hpUibHfzB0sCtJG99lxb/+LMyvlH+z6ctfqG72GDL/zYbIf5OUns7JQ1+yYPW/UT/6XI0kuyGqbp+muH8S+e5GDqTWw/q5hpS4TqrUaOwg7XgcKRd0pCXsY9XKnaSUGiDQ9HqSgJydLNttICDwxsBdRcek3lJ1IsDfiUN7IjjwHy8Cet2YbqHia50rA571Rx0bweyNsSSmZ5J2fB+rPthJmsmKvjcABhKjVxIVl0rGqWT2rHyHT9O9CBnSsAdNnV1dsb0Qz4HjmWSd1aG3sg8nyleybSxudzUz1tRw3VxPWFbxOEdV6Dh0OAvfXp1KLlarUaMnS1fYMlR3DyakQyZR76xk//F6emdMWddBa9oNpfKD3CQOnGhHQJeS5bi4v9EQVTrwp1DUXGSkSttq7IJPn+d4Zc5SPv4kks8+XstnkR/z8cq3mBb2JA+1reLcnha5MWRUEFrzQuMUyLjxfuRsm8ywEa+y6rADIWMH0bq60xy4BBZOm2Xax/wxQ+k/bDLrE3LKvBPAxqUZGr3u+pNujdt0o0ebxlxMPsSBmAMc+v0CNm7d+EfnZmXOwKBq3ormf1/kYn5zWriUuRb2Ls3RXEnl0He72f1NLCn5zel8fxsKc9oRD29P7s77k0M//MSR/4JHDy+amW2uWfuueNhd4NgP3/H9r6lcdemCp/kKCkeaN7cnJ/UIf14pvf880lPSuWq6Strh/ezdt7fwz9c/kfI/wN6dB/7RFW3BGY788A3ffPczSReMKItDiI2a09mvKy2unePID9/xU3waxhZetCnxNKiJixez0TSz/I5DUbGXxkzmrW3peI5awhsDy5n7TeVGyIIlvN5FR9QbIxn89Ehm/zOZ/KLi4B06nuHaJBaPHsZLb+0io9urjPIzr5DdeCo8FPeklQwbNobZO1Mr3CZ2Pox7Z9Kt+um1TI1vaCg9s/examdRYNM/jIn9HDj6wauMnBnByc7jmTPQrcwpXcriHTyeIQ5xzB8xjJc/iCGt5hN/e3IN4u21MxjomMr2DyYXluWoVDzDlvBuiNmcCS4++LqmknLBC9/i97F4dMU7J5UUlRfd2txYVeM3ieXzg9DErWTkc4MZOm4J3+pAbbLucwC1iw/e/MiyyWN4eeE+8nvNYPlrflTnQfbbgfegJ9Eej2DCyDFM2ZxJu/AbdYa6VxgTg+DAvJEMHbeE/YbHmRjqVfL7N5VTNd7hS3g72Imj6yczbNhIZm9Lx9nDqUEPRlSkvHy+SUXtGod2BHSGA8vGMLh/f4a+FYfz8LmM9buTc7hQ+fmsxrunP86/rmTZ94UdDU2vSSyfHQQxS3l5xBimbkzFM+wd3g12u/GdSpRnTa8wRvukEblyHxkVprY+zsvqRN9JS5jok0PUvJEMm7iSg+onGTfQijsgXAKZs2QSvtk7mT1mGCPn7iSr+ySWzwgs8Z6jiuraio9ZYdslpFUS6yaP4eUVh2s0B25n1uSNdsgM3gt1J23jZIaNmcn6466MGuVv9k4oNd6jlvBeqCspGycz7OmhvPTW12SYbvSAnH0ewptYFkfE1fZPvA2p8QpbyMIQV05GTicsJJTwhd+Qaar+vePqB4YwuPFPLJk0nvHv7sbw4FSWjn+gRlLdIFW3T1PUP+lLDIvHjayb31DnSl8n3Xjq1TC80yJ4edhIpkSm0npUOH1L38Ff9L4fvZ0ffTubT8tcQZ+wHvN+xB/n+DhSujzEw+aXcyuudRr/SayZHYht7EomjBzGy+98TU7LdjiorOl7Ayo3+g7pSsaWuQwbPZl1SVpGvPMmIW1o0NT+oYzzN7Bn2jCGTo7gkM66Ppwoz81tY3F7q6mxpobKUj1hcT0rxjkqLTueg2lePFgq7kdLHx5soyN6ybbCAJjKjZAF7zCqbSrrZo6p3j7rkOXroBXthlL5YTj2I4ltH8K3VNuiuL/RECny8/OvWbPitWvXKCgowGg0kpaWRufOnau14+PHj+Pu7o6NjQ1KpbJGA4p3hquc+ikWXZs+dGvRAFqzdenvDI58l4rjQ/541GTc+A6RnJxMp06lrzZCiKrK2DaBYTs7sfzTcOvnHRdCCFEpUteK6jBaeom6qHE2NnJbpihNx57pI4nyWMJnozwqXl0IIYRoYPTfz+Olg73ZNMP/jr7hubKOflDYfni3vAdWGphGVfmSSqUiNzcXOzu7Ku00NzcXlUp62NVjT6s2zUg9lc7VFsVP34mquJr+H3R3t6GLZKIQQtxRUjZOYOruMp6VUvuw4/MZtZsgIaqoorI8bu0M+jrWbpqEqGnm5XzHl1vrODU1LPvfLBi/nvgyHt3T9p/PyuHyzr1qy45h/pgIjpaVzwPfYU2YBJOq7ZbkswF9ZiYnYyNYd8KL0ZPkONW5Mo5zg6ufhRDitmLg6MGTePacJEG/yjAlczDemQdD75ygH1Qh8KdUKmnatCmnTp3CZKraM+0qlYqmTZuiVNarVwzedmxadMTjr1PoDWAvZ3sVGdHnNsGjQyuZ5lMIIe4wnmHL2RFW16kQovqkLIs7QYMu546PMivy0bpORcPnGMiczwPrOhUN363I58x9zB6xkpMufoTMncEAl5rdvKgCOZ+EEKL2GeI5lNSOgLGl58IW5UqN49BdPXj3Dms/VGqqz+LpPv/++29MJhMFBQXXl1u1M4UChUKBUqlEpVLRqFGj69N8ylSfQtRPMtWnEEIIIYS4k8hUn7VDpvoUQgghhBCiaqx+4q84MKdUKq8H7AoKCq5/XlHwzzywp1Qqr/+RoJ8QQgghhBBCCCGEEEIIIYQQ1VepqT7Ng38KheL6e/oq88Sf+d8l6CeEEEIIIYQQQgghhBBCCCFEzaj0O/6Kg3XWBvvK244QQgghhBBCCCGEEEIIIYQQomZUOvBXTAJ3QgghhBBCCCGEEEIIIYQQQtw+lHWdACGEEEIIIYQQQgghhBBCCCFE9UngTwghhBBCCCGEEEIIIYQQQogGQAJ/QgghhBBCCCGEEEIIIYQQQjQAEvgTQgghhBBCCCGEEEIIIYQQogGQwJ8QQgghhBBCCCGEEEIIIYQQDYAE/oQQQgghhBBCCCGEEEIIIYRoACTwJ4QQQgghhBBCCCGEEEIIIUQDoDh27Ni1uk6EEKL+MplMqFSquk7Gba9Vq1ZcunSJK1eu1HVSGjTJ59oh+Vw7JJ9rh+Rz7ZB8rh2Sz7VDq9Vy+fJlcnNz6zopDZqU59oh+Vw7JJ9rh+Rz7ZB8rh2Sz7VD8rl2SD7XPsWVK1du68Df/sTLLN57hpXD2tKxhT0FBdcYtfFPfj93tcR6kx5vwTMPNANg9XcZ/HoqhzUj7sO2kXUPNf6ScIy0M+eqlMa27vfi29mr3HVS//yT0WPG8flnn+Lq2hyA5N9/Z9yrE2nSxL7Eun//beLN2TPp+aAfW6K28s23+/l0w7rrnw8cPIRXxr7M4/36WrUNgFlvzOXQ4cOMH/cyg5984vp6Gz/9jD179/HFP7cAkPDbb0ycPI21q1bQ3tMTgKkzZuHSrBlTJk28/j1Ly774cjtfbN9B1KZPry+L2voFqal/MnvmNADeW7qMTh07MKB/EAAGg4G8vHyaNtWUm3+lfbZ5C3l5eRgMBq5czWXQEwNo0qQJS5ctJy8vj7y8fPLz88nLzyM/30CvB/2YNWPaTduxlOYtUVv5eOMn2Nk1LrGu0WBk+xf/xN7enqkzZuHq2pzXJ7wK3Hx8rdkGlH1cLPlwzVrs7ewY+eKIEst37d7DJ5GfEb01yvoMrCKFQnH970qlEoVCQXJyMp5FZQWM/OvNwSyze5M903sULcvkn+NfYk/n5UT+n4fF7eb88R3b9//I0ePpZOTo0F1wYujq9YzygIxdUxm13ZVp777Ow8464lZP5Y3k3qxf/TxuBT/y1hMLyRn7TxYPcLixweSPCJmUwICPVvPCvbcmL26pPz4iZPxO6BHKxPAn8XU1cHzzfN7Y7sTEyDn0cbSQzwWHeeuJeRgn7WB+b5vCZUfeZ8hsHS9texvz7Clk5OSepbz7WSwp2Ro8A55nlMdh3vjag/XrX8TN+F92LJzF6iNqfHo/zoABj/Owx00bqdeMPy7i2fcOkw+AmgenfMYbD9nUcaoaMGMOGZmZpCd9R9QnX5M/4G2Wv9CJEjn+311MnPAJDF/NssGudZTQ+it9y1hGfdeDVR+9SLui5o/xQjLffv01cUdSOa3TkZWpw/OVKJY96QR/fMLw1xIYsP4Dnm0JlHne57BnxhDePWKDbelTpG0Y61c+g1st/9bbgW7XVEK2uLFoyyv4Fjc3Ez5kyAwdY7fPoU/j6ysyMfQLPD+I5OX2ZhuwVN4vfM2c8C04zfqY17rZQEEya16YwukXd7C47x1UP1UmH0U1WGq3WXG+W2hzZHzxGiN2dWLZJ/+Hl9LKOiPnMO+Oms2ebA9eWLmaUZabibelK39XrQv928rnWJA9mk2ze6MuXvjHR4RN3AUPPM/YUU9yf/N8fo96iwU7nHh54xwedUgl6rU3+P2xxcwb6Mal5K9YtyKC35q/wvvzHqd5jf2q20+TRoqKVypWYfv51qXzjiL5XGeMh9/n2TczGR75LoPPWnmdLKv9bfyROU++j8O8aKYWV//8l8/Cx3Cw78esDki4M9oj0t6oHZLPtaKi8Q2LfZeK6oJBqeV//syd3Wevdn+w2IXDrH//I7YnZICLFwNfepz8j5eie2kH8wMs1LnZ3zHt+aXYmo//FRjR/ZVJxul49m/ewg8Oz/PBgidx4zDvPjObfzn1ZuwrLzKgizMXD0cwZ96PtJv3GVN7mG2/IIEPnp9F2vOfFY4X3CZui3zuHGP9ddGYymcTJrDddRLr5/Tmppy8sIuJw3fhtWY9o1pXPj/0er3V6zYyH8i/HfXrfDf6/AJe+ewUr/ZrwZM+Tnz4wn2sjjnH0bQrONgqGdzNmQH3O3Hpyt8s+fos/83KZ9nzbWlsY/1TSPc0c65y4O+eZs5UlI/FnysUiut/v1bUX1wfsQata/mVpfn2zbdlzTYMBgOnTp/G3t6eq1dzS2wr9qefuHT5Mk889XSJNMX++DMd2rcvkebSabC0rHRaS3/WsmVL/nvmzPV/29raYmtrW+5vt2T4sFCLyz9Y+l6ltmMpzdeuFeDSrBn/3PKZ1d8rfXyt2UZ5x8WSvNw8mjnfXNbKy/dbpazfX/x3BeZlQwEoMCgspzFt+xRe22yg76gwpr3YFe3FzYx6+buibUCL3sEEfLGS9TOG80GuGm0Hf956MxR3hVlaGpXatkJxfb+3eRVnmbMWrbodD44P5cGi07pb6DN0++J9jqYo6NvDcj7fdDyu/89SPqjxHDiD9QNnXF+S8tEXqN0fR6tQoFC78dSbnxFwIpbdu77gg/Fb2P7Su3wQ3O6W/ezapu71Ch9/Yrz+bxtHdf0sL/WFWkMLNw0t3Nrh66DjqUVb2D9oAQOKB4j0Caye9xFpPq/z8VNa5FBURcm6z5D8Ca/NisFh0IuMmDEeL5dklj0/i7SC4vriekVaWPbLPO9dUaBG8/ib7JnUo+zd32EUlurYe1xx5ncyLihQFEdDdZfIwhXtPWbrlVHeM777igPZmdi++RR7i5bl5xrh/acYeGA8exY8Xiu/rc5Zm4+i2m5uT2DF+X5zm6P4v8Xng1V1xsVUTuY5oFEZMOYZUSjUZa9726lK4C+dEydzcH3QgxK/1FmLq9qDB8Y+zwPNARy4P+QZ7o9+n2Mp8KhqF1/pHmJqUOHJcHenJ5g6J5/J4Z/zVcrjjPK0sKsGolL9Gyvaz6IGSD7XGfU9rjiTjP6KAoW118my2t8OzmhdDJy+mINCUXQTtklHhk5BMxdnMu+U9oi0N2qH5HOtqGh8w2LfRVV+XaBQXSr/8zv84FWrP2juHj9GL/JjdPG/9V8z9b3WeLuXMUalcUarUZB2JQeFoiispFLjrHXDWeuGd3tb0p6PIOpwENN6OOHspMYn5BWe8ik8hi16hjKg89dEJ6ah8DMf4ysaIyg13l/Xbod8rui6+NmCx4sCfDoOLJ3PZzlBvPVaIM4Wtmv4M5mTKneealm1+q8yx+a2D/wBPPNAM1rdrWbh7jN88ctFnvRxYqifC6/2a0m+8RppWXks++Yce3/L4pGOdxMR5oG9unJTD97bQsux5D/INxgq9T27xra0dG1udaabB8vatmmNWq3mp58OEvzMkDLXN/9/6e1Ys41NWz7H1bU5k5+fwMw35hLY+xG0rq6cOXOWU6dOM3vmdDp73Xhice1H6/gh9kf+b9RIABo1aoTJZCqRBkvLSqfZUvr9HuhG5KYtpKScpH3726eXap7mDu3bs37DJ6T++SftPMq+9bh04NN8mTXbKOu4lOXS5ct09vYqs6zV1rlcVrCz6NPilUoFpLhpUAkA00n2RifTetgmxgUVXayySg5Ep331CfHdJ/PP17renJhGnni2g38m/MaVIH+Kn0dLS0ggQ+NBe209bUTe40O3Vp+QlHwFhbboVxmNgC22tmb5qCj5d0vLFIBCaUU+GJKJib2Cb5gvtmYrO3d8mBEdH6Zf2wm8EP0NyU974tVQZnZt1BRn57pOxJ0gh5w8BxzMHoC2bWKLrekSRmNR2TScJGrufLbbBrN8Wh+LjSNRsRunfmG+xu/ZxUmfV9gbFlg4yGwqqo+L6wkFFA/gm9cRN5/34/Fs54o+5jDxRj9869PY/C1UmIel8k/rTTenT0hKvITCvfC6lvFbApktu+J9V8XlvcUTC9j6qFlb1HSSyNcXkvHMWqYFut5WHbBbypp8FDXAUrutacXnu4U2R8l2oRXbIJPtH34JT73De6YlTPhwC30/DMOzntQvyqoUQtN50s+Ck2uzkt936UrXlp9w4verKF2bFC7724iCxqhtFShzFRgLDCiuKVAWf+0uZ5wwYswzW9YAVarOq7D9fEuSeOeRfK4dJh26XCeczCZc0aUkk6Zyp8U9ChR2FV0nK2h/N2qHt5ctexJ+48rAon50agKJuV487W1LiwfukPaItDdqh+Rz7ahgfMNi36WCuqCiuuJOP3ZV7g9WQBcbS5JrD0a7K1AoDOh04ORk1kg+n8rJbA2ttRoUCgU5OQYcHMw+b6JGozKQk6tA0ag13bo48a/k/2Ds17Xo5jMjBiPYqhvf9BBFifGC28TtkM8tOpV/XXRWKIAcktbP5u1ftIx7fww977KcCF36eQwunWjRqBYCf0qldVNh1rVennex9ZWm7Dum45vjOtZ+l8lVgwmlAlw0Njzc4S5Wv+hJB619xRuzQK1W071rZ346fKRS33vA535sbCqe6qA4n5VK5fW/azQawka8wMeffIpG48AD3buTef48xxMTeT7kWeDGwTQ/TjY2ai5ezEKhUFS4jTNnzhK19Qs+XL6Mjh3a86BfD5YtX8mSxQv54cefuPuuu+jT+9ES72jrH/Q4U6bPJC09nTatW3Nvq1b8EPsjZ86epVGjRrTQai0uM/+NxYori+Jl7du354EHurN95y4eCXiY+9q2wVRQwNmz55j5xpu8u3ABPl3v53J2Ni+MGMkzTw9hxAuhN/37jz9SaNGiBf/T/4/09P9en9K0siyl2a/HAzzQvRtz31rApNcm0Nrdjd9P/IHx77/p/UhAid9W/L3GtoUt6gsXL9KihbbCbZR3XMqScjKV/3tp5PV96vV6jMa/C98tcg0uX84GwMnp7irlRWWUHuQ5n/EXDi6uaNTKwsEHs7wBZWFFplByU32jtEPTBNKO/0Lqwz2wPXOY3Z/u4mSBAz2VSpRKMBoN6E8nE5/hTuvGgEqNRqOhMLbfgiee68P2Nz9kcdvGjHhYiyFpB+//M5POw2fRq3H9qN9u1ponnvNh5IYP2XtvOAGuVzi0fgtx9wSy3LswH2/K52vFQT6zfFYUP/lTmJclnN3H+5/r6TbwQTxVGXy/eQW77w5mRe+mKJWQtvUdPsr2Y8QQf1qrM0lO0aForqWZjZL6mquibui/fZ8XPoUho4N5pIMzZMTz+Ucx5Hd5mV7NlShNmexeNJvVZ314/a0BNLv0F+eLvltYr9Rp8usVpdk1V6mEpvYOGJJ/I+6ML575qRyM3sh+nRHP4rrDzpbG5HDp0hWU92rKPe+1T73AI1+/x+IFWqaPDsSzyRUSD/xAls8wBrap619euwz6LPQGuJJrRGEycuXyJS4p1WicNaiV3jzxtCcjIz9kR7vxBPAjq79Ix3fELDrYKKGi8t60GS2bmu3MdImmKsi6uwUtm91BJ0NF+ShqiKV2G3So6Hy30OZo3FiNIu8SuitKlHdVvI2sfStZd9afN9/ypnOjCQz8bgYffNGXtcPqx8TBiqoUw+zLXDaoae6oKfX91gQ968PYTz7k21bhPOSaw+ENn/PLPYEs8lKiMAXS224eH69/kGkjHuJedSY/RX7JLy7+zO+grFpa6onKjVVU1H6+Zcm8w0g+14asb5by0ue2DBkVSj9PDfoTe/lwfQLaJ5fyaFMlSsq/TlbY/qYx/xg8COdJn7D6320Y7ZXF7o92ceXhSfRrrkTJHdIekfZG7ZB8rlPl9l0qrAsq+vzOVK3+IOlEvzaej1Qj2bh0EFr0JH4awYF7AhnYxRl90l4+XP8nPScWrZ++i9njv6P1iDCG+Lljm5XEl6u3kOoZzMwHGqP8aytTx36HNiSM5/7hjsaQxsFPPyHO/kHmPNAYpRK6P/ss2vEfsfr+mYzu0YSMAx+x/aQnA19vXXjdNhnQX9ZjKMjBaAJD7lUuXVKgtndGYyf5PPOBxihVjSu4LhpI2TqPKdty6Dd9Fr2aXOZ8RuGq6qauOJu94ezSxUwUd/vjWMXx1QYZ+ANo0ljJMz3u4Zke9wCQk2fCTq1EVUO3Obq3akn2//Qknkixav37vTrSSmvdfMaWAn8AL4Q+T9OmTYnctIWlH6zg3ntb0f/xx66vYynw16f3o0R+tomgx/vh2rx5udt4f/kKAns/ilenjgBMGD+O0Bde5Lt/f8+BH2J59JGAmwKXfj0e4C5HR36I/ZH72rbluWeDOfHHH4wcPYYunb35YOl7FpcVPz1QXuAP4KlBT+Lu5sax44n89tsxmjg0watTR1Qq1fV1FQoFqkYqlErL//7t2HG++/4A9k3s6dDes5KdwhsspRlg4YK3+Gjdx8x/+x0MRiPtPO5jWOhzZR6Xe+9tRaeOHVi8ZClRmyIr3EZ5x6VPYO+b0nn23Dn+Nhq577621/c5Z95b/Hrk6PV1Bj0dDMC/rf9ZigAAIABJREFU93+NWn3rG+Pmd4AEh77K0x9+waQuyqLl5uW8aNonpYXAH615ZsJLnHw/gpdDI3D2CmRE+Mv0m7Hp+sB1h779ab1tA1OHbyTfVPQ1tTPt+7/Kktcfxvmhqax8cwMffrqUl9frUd/TjkdeWsQrT7eu140Ql75TWHplLUvmDWPZZQ3aLn2YsWgkXRorKQymlsrnaxaWKYvuiFFa6ITf5UmbguWsmrKRc39raP+PYN5f+CwdiqqDln4P0uaTrUx98R0u5mto0SWQGVOG0LIeXTPE7cHx8SkszNtAROQ8ItOywF5LO/+xvD9mMC2VYPgugmXf64BY3h8Ta/ZN56J6pa5SXv+Y32ShVEKXF17jxbNrWDRqGGh9eGLEBMKzX+f74nri3m70uu8LPl/6JY99MpLW5Z33zfsyb7mCj9Zs4q3wCC7iTPsH+vBct79RKhvQAFCFstg/bygLfy3+dxKzgveB2o+52xfRTwNtgufxzuUFLJ0yjFVo8X16PrOebIGSKpR3S3X7HaK8fBQ1pYzyVdH5bqFcuvgG4B25hkUf/cKj0/3K38b/DrFq4+90fmkj/3BUAt0IfzmA0CUr2N77fZ5uVTe5URlVOh2v6riiaIqm6c3tMufAKSy4spYP336BVdkatJ0Def3tMLwbF+XPoil88uFGpj2/kBwKP5++KAzfJg37jKhsvVd++1nUFMnnW89lwGwW5q0l4uMpRJ7JR32PO92efOv/2bv3sKqq/I/jb0WPUodQUAMtsSbUQi3MBqd+kAZZqJOSmRap4200nUyLUYfUKIvSKBVHhbyQIKNmaTSplZoKMw2UQankhVLBC5SBGkfRowd+f4AICpyDchM/r+fheThn7732Omvf1trfvdYmeFhnLhZzRddJa/VvgKadh/Luy6cJWTKGp7MdaOfzHG+92APHsjZjPa6PqL5RM1TOtcV628XauaBS54obwrW1B6EJDRo1oKld0b06HGnRoQWH3n+NEWHnMNzmyZ9ffI+/PuJYOH+7wbz1agPmfzCfFyMyMd/kirvXc8wfP4g7GgOtB/PWtAa8/8F8XlySSS4OtOviy0vvPE9Pp6KNdHsAb71q4q1/BtF/lhkHN0/6znyN4X8omr47kiF/+4TfLv6kyOH0i4TWz77PR2Nr63U/daycL3f5dfHXz1kQ+T0mC3w2cziflZi1w8gPiBrmVvw553cT2N/CLQ2vLvBXmetwA4vFcnVvJq/H0g8fJTH5e86fP1/mdIOhMX+6vyu3t3Gt4ZzJjSp6RSzZOTlMmvBCbWellAYNGrBr1y46depUTWtI54OxUzj49AJee+TimAVmjm2azV/fyuYvq+bwVKtqWrWIiIiISBlM5/NrOws3BKN6goiIiIiIFPv9999tnve6eMdfTWvX9jZau9zKz+npZP1yHNPpMwAYjTfh2qoVd7Zri8GG4T1FqoL5/HniPv2Mue+9U6fGWL6oWsd+tmRx7OhpTFk5mPJb4GAHnExn9440zHf50aWFxoMXERERkZpVF+vk9ZHKWURERETkksrUjxsUFBSox59IHXboUDpbtm5j5PBhtZ2VMu3atYvOnTtXW/rZ30Yzb8l6vk43gcFAE3tn2ncPYMTwPnRuVm2rFREREREpk3r81Qz1+BMRERERuaQyPf4U+BORa1LdgT8RERERkbpEgb+aocCfiIiIiMgllQn8qSYtIiIiIiIiIiIiIiIiUg8o8CciIiIiIiIiIiIiIiJSDyjwJyIiIiIiIiIiIiIiIlIPKPAnIiIiIiIiIiIiIiIiUg8o8CciIiIiIiIiIiIiIiJSDyjwJyIiIiIiIiIiIiIiIlIPKPAnIiIiIiIiIiIiIiIiUg8o8CciIiIiIiIiIiIiIiJSDyjwJyIiIiIiIiIiIiIiIlIPKPAnIiIiIiIiIiIiIiIiUg8o8CciIiIiIiIiIiIiIiJSDyjwJyIiIiIiIiIiIiIiIlIPNDKdz6/tPIjIdU7nERERERG5UZw6faG2s3BjuLlRbedAREREROS6pB5/IiIiIiIiIiIiIiIiIvWAAn8iIiIiIiIiIiIiIiIi9YACfyIiIiIiIiIiIiIiIiL1gAJ/IiIiIiIiIiIiIiIiIvWAAn8iIiIiIiIiIiIiIiIi9YACfyIiIiIiIiIiIiIiIiL1gAJ/IiIiIiIiIiIiIiIiIvWAAn8iIiIiIiIiIiIiIiIi9YACfyIiIiIiIiIiIiIiIiL1gAJ/IiIiIiIiIiIiIiIiIvVAo8ouUFBQUCUrbtCgQZWkIyIiIiIiIiIiIiIiIiKVDPxdDPrl5+eX+myri8G+hg0bUlBQoOCfiIiIiIiIiIiIiIiISBWxOfBXUFBAQUEBuSdPcCrnNywXLlzVCu0aNcLRqQUOzZoD6vknIiIiIiIiIiIiIiIiUhVsCvxd7NlnsVg4lfMbd7Rrh729/VWtMC8vj4OHDnGTwy00atSoTvf8yzn9M7/+nspp8280oAE3G1rSytGD5jfdUdtZExERERERERERERERESnF5h5/+fn55OfnY7lw4aqDfgD29vZYLlwoTq9hw4ZXnVZ1+f3sUXYcWsKJ0weunHhsDc5Gd7q1G4WxiUvNZ05ERERERERERERERESkDJWKulX2nX41lVZVOnbyO77a81rZQb8i2aY0tux5lazff6jBnMmN6uiRI6z96OPazoaIiIiIiIiIiIiIiNRxNgf+CgoKoCqDdUXvDKxLTuZl8M3BRVjyz1md94LlLIk/L+D3s0erNU/nzp3j1enT6fv44zw7aDAXLlzgib59+OLzz6t0Pf8MD8e3Rw++2/FdlaZbnj0/prLqX//i/chIYmNiamSdAB+uWs2T/fpdczqHD2cwfuxY/Hv14o3XXuOnn9Lw7dGDrKysKsglvPKPqbz26qsAOLdowZrVq/nll9Jpnzl9mo0bNjD++ed56smAKlnv1cqyXPzPzJbpvegXmlib2akXclJWEzrhGfr16oH/k8MIXpJYopyrgomNU3rg26P0X9Da7CtnzUsmPLAHvsOWklaleZAbhbX9OSdlNaHji6YPHE3omlRMtZfd61deKmuChxWWY+A8ksxW5t+7lBG9xrOmeqsy9U9eJikbYwgbPwD/gfNIufy8aMkmKeIlAvv2wr/fMIKjkskpMbky+3tW3Ev49+hF8JfWNmY9ZKUcpXZlbZvHhIG98O3Vl6C1mWStGY9/4CJSVU+4UtJbDOnjx5NX/PVnQcql2U7vjWPB5OcYEtCbISNeZMH6nzh9caI5g/iFf+f5Qb0Z/NTTBM1ew77TZa1MTKmfEDbhGfr16kW/weMJi0tTnaIaqJyrX0bs6Cvaav4hmymuEdhS36iwPWkm48t5TBjcC3//AYwNWU1qORuxXtdHVN+oGSrn2mOt7WL1XGD7ueKGcY3twStYMtkSXmL+iPjS5+vURQRedj3wffLSes1ZicSGjGagfy98/QcwNiSGlMtXeDy5eB7/fsMICt9MRl0/pdexci6pzOtiVjzLpheVcd9nmBC24coyNmeSEDGVEU/2wr/XAEZMX0pS1YQSymTzUJ9Alb6Lz5a0ft63FwdHR1q5uFY6/T27fsD1tttp1tzJpvkLKODbAxFY8s/bvA5L/jm+PRiJ792vAdXznsJtW7fy3bffMmf+fBwcjDRqVKlNZpOCggL+m/AfOnToSEL8du7vdn+p6Yvff59TJ08SNHlyhd/ZavOmTaSkpODt7c0dd95RpfFka5raN6Vpk6bXnM7KFbEUFEBUdDSNGzfixIkTVZC7S5o2aYqhSZPC/5s25fE+vfk07lNG//WvxfNkZmXy1ZYtnDld+63+kFXpRAS61XY26g9TPBGhn2Dq/yLzg93hwCeEhc4gxH5xFZZzLiaTAfdhcwjpdek8aXByvmw+M2mrFrHllBEunyRiC2v7c9pSgqfH03b0NCJDnMhNieGtsBmEuywn2NtY27m/ruRsiWHZTmdGhL+NdzMDTobazlE9dSSJuC0p5JjOUVZbKWPNDEISXJkwazmeJBERMoNQp8WE9XOt3P6es5mI5fsxXP0I+9e1CstRapcllQ8j1mF68HWinnHHaHTFvLG2M1WHeb7Au4uHl/jCzOHPQ5m70xv/LkVfHVrDG6/GwiMvMO0Fd0iL44NNSfzy+F3caQf7VoSw+GdvJke8Q2fDT2x8N5jZi9uyaKIXZZ3qT3z1Fq+uTMWmlm1zXya+NZwOdtf6Q+uAg6sJnhIDvSYSGuQOez8hcmMimX3dca8Pv6+uUDnXiHO5Jgzdnidyks+l49zeufj/Cq+TNrQnzTuXEjInmfuCFhDcMZctc2YQEuZKVIgPpWok9bw+ovpGzVA51yIrbRdr5wKbzxU3kmtpD17BTGrEVMJ3dSVo/it4XEhh2VtvEMwcIsZ6FJ7zTbmYHH0Imv88nhevs3YOONkBZLNlzjtsMQ4hONIb13MpfPjWbEJmORM1qzdOAOZUIqZMZYvTICbMeR03cworIjeTctyPtm2qpYSqRp0q5xLKui7mJRM+cTY/PjiRkChPnH9NICJ0Lv+IcCNqQlH6mEia8xKhKe6MmLIY71aZJCyLYcve/ni5VM8N16qPIlUhB0dHdqV8x4MP98T+ppttXu7o4Qwyjxzhrg5327zML6d+uKreeyfPpPPr73todcs9lVrO1vcbnsjJoUXLVri7u1c6b7bat3cPv5ty+evzY1m44J9MmDixVN5Onjx5RVizrO9skX7oEP+Jj+eZZ5+lw922b5+q0qxZM5o1b17udJu3y8mTuHdwx8Wl8D2PVR34c2zejCZFgT+Anj17EvTSS4waPbo4aP6HP9zFO+++y78//ZTlH0RV6forK31HCjmBbtgWZherjD4ERXXFYCyqRrkMIdB7HSFVWc4WE7l54NTGHZc2FUQHMtYx/2Pw79uJuP9VxYrlhmNtf3YfRMjCQbi0LZzu0mscgV/HE74tGbO3T5k3NKVs5lPZ0Ko797m74qIbb9XHvT8hYf3JiXuJwBWXTbOk8sknaXgOex1/D2egPxOeiSdw7QbS+o7E3eb93UTS4qWkdgnAO231jfdEtLVy1P5dywofHmrbpTttXQr32mp8UPX6ZzDSqnWJW2MntvDBV2b8pg3kTjsAE4krYznoPpzwMb60Arj9Bd565OICGez6Pos7H+tH5+YAd+Hf35uP5n/PQYtXmQG75o/8g/BHrvy+fjORsDyGtI6jiJrghwtA2xcJ71Xb+apvVM41xWQyY2jpRts2ZdzAtHadtNqeNJMUt55c72mM9XXHAASOCWDL+NVszPJhoEtxLup3fUT1jZqhcq5dFbVdrJ4LbD1X3GCupT14+f5uTibuSxPeU57H+w4D4MeEcckEhqwm4ZnX8XUEs8mEuZkbd7Upq53vjP/05fgajUVtST9GDN7MxrBEUs298TZAzsalxGX7EDxnJN6OAK4EL+hdDQVTxepUOV9UznXRvivPzV6Moa1rYUDcpSg/KzeTNt4DDzvg4Goit4D/rGkM9DQAbgyc2b1qyqoclXrHX01r5eJKm9vbkpQQjyn3d5uWOf5LFj/u/J77HvgjjQ223y7MPLXzarPJLza86++nn9L4c58+7N+/j9EjR/JE795YLBYKCgpYvXIlgwcO5M99+vCPKVM4drQwALkiOoalS5Zw+HAGvj16sPyDD8pMu6I0lixezJDAQMznC5/3PHToII/5+ZG2f3/x8vHxCdzb5V663n8/J3JO8GPq7uJpQ54N5PMNG9i4YQO+PXrwyj+mlvkdwMdrPuKV4H+wfft2xo0ZQ+AzzzL77bcw5V7qA56UlISLq+s1B/1Wr1zJksWLWfvxx5zIKQy6HT16hJcmvsj4sWMZ+ZfhPDf4GZ56MoC+vXsTOnMmAE5OzWnufCnw9/Gaj5j2SjBffvEFTw8YwD+mTAEKh9F89513eKJvHwYOeJI5771L3pkzAEwOCuKbpCQ++/Tf+PbowQ/ff19mHitKw9p2cWruTPPml6L9bd3cKKCAwxkZ11Ru1cbeWPrmvJ2J1LVvMHZgYdflsWGbyx+mMieVjQtmMHZwX/x9ezFwxAzWpJZ4lsOczpYFUxkxsC/+Jbtb95pBQlGaWV8vJWT0APx9e9Ev8KV6MdRMcSOtiNlMiXIuHFJ1QmwyCUumMqJfL/z9nyEoOpmcrESWTR/GQP9e9Aucyppyx2DIxZRrwMmxovNkNhsXxJDt+zz9b7t8momU2BmMfbJwmK/ASbOJu+HHe5DyVLw/G4uDIBR9djKC2VzXx52oWzKiRzN8cRrmAzGM9e3BiKh0sGSTsnYewSMG4F80rGTYxvQyn5YrVPFxbc7YTMSUoqFEnxxNSLSG6LnC0WR+zHHDs8ula7hTZw9cM1P5MQds3d/NO2OITHDluRHdMd6IQydaLUepGkX1iVXJJCx4iUD/XoyNTS+cUt7xbo4ntN9UtpjMJMzsVao+dkXq5aVhyWTNhF4MDI0vrq9lxb1Ev8GzSboBqhL71sWyyyOQpzoW1cHMKSR+b6brI0VBvysYad4cjqSmcvFRw9O/5WC+2cjNVm6Wmn/L4MiBnzhQ1t+hLE7Xp/OLOZmEHWa8ehUFo6R6qJxriBmTKRdn53J6ANhwnayw/m1JIzXVjLtnp0vt+Ds9ucdY+H3xMvW9PqL6Rs1QOddd1s4FNp4rpITK7u+nssjJc8L1tkv35gxduuNJMil7Cz+bTCZwNOJQXr3PWPqerNlsLjzfFz1glpSwG+ODfng5XuNvq0tqo5yp+LrodDHoV8RoNIL5Um/FjK8TyXT1w79LzT3eXqcDfwAdO3WhQcOG/C9+G0fSD5U7n8Vi4ad9e0hO+h9Gh1twatGyUus5fe6Xq85jro3Lnjl9mrnvzmHy1Kl8HBeHnZ0dH61Zw5bNWwh5/XU+iImmQ8eOvBIcjMVi4amnBxI45Dlat2nD2rg4Bg0eXGa6FaUxZOhQCiz5fPzhhwAseX8xj/f2x719++LlE+Lj8by/K46Ojtzl7k789vjiacuWf8D93brx+OOPs/HLL3lt5htlfnfRjm++Zce33xL23hwWLFzA0SPH+OKLL4qnHzl8mNtvv71S5VuWR/z8ePbZZ3F1bc2aNR+Sn59Pmza38d7ceSyIiGDpB1GsWLWSj9au47MNGwiePh2A9h3v5qWgoFJp7Un9ka+2bCHi/cXMfPNNAELffJO8M2eIeP99wv/5T3777TcWLVwIwKshr+HZtSuPPfYYa+Pi8OjUqcw8VpSGte3SP6A/ffr0KZWe+13t2bt37zWXXXXwf7J7qZObadsiIg90YkzoYsImeJK7cS7Lvi67UmDaG0+S2YPA4AVERc/hubapRMyOKX6XXFrsDMK+cWL43LVs/HIlIf3cMHYZQmh4Yddr0455vByyAXOPFwmPWkzoc278FDGJkLWZ1f/Da4LFRNqXc4lKcr6inFNXzCOh5UjC135J1AR30pbPYMz0DTgMXsCaz1YyoWMay+avo+xwsQlTnpmUJUVBwsGjCVmSSE6JC5dp2yKWHejO2BFdrxjCwZy0kNAV2XjPXEnc2sUE+7qC+maJNRXsz5fmSSNplxm3ju7aoyqh7eA5hA1zx+A2iLC1nxEe6AY5KSTsNOA16nUio5cT2t+BhDmziSuna06Fx3VOPGEvLyWjy/PMX/UZsbMGYdwyg7DP6sm5tqpk55CNK04lu2Y7u+JEJpm/ljF/Wfu7JY01C9djeOZF+tXloVeqU2XLUa5J+ofvEGf3NO+u/YzwwW4VH+8GH4Kip+Ftb8Ar6GPi1kzDu6zGcUVp2Lky8IUhOCcsZcVOM+QlsmJlOp4jRuJV38eMyktg41dmfPp4UzyezS8ZHDG35U738n68E97DhnNbylsEvxrJlv/FsXhlGvc/5c8Vz2WVYmLX+iiWr4hiZVl/0VHEH67KH1fLstLJMLvh3rG+70S1TOVcQ8yY8iA7YXbhQ55PPsOEmatJOVU0uTLXybLq35ZssnIMODmV2I52zrg6Q87xone+3wj1EdU3aobKue6ydi6w5VwhpVV2f7c3YrDLIftkie8sBjCYyTle+EScyZSL+cgGQgb3xb/vAEZMmk3c3rKeljOTs/MTwlfux21AQOFwlZZ00o6Am0c9u79SG+VcqeuimZQdqeDeCTe7ws/pB9LhLveizzWjTg/1CZBx8AAF+fnc0+U+ftrzIz/v30fr22/H4RZHGjc2YDaf40T2b2QePcItjs3408M9SU5K5Od9e/lDh442rye/IP/qM1mJZYf8ZWjxsJ1nz57lg6go5oaHF3/3l+HD+fenn/Lzzz/Rvn0HmhiaYtfQDkfHssPytqTxwsQXmfn667R1c2P37t1Ex1zqH3vg5585dvQo999f+F6/bg88wFebtzDub38DoHHjxjRs2JAGDRtiKNGDsqzvAJo1a87LRYG1m26+ifu63kd6iYBtXt4ZHG65pfjz3r17+SYpkaHD/mJzGQK0bFkY2P3Tg3/im2+SyMrKpHVr67VRQ+PGOF/2DrMzZ84wZeo/aO5U2BNw5w87+TE1ldUffUTjxo0B+OuYMYx/fhwvBQVxs/FmGjVqROMmhnK3i7U0mjRpUuF2MTo4XJFmc2cnTlbxkKJVZWy30g0/Q7fnCQ3yK2xU3DkI37XxfHMwE7yvfD+d8cHnCXnw0ud+A/34cFIy3x8fibuLiZ/SMnH642S82xgAV7z9uxMxKR3zna4YySYudj0m72kEBxaNL972RYKP72b4ihhS+k7G8zq+smXFTWVMeCImnPGa8N4V5Wx8cCRB/Qov3sYePnjOScQ8cBoDPQyAEe8enoTNTCPNDG0vLwc7D3yHDcGrcwC+HQ1kJcUQOnMGwRff+2BKJiIiEY8Ry/F25IoelOZTJkwGZ1xaOWM0gkffIXhUY1nI9c/a/lw838albDzZnQn+em9opRiMOBgAOwNGRyNGO6ClHxNC/IpnaTvwabxWziBlp4mBLleWf0XHdUZcDEkeI4kNLLph5OTH2IEbGLgtiZy+/TXU80VFT/SVqh4ZwICZsjqxlrW/Z8Ut4kNzAKED3YDU6s5x3VTJcpRrY2rVmwmjuxcPaWP1eHcseqrY3gFjOSMHWE3DfRAv9NtMUORqPLziSXAdQmSv+v8yYfP3/+E7Oy8me5Qot9MmTuNA4yNrmPvuGhIPmLj5di/8R73AU56FZ1dD87vo0M6FE79tYfEbOdBxOKF/tHbmNXL/sFe538pc9UauiVwcMKSvJjT0QxLScjG6daf/uBcJ7Fb/960ao3KuIUY8/UfynF13/B90w3A0noiZbxAS6kDkrN642HidLLf+bTZjpknp5TFgMFwaheCGqI+ovlEzVM51l7VzgQ3nCrlMZfd3oyfeXc4REbsaX7cA3PN2E7dgEUkmcOccYMTljwGMsHfC1787LuY0NobPIDx4Lk5R04qG7gTSYhg7YSlpeQZcek3m3cCLgb7CIfqNllRipy/lw6R0zI7ueD89kQkDPa7f9zTWQjlX6rp4cDWx28A3xKfoXokZkwmcXEwkREwl9rNkMnHGvccQgib0vvKebRWp84G/UydP4OnVHcdmzWl92+3s2fUDRzMyaGpvz4Xz52lq35Tffv2V+7s/RMtbbwXA849eHEzbbyXl0m4ylP/eN2uaGmy/3XXHHXcW/3/wwM+czcvj5UkTS81z4fwFcrJt6+9uSxpe3btz3333ETJjBn+bMIFbHC8F3uLj43F2dubOP/wBgAceeICVsbHs27eXDpUInF5kZ1c6bH2T/c1kHr3UG6BJk6aYcnOLP9955520dm1d6fWsiInm7NmzmM+dw2w2k5uby9GjR3j3nXc4d/YcZ8+e49zZs5w1n+Xs2XM8+Kc/Fff6u1zz5k7FQT+A1N27yc3NZUBA/1LzWc6f58zp09x0s/X3TdqSRkXbpSw33WTP6TOnra67NgRFpxI21KP46RGDvfHSxcPOiIM9mM9UMLjc3s3EfRlPyq50MnOzybE4k5sLuBjx9OpExNr1JDztgbdTNklfJpLj5oebAbCkkroXPMaX7pHW9v6uuKxI5scs8GxbLT+5Rrj4TyPSM5P01M2sWTaeoFOzSpUzdoZL/xuKuvGXOAQN9gaMmAoveldcRJzxGjjk0rq8RvJC/3gmbNlM2uAhmFfMI8F1FJH+ZTfgjT1GMvbrqYQNH8YW39749+2Dd7lPqtdN5oQ3CJyVWNTtvgleU1YS7H0dR4rrOKv7M0DGJ4Qt3o37sMX4KpJUJczHU9mycT1JO9LIyM4hO8+Me15hxfZy5R/XJlL3pmHaMZuB/rNLL3SnFyZQ4O8iQxMMF8+7F1/2bTZjxlD65d9Q9v5+fAMRyzPxnf42HgagPg6rZYvKlKNcM2Nbd9oW1x+q4ni3JQ0DHs89j+/wqYSscCdwQf8bYtjAfSkp0HFM6ffy2QGWVD7fdB+jXl7GuFvN7PvwNWa/PZ9Wi1/F5+af+Oi1tzj4aBjhfdpyYm8cy+dH8ubbTrw1w7+c4UFLOJ3CRxGfcqC4Km6guVcgox+5jivJZbEDLLv5ZKMnE4KXE+RyjtTYGYSEzMMltvDdLVIFVM41xqXHIAZe/NDWh7Gj/EiYvpmk473pZ+N1stz6t8GAgXOX3RwtvFlqsDfcOPUR1Tdqhsq5RlzV/Q1r5wJr0+VKld7fnfEPmkZm2CKCBy6Flp3wHz0E3wOzyTEWdgoxuPcm0L1odnt3/CeMJGHwbLbsMOPtW7Qd7hxEaJQPmQdS2BK7iBeCTbwb2p+2gMHOREJcPJ4vvM2aEGdykhYREjKDiLYrCfK6TrdjTZdzl822XxfNacTOiiHTazIhJYYzMdhBzrZ1pIyeSNja1zEcjSd82mxCot1ZNsq9ggSvXp0P/HX2vPSMYoMGDTiZk0MHj064trk0sMl3SV9z6mROceDPsVlz7nvAq1LraeFwD+nZ/72qPLZyuLr31eXnFwCCGMftAAAgAElEQVTw/pIluLiU8cLmKkrDbDZz6MBBbrr5Zs6cPlNq2n/i4zlx4gRP9C0cVrKgMDkS4hOuKvBXloKLiQJt2tzG4cOXBh40GAxX9Bq0xXNDhpb5/Xtz51U+g5cpKMinZYuW/OvD1dWaRkXbpSx5Z/Jo4dziqvNUndJXxrCl39v4V9DQO1fO9xlrX+LlFefwHT2SoBFdcTkew9gxm4unu/g+jfeH84ia8gzhZwy4dvQhJGQIpW5V1NeXQRuMuLR1x6WtO57GbAaGWi/na+Hc0hlyTZgsyWz8LB2TeSHD/QuHp8VixmxOZEK/ePqFLmdsFzf6hazEe288G+NWEz4+hrjRcwgbWD0Xq+pgeHAikdGX9kxDhe87lGtmbX82JRMRspB0z8lEPnl110QpzZy6lKDgzRj7jyQweCIeLXcTPngqGeVVUg3lHdeFQ34a/V8nLqh6Xz593WvlgjO7yToOxReq7GxycMW15N35cvb3rC3rSDiViWF6XzYWfWfOM0NYX/ptm0hc6HXwEvaqYGs5SjWpiuPdhjSOp/FTnhGjnblwP69fAxCVIYN9+03c6nXZUEvNXWhlcKfrmEDuL2zO0vnpp+j8ybvs2g8+dnFsPOHNxMcLD4bmHfsx8RUz/xgfy8Y0f4ZZq3rd7MlTL3tWxw+qW5xdcTW44zVhCF4uAEY8Awfh+eFsUvaCb+VuD0h5VM61xtDKFWdSMZmw/TpZXv3b6IxLSzMZx00UPwxmySYzG5xaOt849RHVN2qGyrlGXNX9DbuKzwXY5VQ8Xa50Nft7y+6MmNWdERc/mzYQPMsND7dytqHRCRcjZJhygaLtYGfAycUNJxc3PDoayBi8kA+TehPk5YyTswHPwRPp51m4DV0efA7/LuuJ25UOXtfPPbxSaricrV0Xo0J7Fz0cmU1C2AxW5PYhZJZfiQcmjTi1dMDw4EiC+haNq3SHH889tpoxO5KB6tkOdf4dfyXl5+fToGFDWl0W4Lrzrg6czcu7prRvb/4ATRpfObyiNfaNm+Pq2PWq1nnHHXdgMBj4b8LVBRxtTSN2RQy3urow49VXiYleTlZWYQ+8o0eOcPDgQaYEB/P+kqW8v2Qpi5cupUePHiSUeM+fXSM78i2l7xKW9Z0tuj1wP5nHMtlfR99VB9C+Q0d++fUXfvoprVrTKG+7lOfEyRM0a9bsqvNUnQyWqxyiwZLGho934/bc64z170oZI8+REbeUlG5/Z9mylaxZtZzwkJFFjUzAzh13d0hLSS41FGXG98lkGd2567p9dNyE6bJTmsHecPXlXJZT2ZhKHcJm0vamg6sbroauTIhaSWz0cqKiCv/CR3hgaBNASOR7PFfimQCnjj4ETlnA/NFupH68gdTr6WlQOyNOTs7Ff8b6GkCudTbsz+Y01kyfQZxhECFT/NR7rIqkfLaONM+RhAz3w8PF9hvqVx7XRu5yd8W0I4kUjehSMZdO3OeUTurOS++8yPo+mczbPLjnYpC7gv3dpe/bxK5aWXzujVoyDf+WBjxHLyYyyI8bhi3lKNWkKo53W9LIJC58NQyYRegAiJsfQ1p9P79YMjmSCc1vvewq18KTzq3T2VfyHSLnCwOhBgNgBrPFXPrJYkcnmmPGbGsT2PIj/5r4HM+PGMHsTbaNLHPdaemJ523ppKaWKEezGa4Yokyuicq5ZliyybnsXQs5e1NJt3PDpSU2XCet1L/t3PHwMJBash2dlsKPeZ3w9DDcOPUR1Tdqhsq5ZlzN/Q0r5wKr0+VKVbC/52yLJ9W1O39sC2AmJ+eySnJWGj+dcsDFpTCOYTJdNt3egNGu8F2x2Lnh2dmZ9NT9XJqrqNfm9XzhruFytnZdLKzdm0hdMpXQJFfGvvE8Xpflw93TA35KLfUg9jmzGQxNKv/7bXRdBf4aNmzIn3x6XDGcZHNnZzrdd3XBt4vsGjbBs+1fKrlUA7q2G45dw8ZXtc6bbr6ZocOGsWzZUr7YuJHjx4+Tuns3q1eurHC5xo0NHP/1OAUFBVbTOHrkCB+uWs3oMWO4v1s3/ujlxbw5cwGI3x5Ps2bN6NmzJy4uLsV/jz3uz5Ejhzl06CAAt992Ozt/2MnhwxlkZh4r9ztbuLfvwP0PPMAn69aR+L//8csvWWRmHmPHtzvo8/jjfJ/yPQAnT57kyX79WBEdU+bn/fv2Yco1cezYUZISEytR6tZ1e6Ab3R54gNdDXuO7Hd/x22/H+e9//sO2bdvKXcbQuPBkefzX4zalUdF2Kc9P+3/CvX374s+m3FxysnM4c/o0BfkF5GTn2DxEbFUzd/HBq+VVLGhnwOEmSN+ZRNrxbDJSNhAR/glpl50ETQdTSTmaTU5ONjmnTJiLp7viP9gPw7Z5hK1KJO1oOqlfziN0ZSYezwzieu2xbvpyNsNHzSB2WyoZWZlkpGwgLHLz1ZfzFcwkLfgrY4KXsmVnOllH00ha9QbhW8B7oB8uGDC2dMXF5dKfq9EBGhlxcnHGaICMNW8QsmQzaTlmzKZ0UvfmFD5xo+CZXMbq/mzJZGPoVCKOdGXsxN4452SSdbTw7/L6q1SO0d4B88FkkjKyyUpLJC5sEVtKVmabGDBgIju7sBlX0XHt3m8I3ub1hM1cTUpGNqacdJLWxrDxYC39uFpkNhVej0x558ByDlNONjk5psKGlJ0HvQe4k7J8HnF7s8nZ+wkRH6bj+WRv3O2wvr8bnUude11cChvtBkdXXJyu04va1bBWjlKtruZ4NxiaQN6lG9XW0sjZOI9lR3wYPtADj8AX8c9dzfw16dX/42rT7zmcMBtodsvlT7q1xXeAJ/tWzGdLWg6nf88gflksyS188ekIdHkEH/sElkclcOQ0YM4iccUaklt4093Wh4Pt7uHZuStYtGwZkx+tr4/XuOE/uCupy+aycW82plPpbImIIamVH756EXUVUjnXhJwv32HM+BnEJqSRlZVJ2ralhC5OxrXvILyNWL1OWm9PGvDqF4Dz10uJ2JJOTlYysZHryPXuj29Lbpz6iOobNUPlXKsqbLtYOxdYnX5juqb2IOnETepLv6BPyALARGr0bCI2JpNxNJ3ULxcRsjgNr6GDCufPWEfI8PGErU0k7WgmGTs3Ez4zhjT3/jzdzQBZqwkeOp7QNYXTsw4mEjdrKUn23fHtZgAMeD49CNeERURsScdkyibts0V8kuaOt7dbub+xLqhT5Wz1umgmbc0Mgj804TvhRbzss4vb+RfbR8YHh9CPDYRFJZJ1ykRWymqiNmTj2cO72sqwzg/1WZPaNOvG3a37s+fYJzbN36n1AFxuufea1jn42WcxOjgQEx3D3DlzuO322+n12GMVLvPII48QGxPNo4/34tZWt1aYxry5c+nRsycdOxZ20Rn3wt8YMWQoW7/6ioT4eB7ucWUg9f4HuuHo6EhCfDzt2t3BU08PZO/evYwZNZpOnTszOyyszO9s1ffPf+a2225n9+5d7Pz+e24y3szdd99Dw8vyYdfIjgYNyv78w/c/sG3rVuxvvon27TvYvG5bvTZzJsuWLCH0jZmYz5/nrrv+wOBnA8udv3WbNnS8+27emT2L6BUrrKZR0Xbp+cgjV6R/7NhRzp830+6OOy7lMSSE5O++K/48cMCTAGz88ssaf2ojdPrVvpfFjX4vjCRt7kImBC7EyeNRnhv1PL7BMcVzuPv2xu3DpQQPXXop4Gdwxt3/RUIn+eD04GTenb6UiOh3mLA4F0NLd7xHzGLsk3X7AlYRY6/JhOQtJSp6BivSs8HeFXfvcYSOrar33xjwmvA2zy1fSmzIatJPGXC6sxO+U95jhLdt7+lz6dadtstXEzz0DXLMRly6PEpQUMAN8X4eqRxr+7N52yLCt2UD8YSPjS+xpDP9wj9mQpdayng94PHciwQeXUTYqGfAtSv+Qycy4tQkEi7O0KYrf7xzNWveXY3vspG4VXRct/QjeC4si4ghdMwicnDGvZsfAz1vhOH5SspmS8gAwnZc/JxKyMANYOhO8Nq38TVC2ydfJyT7TcKDniECVzwHvE5Q38LRKrS/266icpRqdhXHu5OnDx7LFxIWkYj3lO4Vp3EqkYhlu/EYsZzC1150ZcRYH4aHzSOux3v0a1OTP7YGmXI4jRFjGVWt5g+/zCumSBa/9RyLfzfSysOXcTOG08EA4Mmw115mZWQUrw57i9MUTp/42nA6691IpTj5TibUtJD5Ic8QfsoB1y5+BIWORJ0SqpbKufo5+U8jJG8hUUsmseKIGUNLNzz//DpBJd6PXdF10pb2pMFjCKGTTIQuHk1gjgNu3kMImeBTxlug6zfVN2qGyrm2WG+7WDsX6FxxuWtrD4IB7Arf91bIiPNdzmQsmcGYMDOG27riP+E9RvQoKuG2gwiZDhHR8wiKyMRs74q71xDCxg8qfEe3yyBCgmFZ9DyCFmdiwohbl0eZMOt5vC/2OGvTn5DpuYQtfImBs85hdOuKf8jrBF66xVwH1bFytub4BiIjkjFZYOPMYcXDgQK4j1hOxFA3MLgzIvTvLJuziDEDM8HJHa9nZjGhb/UNm9sg12wpsDZTQUEBFouFC+fPk5l+gM6dO1/TSnft2oWr2500atwYOzs7GpSM7tQBh08kkZL+AectZb93rXGjm7nfbSRtmt1f5nSRqrYyNpacEzmM/9sLtZ2VKxzcm8odHavr8c50YsdPJX3gPwnucfFEaCZry2xemJXDc7Hv0e8GfspIRERERGreqdMXajsLNwTHm/WcsoiIiIjIRfl5JuszFbG5Jt2gQQNo0AA7Ozvy8vKwt7+6xwvz8vIKe5g1aFDnAn4X3d7cCxeHTqTn/Jdff9/N6XO/AXBT05bc6tAJN+eHaGx3Uy3nUm4U5vPnWf/vz3g7bHZtZ6XmWTLJPJKLKSsHk6VojPJT6aTuSMN8px/31NeRikREREREREREREREroLNPf7y8/O5cOECuSdPcOb3k1gsFmuLlcnOzo6bbmmGQ7PmNGrUiIYNG9bZAKBIXZCRns727dsZMnRobWelTNXb4w9ydsQQEbWBpPRcMDTBYO+Eu1cAgcN646EXQYuIiIhIDVOPv5qhHn8iIiIiIpdUpsefzYE/gAsXLmC5cAGLxUJ+fj4FBQXF06yuqKiHX8OGDbGzs8OuUSMaNWpUPE1Erk/VHfgTEREREalLFPirGQr8iYiIiIhcUuVDfTZo0ICCgoLCITqBBg0bFgb8CgpsDtoVFBQUD+95Mfh3MW0RERERERERERERERERuTaVe8cf0KhRI/Lz8wFs7u13eRoNGzYs9VlERERERERERERERERErk2lxs642PPvYuDuWijoJyIiIiIiIiIiIiIiIlJ1Kj1ovgJ2IiIiIiIiIiIiIiIiInXPtXfdExEREREREREREREREZFap8CfiIiIiIiIiIiIiIiISD2gwJ+IiIiIiIiIiIiIiIhIPaDAn4iIiIiIiIiIiIiIiEg9oMCfiIiIiIiIiIiIiIiISD2gwJ+IiIiIiIiIiIiIiIhIPaDAn4iIiIiIiIiIiIiIiEg9oMCfiIiIiIiIiIiIiIiISD3QyNhYsT8RuTY6j4iIiIjIDePmRrWdgxuC2hgiIiIiIpf8nmf7vKpJi4iIiIiIiIiIiIiIiNQDCvyJiIiIiIiIiIiIiIiI1AMK/ImIiIiIiIiIiIiIiIjUAwr8iYiIiIiIiIiIiIiIiNQDCvyJiIiIiIiIiIiIiIiI1AMK/ImIiIiIiIiIiIiIiIjUAwr8iYiIiIiIiIiIiIiIiNQDCvyJiIiIiIiIiIiIiIiI1AMK/ImIiIiIiIiIiIiIiIjUAwr8iYiIiIiIiIiIiIiIiNQDCvyJiIiIiIiIiIiIiIiI1AMK/ImIiIiIiIiIiIiIiIjUAwr8iYiIiIiIiIiIiIiIiNQDjWo7A3XZ2X37OPPDD5z/5RcaNGhAo1tv5ab77qOpu3ttZ01ERERERERERERERESkFAX+ymDOyOCXefM4u3//FdOyo6Npevfd3PriixjatKmF3ImIiIiIiIiIiIiIiIhcSUN9Xsb0v/9x+OWXywz6XXR2zx4OT5rE6W+/rcGcSX11+PARVn/4UW1nQ0RERERERERERERErnMK/JVw7uBBssLCyD971uq8+Xl5ZM2ejfnw4erN07lzTAmeRg/fXvQb8DQXLlzA9zF/PtuwsUrX8+6ceXg95MM33+6o0nTLszs1legVsfxzYQRRy6OrLN0FiyJYGBF5zemkZ2QwfNRf8e7px7QZIexPS8PrIR8yM7OqIJfQw7cXH328DoCWLVsQu3IVWVmVS/v06dN8+tl6Roweg/+f+1VJvq7Wm+v2kwuAmS+C/fCbmVir+alfctkaEoCXzzhiM6s23X2fzmH8s33w8fHDb9BE3vk8HXOJ6Z++7IPXQ6X/xn+UXZWZkBtEdvIqZowbhF9PH3yeGMKkyESOWS6bPqZoesAoZqzaXXROkUrJ203s5CGF5ThoDl+brcy/ZwmDeo4j9kiN5K7+yMtkx/po3hwTgE/AHHZYLptuyebrhRMJeNwPnz5DmLQkmZJnzsrs78fWTcTnIT8mfW5tY9ZDVspRatexr+YwKsAPr559GP9RJsdWjcNn0EJ2XX48CHz9Bv169MD3ir++hJVoeplSPyFswjP069WLfoPHExaXhuniRHM6W+a8RGDfXvj7D2DszNWkmspameTuXseb4wbh19MPv6fGlWinSFVSOVe/Q9GjrmiL+UzfdKm9Zkt9o4L6N5g59PkcRj3lh49fAEOnr2JXORuxPtdHrJaTtfqI6iu2qXQ5Wds/r2F67nom+Vx5r8PrIR8GRRZ1Atm9kIDLpz9RRr2/jrHWzqh0uztzOxHBo+jj54fP44MYNWs9hy6eBqqkHHM59N/1RISMwq/nKKIOVHWJVJ/KXQcr3l/NmYlETS8sZy+/AIZOj2ZHyQPElv3x1+TiNHz6DGH8nE2XttV1yup18HKWTL6YU+I8s3D7Zde9XPZ9NJNRAX3w6dmHgHGz+PRAidSu0+O+InV6qM89e/bg6OhI69atK73s999/T9u2bXFycrJtgYICst55hwKz7UdF/tmzZIWF0XbuXGjQoNJ5tMWmLV+RlPQNkYsW4OBgpFGjqt9kBQUFbI9P4O6OHdm6bTt/fKBbqekLFkVy8uRJXvnHlAq/s9XnX3zJjuQUevh4c9cf7qSg4Jp/QrEmTZpUSXrLo1dQUAAf/msFjRs3IufEiWtPtISm9vY0bdqk8P+mTflz3z58vC6O8c+PsTmNY5mZfLlpM6dPn67SvF2Ng18lcuyJ9nSwq+2c1D+5iQsJTzTjUMXpmn+IYcaSdB6aOIdXOjlwbNMcXg2dTJPbVjOhE4CJXJOBDsPn8vbjzsXLNXF2LjdNkTLlbmfezHXkBkxiyXR3OLCON2ZOY6r9UqKHusH+JUwK3k67MdOJnulMbnI0M2ZN5x3XaF5/uKr3/Pote1M0kTudGbNwNj2bGWhhqO0c1VOHE/loUzK/5Zo5V8bkQ6umM3W7K0FhMXQjkfDp05nhvIQFAa6V29+zNxEelYbBvkZ+VZ1TYTlK7bLsJnbhOnIfmsnqwPY4GF05t762M1WHdZtIZOzIEl+YSf/3G4R+70M/z6KvDq4meEoM9JpIaJA77P2EyI2JZPZ1x90OUpfNIPwnH0Ki38PTkEZc6FRCFrgRO6U7OtWXcGAVk4Ji4LGJzJnSHvasZd56tVOqnMq5Rpwz5dLkgXFEBz186Ti3dy7+v8LrpLX6N2D+YQlTwpLpNmUhr99t4ouw6Uyd7cqqmQ+XbnvW5/qIDeVkrT6i+optKltO1vbPa5ru8CjT/tW1VD3evCeaSXMzGdinfeEXplxymz3MKwvH0e3iec3OSIu6fI6z1s6obLs7L5l3XpjNrocm8nZMV5x/jSd85hwmLXBj9aROGKqiHC3pbF27iV2mXMyW66jtX8nrYMX7azafh83ic4ehvLbUh9bnkomdOYupoc6sfrcPzmC9HM27CX95Ml84DyZo/kzuMCezbOEmdvz6KO1uq5kiqQ7WroOlmdm1cDLv7OzKKwun0+VCMhEzZzIJZ2LGdcIA5P53IS9F5/LsrFUsaQ+7oqcwadoSOsSMK9xu1+Nxb0Wd7vHn6OjIjh07Kh3YSE9P5/DhwxiNRpuXOb1jx1X13jt34ABndu6s9HL5+fk2zZeTk0OrVq3o0N6d1q7VU3H4cc8efs/NJfCZQWxPSLgibydOnrximbK+s8XBg4fYtj2eJ/s9wf899CAuLi64urpcVVplMToYueWW8i8WNpf7iRN07NgeV1cXWrRoUVXZK9a8WTOaN29e/PlR30fYsPFzCioRtXS/6y7+OW8Og54eWOX5q6wl84eqkVcdzLuJWhBP6yf8aFfF5Wu4dxRLYucy4ZH2tG7lSrfBo3jMNZsd36YXzmDJJfcMON/Wnta3uRb/OdfHBp9UL4eHmRa9hDlDu9PO1Zl2Dw1lhE8TDn1b9JRn+0G8HbmQ1wM60bqVKx0eH8eI/zPz9dbk8p/kkjKdO5kDLTvRrb0rrV3LqxDLNWsfwNtz5zJroDtNLp9m2c2adfu5f+g4nujkSutOAfw90J1da9azz0Il9vdcvo5cwq57A+h5Iz5vYa0cpZYVPhx0x71/op2rK87X0X2aWmEw4tLG9dKffRobvzTj/8Ig3O0ATCQsjyGt4yiCJ/jh0dYNj14vEj5nSNH0dL7/LhP3Xv3xdAKM7vQb6AM7U0jT8VBCLlujYtjfcRSvT3qUzm5udH58ktopVU7lXFNyc80YWrWjXYm2WGvnotqdteuktfo3Zv67bj2mh8fz4qPtaX1bV4aPC8D4n9V8WmqUmXpeH7FWTtbKWfUV21S6nKztn9c63YBzyePqNtjxaTzOQybxVFGQxJxrwuzoRoeS87k61O32lbV2RmXb3fZdGfHuEhZMepTOrs60vjeAvwd2Ivu/m4q2WxWUo10nhr87lwVT+lT5Pa/qU9nroLX90ZknQqKJCQmgm5szrds/ytjArpiTE9lZtGGslWP2+iV8lPMwQSGj6Hm3K+3u7cPrkbOLt8P1qsLr4OXMyaz53ETPUePpeaczzu0f5e8THsX06Wq2FoUwDn2TTO69fRh4twPYOdA5cBDdcpL5umgEpOvyuLeiygN/FosFi6VqrnCtW7fGzc2Nbdu28fvvv9u0TGZmJikpKXTv3h2DwfZNc3rH1Q9xeea776zOsz8tjUd6+bN33z4Chw3nkUcfx2KxUFBQQEzsv/hzwAAe6eXPxJf/zpEjRwFY9kE0iyIXk56RgddDPixeGlVm2hWlsTDifQYMegbz+fMAHDh4kIcefoR9+y69w3Drtng8772XBx7oRk7OCXbt3l08bcDTz/Dvz9bz6Wfr8XrIh5f+PqXM7wBWrV7Dy5OnsmXrNv4ycjQBAwfz+puh5OZeGofm68REXF1dueeeuytZyqUtilzMnPD5vDU7jJDX3yAx6RsAHB0ccXR0LJ7vk7h/88qMEDZ+/gV9nghg4st/BwqHyQx9eza+j/nT+4n+vP1OGGfOnAHghYkv8b/EJNZ98ileD/mQnJJSZh7KS6OgoIDRz48nZOabxfN+vO4T+vYfQF7RMLLOzk44O1/qjdqunRsFFJCennFN5VJn2OWy66OZDA3ww6dnAENnbbqse3UJ2bv5dP50hj5VOORknyHTid1dotphTueL+ZMZFNAHn5LdrXtOZ2tRmsf+u4SpwwOKh6ysL0PNHFq1kE8bBTDhcefLKmKFQ6qOik5ma+RkBvXxw8dvEOOjksnOTCQieAh9/PzwGzSZ2N3llYQBB4fSaZ6zgMH+4nnTRK6pCS2alXcezWVH9HSGPlE4zFfAC7P4qNx1yY3OUHpn45wZsDcWVaAcaO1WcroDzkYwV6IHvsChqFEMjtzPuQPRDPXxYdCSdLBks+OjOUwaEoBP0XAub65PryCgWvFxbU7fRPjLRUOJPjGKqVEayugKR5LZld2Obvdeujvm3KUTrpm72ZUNtu7v5h9imBfvyvBR3XG4EW8cWS1HqRpF9YnYZLbOn0iAnx9DowsfACr3eDdvZ0afyXyRa2ZriF+p+tgVqZeXhiWT2HF+9Jm5vbi+dmzdRPyemsXXN0BVIvXDGFK6DOE5j6I6ljmZhB1mvHr5UfbjkEacnCB9Zyo5Rd+YjmdjdjDicN3cJKsB5mS2fnuOBx/3o/LjBYnNVM41xEyuKbfUPYNSbLhOVlj/tuxn1+5ztO/qcemG5h+60sVhP7tKtMVvhPpIheVkrZxVX7FNZcvJ2v55rdMvY/5vNFGZfox90q34u1xTLjS73q6z1toZlW93O7u5luoB7GB0AHPZo57Un3K0orLXQVv2R4fSwSWz2Qw3GWlSVG4Vl2Mu/92+G4eHHuWhZlf5m+okK9fBy53MJPuMM61vu1SShnu7cz/J7Nhb+NnB2QH2J7PzYnsjO5PfcMK5qN9Yfdxfqzzwt2nTJjZt2lRl6d177700bNiQr776ioMHD5Y7n8Vi4ccff+Trr7/mlltuoWXLlpVaz/ljx646j2Yblz19+jRvz36XGa8E88WGf2NnZ8fK1R/yxZebefvNN/hw5QruuftuXp4yFYvFwrODn2b4sCHcdlsbvtjwb4YEPlNmuhWlMXL4MPIt+axctRqAhYve5899e9OhQ/vi5bdu384DD9xPM0dH2rd356ut24unrYqN5o8PdKNvb38Stm5m9ltvlvndRYlJ35D0zbcsCJ/HsvcXceTwUdZv/Lx4ekbGYdza3l6p8i3L82NGM2nCC/xjchAhM6bR3euPADzs83/4/N//lZr3xx/38OWmzURHLSVs1lsAvPraG5w5c4bly5awOGIhx4//xrz5CwB4642ZdLu/K338H+eLDf+mS+fOZeahvDQaNGjA1L8HsWnzFnbu2s3Zs2dZGvUBL/5tHPZNmxYuOy2Yu//AT1IAACAASURBVP7wh1LpdXBvz4979lxz2dQFuV8tZN7PnZgwaykLJnUld/1cIv5TdmUid288X5/zYPj0haz611yGt9tNeGh08ZNf+6Kn8eb/nBk7fx3xW1fzdoAbDvcOZU5R1+vcb+cwftp6zvWcyOKYpcwZ6sa+BZOY+lGVvhCv5mWu453VuTwxsfynZ3dFz2Frq1Es+XQzqya1Z1/UdIYGr8fh2YWs/2I1f787jci56zhkbV256WyNXMgXlocZ+OjFnsW55OadY0dkURDxqVFMjUwku2i7mBMX8mp0Nj1DV7P50yW8/qgrXNn3RaQ0Sy77Pp9DRKITfx7YvewhbC37+fqHc7Tr2P66frKqprULnMOC4e1p4jaYBZ+uZ8lQN8hOZusPTXho7Eyi/xXDewFGtobNYk05p8cKj+vs7bwxcQkH7x3Pko/Wsy5sEA6bpvPGp9f5ubaqZWeTjSstSj4V7+xKCzI59msZ85e1v1v2Ezt/PU2enXTdP6V51SpbjnJNDq2exUd2g1nw7w0sCXSr+Hg3PMy02On0tDfw4JR1bF43nZ5l1VMqSsPOlcCJQ3HevoSoH8yQl0hUbDrdRo3iwfreezAvnrgvzfj286F4bJqsdDLMbrh3LG+0Gmd8R43EbccbvDBlERsTPiF8eRpeg3vTtoayfV3ITOfQuXZ0uLu+70S1TOVcQ86Rewayt88ufMjziUGMClnFjosDL1XmOllW/duSzbHsJrQo2WXbzpnWzvDbr0WRmButPlJWOVkrZ9VXbFPp+rGV/fNap5fOHJ+vjcf58SfpVqLxmZtr4tyR9Ux9qg8+jwcw6IVZfLTnOns6yVq7utLtbjM7vt0N7p2444q6Xz0ux8tV9jpYqf3RTPYP63gnNo12TwUUDzdZYTla0tl3BNp1cq9n90+sXAcvZ2/EYJfNbyWnW5qA4Ry//VpYVu0CxvHUTet5acx0Ij7fTuyc1eQ+NpiLbzeqj/trlQf+mjZtStOi4EZV+Pnnn8nPz8fT05M9e/awceNGUlNTOXLkCL/++iuHDx8mJSWF9evX89tvv+Hr68vZs2fZU8kASoGNQ0CWqRI9HEeOGEaH9u40adKEs2fP8v6SZUx/5R943HM3LZyd+euoEZw69TtpP/1E06ZNadKkKXYN7Wjm6FhmuVpLo0mTJgS9NJEPolewPT6BH3bt4vm//rV4+Z/+n717j4uqzh8//krskMqsl1HWIQuyhEzIh4SK2MKiECobLS0ubK5sFK0ubgaVlzTMr6iJWhiupBuJQbSw8VtadyldTMRWl1YWL2AKrgpJsJmD1kGJMfT3x3AXmOEq4vv5ePDQmTPzmTPvOed8bufz+fz3NKWlXzHJxbiun+ukSezb/1n99jvvvJN+/frRr18/FEWhf//+LT5XZ+jQoSxbsohBgwYydOhQHnlkAmeLi+u3X6m6wo9+9KP6x1+cOEn89pZHMnbEwIEDGTiw6VyEV6qqiFy+DK12GIqicPjIUfILCngtcjmj7r6bu21s+H3YfP6x51MArKwG0b9/fxRLhSGDB7e4rqKpNO4ffR+BvwxgU+wfSPnzh9x7zz14e02vf/+IESNuSFc7XNvlawn2lIRDTTMry0lhxCzxx8XeFiffQHxsqyk+03LjsGZqGOteDsJzvC02oxwJCPTGpjyP3PMAKoVF5Win+OI5SgFFh6fvFDRFJVTfr0ODnt2JGage4awK9sDB1hYn3whWzdGRm5hI7i07YEjP7rhEyqYuIGR861m35tFQXvW3R2MBNtPccbGoxj4wkjmOGrDQ4unpDKdPUdhqHAzsXumL+4y5LM1QmBMVjk9dYdzCkRlPBzN/cRwZez4m8WVXLqS/SkSycSRA9aVKVEstNtZaNBodTn7BBDj2rWKG6Fpl6Yvx8vQleG0ONvNW88LElgvKZRnx7Px2CnN8bVvcLlqhaNAogIWCZkjt/629WRQVRsBUR+x0trgEBeFmWUTu0ZYLr22d18XpSRx0DGVVsCt2Wg1ae28WBo3hP3tzZNRfY7V3wDaZdEIBBYPxDvJmWjrey9LjSK72Z1HQbXwOtDOOonNUa18WzXPFZoCCYmH6fFeGGO9KthygQdPKzAAmrxn2QSx6EtLiUshKiSdLF8zCGdoW0+pLDLn7+dzClZ883LhVrBIVDUpJKmvn/4KZ0x9j9jMrSM5tuLoq2jE8NFqH5Td7iI3cxGdDZjHbre/Hq13USlSsUIpTWBHqXz+TSPN6iugkiXMP0eDiG0rIvEjid+5hz+YwHEriWRqVQRmYnU+2Wv6uNlCN0vT9tY/rRv/cTuWRVuNkKs5SXjFPe+Nk6vjs7PbG9DnsPjoMt2lNj3MbV3/mBy9gXVIG+1OimaPNI3bJpvopA28FpurV7a53n0lh+17wme3ODSWQPhzHG7Q3HzT3eCxKJNjLi1lhWyh0DGddcEOHbNtxNM7WpakpIGFZ7Uwb/qGsSCm4xWdCM5EP3vByZzzHG9idmEL+JQOG8jySo7ZwUAVD3ZxHA3U4jbNDa1FM2tpIYo+NYc5sZzPjfGu6sUejk3760592aXoXL17Ezc2NoUOHcu+993LkyBFKSkoYMGAAV69eZcCAAXz99dc8+uijjBxpnBxlypQpFBYWtutz7hw+nKoO7mP/dqwB98DohlFe/z19mqqqKsKeX9jkNVev/sCFC3pwMJ2eOWlMdZvCIxMmsHR5JC9FvMDgwQ0db3v3ZTNcq+WBB4z75Tp5Iu8lvc+JkycZ++CDZn+vOhYWTW/7GDhwEF991dDhc5flXahqw6XngftHc7dN+ycJeXvbO3xf/T3ff19N9fffM8PnsfpRf80NHTKEYcMa1tM7ll/Ad6rKjJ/5NXndD1evcvnyZQYNGmTy881J47lnn+EXvwwifvsOdsT/0WSaAwcOaPd6lr1FcuQmbFKi8KkdVq4M0DSM5LHQoBkI1VUtTQZgpJ7IJG3XfnKPFVOmVqCvGYaqAjoNLlMcif0wg6xARzy1eg7u+hd6O2/uU4CaAo6dAKeFzk1GDtlNdMYmMY/8cnC5Besp6oF4th51ZH5CKyOi6lhYNtzRo2hQLIBGp6AyUEGDSnU1tHzrj4JneCJOT5WQfyCdrS+HcWFlHItcNYAWt6Dg+lfauIayyD+b5zIzKZwTisO0UBYeWMya4Lns8vLFz88XT/tb685fQ3YU/q/n1E5ToeD2SiqrPKTzsrvY+EaS6FxOccEekuPDCLsUTVyIY9NDsySdNduO4xAS39AJLTrFcL6AXRkZHDx0irN6PforBhyqWm6J0LR6XqscO1GEeigaX6/opm+63xUVbqz83a4UBUvAYADq7kMyGDCgYNl8jdSWjvfzGcQmlOOzcj1OCtBHp9UyqT1xFJ2msbVvtK5KV5zv5qSh4DR3AT6Zi1maaE/INv/bYtrA47mHYdzvGNe4ymQB1BTw0ScTWLjsPV4eWc3x5BWsXPkWI5NXMd3qFMnL1vDfmW+y/QlbKo5/xNY34li2UsvmtbNamR70NlQbx7QMZxZFJvKqzsCxxEiWNquniE6SOPcYm2lBzKl7YOvBC/Ny2PdKJgfP+xJgZj7ZavnbUsESA037QIyPlQHKbVceaTVOJuMs5RWztLdcZ+r47Oz2RtS8HAo1zoQ0azdS7H0JqZsobYA9fuGhZAVEs/vQEjy9b4E2A1P16vbWuw1FJKxNpHzKEqJdb2z36bNxbEl780Fzj8f7g4hJ8qDsTB67EuN4bnElW9b7Y2dhIo7TwNJCZd9f9uMSsZ6Po7RcyNnC0lcjibVLZbnrLRpnTOSD1s1frcVvSSRl0VuI8I8Ha0f85gXjczoavZUGMJAbu5jYylDikzzQlOfx4eZoNoZHo9m2BE9t3zxeu7zjr6u51I5EA7jjjjvQ6/U4OTlxzz0N00UeOHCAioqK+o6/oUOH4urq2q7PGfDww3y3d2+H9nHAww936H3Xrl0HIGnHdmx0OhOv7ngaBoOB02fOMGjQIC5fvtJk277sbCouXmS6z0wArhuTI2vf/g51/LXkel2iwKhRoyj5smEdO0VR2rUWY53fzXuuE/tzDesRI/jrXz7s1jTKy8uprq7mjjvu4EqV6W7lqitVjGhHJ3Jv4vKDcc5knzZOu9ZueCtOC2dBogGfeaEsD3XG5ptEgp9tmC7YxisIz5QYtr4UyMYqBd1YD9ZFBWPXOJE+NP8yQNZfMim7BGsCvaibSLfaYKB4ri9ZwXHEB3fsetESZYgWmyFabOydsakMZEFCBnNcg1psfBturYVK1XjXkGJLQFQqniey2ZmeysZ5SaTNi2FLkH0L7+ydlEfDSUxuODKVVtczFF1CMa4pYGNrj4uVnllRiezyX49fXcFYzSP21S0UOy8hMaDrjvHbmaEgnrAlmWj8Q3kmMgIn6wI2BCymuLXGm1bPax2goPFdzZ4l7Stf3XasdWg5bpy2qK7iq9dzAR2ejSsnrRzvZZnpZF0qx/KVWfyt9rnqKgNEz8JrbwR71vv20Be5ycyNo+gmXXG+m5HGN0UUVmnQWFRjqDLQyl1KfUgJXxSq6NyaTcWk1aFTxjB54VwmjwSwYsKcQCb8eT2HT8J0i3Q+0ruz7GfGk2HYuJ+zLKqahSFJfHRyFvO7psp269PqsFHscYsIxq32suoSHIhLynqT9RTRDhLnm0ax1qGlALUS8/PJ1srfGi021tWcPa9C3a2mNXrK9MY6321XHmktTqbiXCXlFbO0t1xn0fbx2entjRSfLMJgG9TC1JXNaLTYaKC48ha45dFUvbrd9W49WdGRJKi+rNvo3eK375NxbE1780Fzj0cLBa3OFq3OFqcHFYoD4kjO8WX51BbKx03iOAztMIVH5oQT4GxM32ZqMH7jM/jwaDG43jptdKY0zQdbeIG1K/PfcGV+3WM1g4i1djxsp4CaSdpu8InzMB55OmfmREWinxtOcmYwnkEtnAt94Hjt8qk+u9O1a9fo168fNs1GiDk4OFBlRsdKW6ymTsVi8OB2v6//sGFYTZ7coc+8f/R9KIrC/v3/7ND7zU0j4b1EdLqRrI1aybsJOygrN47AO3eulNNnzvJa5HLe35HA+zsSSH4vAa9pnmTta1jnr3///tQ0m860pefM4TrJhbKyck6cONnu93aVsQ8+yP++/pqiU6e6NY31G9/kqaBAngoKZN36Dfzwww9tpllx8SJDh9yqt0g2H7Zuppoidn5YgF1wFAt9nbFpYcBYcXo8uROXkJqUSkZaEvFRofWZKxb2ONhDUV5ekyHsxXl5lGnG4HCL9h3MWJZEemoSKUm1f68/iZ1iz1NvxBPt30VDGFX1hmH/ygAFrlSi1gCX9MZ/6xkoPFECOjtsGhXmtGM9CFkWxzvzbMlPyyD/Vrob1EKDVqut/+tLC/j2LipqsyxaGahgWdPorjdDEcnLIkmzDGLdspYrE6L9cnemU+QcSnSoN0468y/SN57XGhzsdaiH/nULT6HcQ3SOuGiLOXa0YaqXsrw8ykc54lSXxbdxvNv4RZOeltpw/d8RiZ+1gsu8d0lc4t2jX+WmMieOopt0xfluThrlpMWkQkA0MbMhbVNiG1OT9xE15ZSUwrCRzXK5EROYMKqE48crG54zGABLY/naAIaa6qYjbgZrGYYBQ+eqwH2LtTMu95SQX9CohFtt7FDuUD1FtEzi3DNq9OibVdb0JwootrDDZgRm5JMmyt8W9jg5WpLfuB5dlMexK464OCq3UXnERJxMxVnKK+Zpb5xMHJ+d3l5P5WyxvrYzoTEDen2zQkl5EYWXNNjoevksR6bq1e2ud6vkb1vCazk6Fr6+ALcWj+s+GMe2tDcfNON4VNVmcRqooLEwrnFnMo4WdriM11JccKrRgAvjNLqWimXnvuvNZCofNIM+K5t8nStutgAGqn8AGjfNW2jRDgG1UqWvHq+3VMdfv379mDZt2g3TSQ4fPpxHHnmkc2nfdRfWYWHte9Mdd2D9+99zRwdLuIMGDSL0mafZ+k48f8/4mPPnv+FYfgFJyR+0+b4771Q4f/4brl+/bjKNc+dKef+DFH4f9jsmTZyI2xRX1m98EzBO8zl0yBC8p09DpxtZ/+c7ayZfnjvHmbNnAbj3nns4fOQoJV9+yVdlZa0+Zw4HBwcmTZrIh//vL/zzwEH+97//8VVZGZ//+xAe0x/jP3mHAbh46RI+sx5n+47EFh93xuRJE5k8aSLLIl/j34dy+eabb8je/xl79ma1+h7lTuNv/PX582alkfHJLs6dK+WpoEBCfjOXy5evkPyn1Db3q7DoFA86mDG/ay1VVdHrK7h8+TLXr11Hr69Ar68w+/1d6aC1NzMcO/BGC0s0A6H4aA6F5/UU52UQG5NOYaOGjWpDNeqZAnJL9ej1evSXVAz123X4zfFG2buJNck5FJaWkL8rhhUflOP0VBBut2jlU9HqsNE1+rO2whIFjbUObZfkNwYOxoUSFB7H7rwSysrLyc+MY0N6OXbT3HGwMHBwcyjBi+PZfbSEstIiDiZHsSETfhrohQ1QnBLF0m2ZFOoNGNQS8k/qjXfySeeZaEbdtZ6gpyNJ2FtAcXk5xXkZrInLpHq8B27WQE05O6OWEFvqzMKXfNHqyykrNf41L/+K9tEMsKL6dB4HSvSUFeWQFh3H7saFWUsFS1T0tSXqts5rB/9gPA0ZrFmZQm6JHlVfwsG0RHaeuUlf7iYyqMb8SL1igBoDql6PXq8aK1oWjvgF2POfhE2kndCjP5FObGoJj8z2xcEC08e7Rtv0+q8z3pSgDNFho71FM7WOMBVH0a06cr5bWipQpeeCal4a+owYtpa6Mz/QEafgCB5XU9iQUtL9X+5m+raCCoPCsMFWzTbYMjPImePbN/HJST2V35bw6dYkPrf2Yvo4YIIX0wfsZ9vW/XxZCRjK+Wx7Kp+PcOcnMtqvEVv8nnImP34TO0/oUS+VsDsuseP1FNEKiXNP0O+KJnheJAnZRZSVl1O4N54V2/LQPRGIpwaT+aTJ8jcKU/390f4zntjMEvTleSTEpVPp4Y+PNbdNecRknEyVR6S8Yh6TcSoh7XlfvMLTa9fuMnF8dnp7nQr0FaDVDmu6vyV/YUlwGGvScigsLaf4aCYbViZRZO/PnIm9+Pg3Vc8wWe9u/jsYKEyJJCJFxSc8ArcB+vrXN+2Q6UQcDWptO18l1VSjXqqtV/XqG8pN5YPtPJ7LU4iYE8aKFGOcys7kkLY2noMDp+AzUTEjjgouQUHY7I/jrcwSVFVP4c440ors8fS4Bdc+qmUyH7whzir5CdHEZuRRXFpC/q44lmw7hVtIoPE6o3FlxpRK0jbHc7DUAKgU74onrciOGe72t+55b0Kvn+qzJ1m5uTEsKIiKlBSzXq/99a8ZNHFipz4z+Ndz0Gg0bN+RSPTGN7n33nvwnTmjzfc85jWdhB3vMWumDyN//OM201j/xpt4TZ/GQ2ONtcKIF54n8Fe/JnPPp2Tty2b6NM8bOlInT5rIkMGDydqXzej77uOpoF/yxYkTzH36WcY/7MTmTW+2+Jy5/J/w49577uHosXwOHz7CIKtBOI576Ib9sOhvwR13tP64M6JfX8PWbe+wYuUqDFevYj/mAYLnzmn19aNG3c24h8ay+vV1pKV80GYaamUlm7e8ze9/N48BA4wTlke88Dwr/i8Kb69pLU7JWvrVV1w1GBg9+j6zv8Mrr67gUO5/6h/P8vs5AJ9l7enQ9KmdsXx9qHHu/3azJSA8lMKNcTwXGIfW0YuQeWH4LGno4HXw9sUuJZ4Xn4qnui7zV7Q4+IYT87IH2qlL2LIyntgd0Ty3TUWxtsczNJqFAbduBtf9FNwWrmdhwhaSo56l8LwBS2t73AJWsyjYOA2AW3g0ITvi2R6ZQvElS4bfPw6fZZuY71E7dcAkV+wSUnlxThQXqjXYjPdi+ZInb4v1eUT7aGYsZl1VPFsTIkko0cNAHfbuC4gJM67nZNgbx8a9eiCbDaHZjd6pJSAunUXjb9KO9wFOwRGElG5hzdOBoHPGLySc+ZfCqb/NZZQzbven8kF0Kj5JodzX1nlt7c2qzbB1SyKvPRvHBbQ4TPJmjvPtMD1fY3p2Rfqz5lDd4wKW+meA4sqqnevx0YDd7CjWVUSxISKQWHQ8EhDFq37GvF+Od/O1FUfRzTpwvmudPXBK2MKaOG88l7m2ncalf/FW/HGcQhNx0wA4Mz/Mg6D1MaRN20TAqJ78sj2oUk8lGqya9/sBw6YvZm1lHJtX/orYbzXoHvbi5bXPMk4BcGb++sVsj32Xl2avphLj9mXrn2WCrCHVhNZ7MW9WxrEhMpCNlzToxnt3op4iWiNx7n5a30jWVcWxdVsECaXVKNZ2uPitZnmj9bHbyidNlb8BFMdgYl5WWbHtWfz1Guw85rIu3KPtNeb7GHPiZKo8IuUV87QdJwUswLJRk6Cp47Oz2wGoUVFVsLRqdtTbBhG9Et5KiGHBlnIMA3XYu85ly/NBjdZE7n1M1TNeOG+qHtLsdzifQWxcHmoN7Fw5l52N3uEQmkRi3YJ+nYhjcXIEgfFF9W8pft6fZDT4vZHB8l48dXTb+WA7j2ddENGRsDUhhgXbylHRYDfei5c3LsBzCDDEjONxlD/rVqqs2RyO71oDGjtnHl8dRcjoHg5MFzKdDzaPswatvZazWyMJjq5GGeWMX/gm5k+rOy61+CyLpnpLHBvmzaLsCgy3m0LA6ujadf1uzfPelDuuN16ATQCg7t/P+S1buHblSovb+1lZ8ePnn8fKza2H90z0Ve8lvY++ooIXX1h4s3el3fLz83Fycuqm1EtImLeYs4FxrJpWN2mAgbLMaJ5bqyckdVMLC7oKIYQQQgjRfSqvXrvZu3BbsLrzlpqgSAghhBBCiG713Xffmf1aGfHXAo27O4Ocnflu716uHDnC1a+/BuDOH/+YgRMm8KNp0+g3aNBN3kvRVxiuXuWjv/6Nt2I2AnD58mUC5wS3+vonHv8Zzz0b0lO7d3PVlFNWWolarketqV2D7VIx+YeKqL7fGydZCEwIIYQQQgghhBBCCCGEqCcj/oS4yYqLS/g0ax/PhvzmZu9Kh3TviD/QH0rkrfgMDpZUgqJgOUCLvas/z4T4yoLZQgghhBCix8mIv54hI/6EEEIIIYRoUDfi78iRIyZfKx1/QohO6e6OPyGEEEIIIXoT6fjrGdLxJ4QQQgghRIP2dPxJSVoIIYQQQgghhBBCCCGEEEKIPkA6/oQQQgghhBBCCCGEEEIIIYToA6TjTwghhBBCCCGEEEIIIYQQQog+QDr+hBBCCCGEEEIIIYQQQgghhOgDpONPCCGEEEIIIYQQQgghhBBCiD5AOv6EEEIIIYQQQgghhBBCCCGE6AOk408IIYQQQgghhBBCCCGEEEKIPkA6/oQQQgghhBBCCCGEEEIIIYToA6TjTwghhBBCCCGEEEIIIYQQQog+QDr+hBBCCCGEEEIIIYQQQgghhOgDpONPCCGEEEIIIYQQQgghhBBCiD5AOv6EEEIIIYQQQgghhBBCCCGE6AOk408IIYQQQgghhBBCCCGEEEKIPkA6/oQQQgghhBBCCCGEEEIIIYToA6TjTwghhBBCCCGEEEIIIYQQQog+QDr+hBBCCCGEEEIIIYQQQgghhOgDpONPCCGEEEIIIYQQQgghhBBCiD5AOv6EEEIIIYQQQgghhBBCCCGE6AOk408IIYQQQgghhBBCCCGEEEKIPkA6/oQQQgghhBBCCCGEEEIIIYToA6TjTwghhBBCCCGEEEIIIYQQQog+QDr+hBBCCCGEEEIIIYQQQgghhOgDpONPCCGEEEIIIYQQQgghhBBCiD5AOv6EEEIIIYQQQgghhBBCCCGE6AOk408IIYQQQgghhBBCCCGEEEKIPkA6/oQQQgghhBBCCCGEEEIIIYToA/rf7B3ozU6eP83hr77g68oL3AGM1Ixgwt3jsB9x383eNSGEEEIIIYQQQgghhBBCCCGakI6/FpRc/Io3s9+l8JszN2xLOJTGQz9+gJc8Qrl78MibsHdCCCGEEEIIIYQQQgghhBBC3Eim+mzmYPF/eOGjVS12+tX54uv/8vv0lfz7y6M9uGfidnXuXCmpf0672bshhBBCCCGEEEIIIYQQQoheTjr+GjmjP8e6vVv5/odqk6+tuvo9az+N48uLZd26T9XV1SxZ9io/nf4YT/zil/zwww9M95nJ3z/+pEs/542Yt5g81Z1/H8rt0nRbU3D8OInvJ/OHuK0kvJfYI58JkPxBCj6zHu90OiVffklI6G/5iacXr65YSdGpU0ye6k55+f+6YC/hxUVLeOXVFQCMGDGc5D+l8L//NU378uXL7Px7Bs88N4+Zjz/RJZ/bUWr9/wzsXuaFV1TOTdybPqKqnNyMRNbM88fdP4bcmsYbOxDn8my2LgvF18sL9xmBhEZnUGxotL2mnN0x4fjP8MLddy4RcdmU1bSaWiepZK30Z7J7GMnljfdBz8G4RvsQn4e+u3ZB9Kw2j2cjtSCdNWGBeHl64RUQxpr0okbXFmGWqgKSF8/Fy9Md98AYDhpMvP5EPIGeYSSX9sje9R2mjmcT1zJ9Xgor5gUafyf/UFakFLR6rJelh+M+1YuIXaZ+zD5I8oRerWxvDKH+Xkz29GVBWjllKWG4B8aR321lh1vYwdU88dOfMv2Gv5+xsVHVq/L4R2xc+CueeOwxnghawMa/nqKybqOhhE9jXmTOzx5j5sxfMD8qleOVLX2YMGqlrCm6jJTbul9xYiiTp7o3+XOPzKS+RGBOeSOstrzhN5eIbTnN6ncGinfFEBrghbuXP8GRKeS38iP26fLI+Ry2Lp5rPJYDw9mQUYKhPdtNllfMj3Of1u5ynam4dfd2leIDGWxdGYqXZygJrY8N6V1M1FPaUw8xUilMiyLU3xd3T1/8w6LZeabZdeB8HgmRtW1NvnNZEJPZqK3JdJxNuGO30wAAIABJREFUpa/PS2RpiD/utedg7N7elLGbV94wlOfUx2iylz/BkYnkNjkBzDje2opzW+19agYR7u435CeTp7oTuK2oS6LQ/cws15lq9zR1vJlqF21Hu2lvyTd7dcffiRMnKCvrWMfakSNHqKioMPv1169fZ13W2xhqrpr9nu9/qCY6axvXud6RXTRL5qd7+fzzf7Pt7S28/Ye36N+/62dnvX79Otn7P2Psgw+StS/7hu1b3t7GmtejTT5nrl27/8FHO//O6PvuI+DJnzPjscc6lE5H3DXgLu66665Op/Ne4vtcvw5//uB9whf+vgv2rKkBdzXs51133cXjP/Pl/6X/tclrysrL+UfmHi5fvtzln99eG7Jvx5JrNzuXQ1pmHmdVA6ZvRTChKo8Nz6/n4IhA1iWlkhIdxPBDMURsKaittBjIj1vMhmO2LIxLJT0mmOH/iiJiWwHdkUWpOXHE5hjQNHu+OCWSpdlaQjYmkRL9JJYZkaxI702FOtFhpo7nMylEvBzP2TGhxOx4l5jQMZzd27xxQpiiz0xk2zEtIXGppGwKxkW52XvUR5k4ntu8lhXFE7EsA2ZEkpiayjvzxlC4LbLlfFSfSWzCKZQB3fptei3JE3qxmgKS49JRp0aS+kE8q3x0N3uPejeXcLYl/4nk+r/3WBs0BqsHA3liQu1rzqaybEk8Xz7wLGvj32HtM/Z8uS+H8tp88Pj2FcT+dxwvJ/6DTz5cx8yqP7NyS063lNP6gtbKmqKLSLmtR1RXqlhODCM1NZX02r+UcA/qindt5pNqNm9FpaO6RhD/QTqJSzxQ019laXJJffqGo/Es2ZiHw7w4UnZE4qkmsXR99o2dAH25PFJTQkJkFLsHBrElNZX4MHsKN4ezuq5cZmo7pssrZse5j2tvuc5U3Lp7OzUlZP0lk/xyFcOtdG1rq57SnnpILfVAHC8mqniuTWH/nhRWTSwh9tV4CutiYigg9qXFpKmuvLw5icS1QWhPZpJ7vnaziTibTF+fwerIDJTATezZ8zGpEbbkro/qNTeumlfe0LNrYzS7LHz5v3dTSY8Lw6k4kaVrMxo6v00db23F2VR7n8abVz9oyEfSU1NJXemLzRBnZvvad2k8uotZcTbZ7mnqeDPVLtqOdtNelG/26o6/wYMHk5ub2+6OjZKSEs6dO4eVlZXZ7/n3uaMdGr13Wl/C0a9OtPt9165dM+t1FRUVWFtb42A/Bhtd91Sqvzhxgu9UlTm/CiT7s89u2LeLly7d8J6WnjPH2bPF7Mvez5NP+PHoVDdGjhyJTtdzayUOHTKEoUOHtrrd7N/l4kUefNAenW4kw4cP76rdqzdk6BCGDh1S/9h7+jQ+/mQX1683dDKPeeAB/vBWDIG/nN3ln99eB7PypOGhq9n7s27TJqJnj8Gys2kNcOaZN+LZEuGNk06LzXh/Fs1xRH8g05jBGfL4cFclnqEL8BytRWvvzaKF3lTuTCWrY6d66wwFJGzZj42fF3YWjZ6vKeDD9CIeCQ7Dz1GHjaM/i+aMIf/DjC7eAXFTtHk8q2QlJFH0YCirIrxxsrXFaUYE8ZuDcbBoIS3RqupLFTDCERd7HTY6LdLv103aOp7buJYV1gD2gazbFscqf0dsrHU4zAjjmUcNLeSjKge3xZM/3h9PbU99sV7EVBzFTVaJWqlw3/gp2Ol0aKV3pW2KFSPv1jX8DTjFJ/8wMPP5QMZYAFTy2XtJnHowlGULvRh3ry3jHnuB2Ji5tdtLOPKfcsY89nMmDAOsxvDEbHc4dphTcj7cqLWypugiUm7rKapqQLG2w26UDpu6P21t6c5UPqnx4NXEeGKCXbHTabGbGswz7pYUH6obZWXgQHoGlR4LeMHbHptRzoSE+WP1z1R2NumL6ePlkRMZ7Dw9jpBwXxystdh5hLEowIp9qRmUmbPdZHnF3Dj3ce0u15mKW3dvBywcCXljE1uW+N5aeUlb9RSz6yENiv+dhzrel9ljNWChwWlOIC4VeRys7XjTZ8STVuHByytD8Ryrw268L6u2rSdgFJgTZ1PpU5DHfxRX/LxtUSwUtK7B+I0pIjevF3Sdm13e0OK3MpGklf642Gqxsfdm/hxnDHk5HKsLvInjrc04m2rvQ0HbOB8ZBbk796OdG1H7O/Vy5sbZZBxMHG+m2kXNbjftXflml3f81dTUUFPTNTUQGxsbbG1t2bdvH999951Z7ykvL+fw4cO4urqiKOY3eR06l9/R3eRQ6TGTryk6dYppj83kZGEhc34TwjTvGdTU1HD9+nWSkj/gcf9fMO2xmYS/tIjS0q8A2L4jkbe3vUPJl18yeao777yb0GLabaURt/WP/CLwVxiuGkcynjl7lqke0ygsbBjOm7VvPxPGj2fiRBcqKi6SX1BQv+0Xv/wVf/t7Bjv/nsHkqe68uGhJi88BpKR+yEuLl/Jp1j6efvY5/GcHsWrNWlS1YR6agzk56HQ6HnpobDuj3FRS8gfEbf0jqR+mUVFxEYBzpaX87vcLCQn9Lb/69W/wDwhk5uNP4Ok9gxUrVwGg1Q5Dqx1Wn05K6oe8vOQVPv5kF75+/oS/tAgwTqO5dt16pvvMZJbfz1m3YSNXrlwB4PnwF/lXzuekf7STyVPdyTt8uMV9bCsNU7+LdpgW7bCGK4SdnS3XuU5JyZedilt3MRiaFRMsVPLTogj298Ld05/g6MzW7wDVF7BzcyTBAb64u3vhOzeS5IJG6RlK2L15MYH+vrg3HpbuGUlWbZplB+KNQ//djUP/b5upZtoRZ62trsndMRorDRhq7wK7VI7+ihabUQ3XTGW8K4+QR+5J4+OylDDcF6eTvyuGBYHGYfGBkekUXiph9+ZwAn2NQ91X7CxpsxO4OCWOnf39WThD2/R1pXnk6+1wGd9w3GsfdkRXXnBDGqKPMeSRdagatxle2NzsfbmFFSeEErStiOoziQS7uxMYXwI1enLTYoiY64977XQua5pPT9SESm5iJMF+xun7/J+PJq2g0WTOJZnEvlQ7lahfKEsTZOrFG7RxLcvXA2iwsW1yNUZrdWM+ajiaxFv7dYSEuqK5HRv2TcZRdA3j1OGhyXlkbQ7H38uL4ETjaJBWz3dDNit8F7NbNZC10qtJeeyG1FtLo6ac5DAvfKMa7vguSw/HKyCag7dBAe74n5M4/PBcfj2uttxlyOOzXAOTH/Oi5dshrRg2DEqOHaduPpvKb/QYNFZobqXGyB7SallTdA0pt/UQA2ql2qTtogkz8klF0/SujGoDMMDKeGNYTRH5BdXYO49ruFHsfmce1hSR36gu3tfLI4bycvRD7LivUagcJjqjLcojXzW93eTvYGac+7z2lutMxa27t/dZ5tVDmrxDq4GiPI7Vlc/05VxgGForAJUD2QVopnozdUgLbzYjzm2nD2i1aC8dJ/907T7W6LlwSYNG2+nb4zutXeUNjabJTbkGgwEGWmFpVjnORJwx0d7XjOFAIgnlXsx/0tacD7/p2hNnU3Fo83gz1S5qRrsp9L58s8s7/jIzM8nMzOyy9MaPH0+/fv3Yu3cvZ8+ebfV1NTU1fPHFFxw8eJAf/ehHjBgxol2fU/Ztx9dm++rbr8163eXLl1m3/g1WLF/G7o//hoWFBX9K/TO7/7GHdWtW8+c/vc9DY8fy0pKl1NTU8FTQLwn5zVxGjbqb3R//jblzftVium2l8WzIb7hWc40/paQCEPf2H3n8Z7NwcGgYzpuVnc3EiY8wZPBg7O3HsDerYbrPlOREJk104WezZvJZ1h7Wv76mxefq5Hz+bz7/9yG2xL7F9j++Tem5r8j4ZFf99i+/PIftvfe0K74t8fH25jdz52Cjs+GDlFSuXbvGPaNG8fYfYkmI/yN/ev890tNS+eRvfyUrcxerVhrXyxs7diyvLFnUJK2CguPsztxDYsK7bIx+HYDX/m81V65c4b3t8byzNY5vvrnAW5u3APD66ihcHnHGd+YMdn/8Nx52cmpxH9tKw9TvMvsX/jzx+M+apOcwxp4vTrR/dGlPsHvQvkkmpu6N463TjiyMfpctEc6oGZvY+s+WL9Hqyf0crB5HSGQcKR9sIsSugNi1ifV3ZBQmvsqaf2mZvzmd/VmprPO3RTM+mJi4MFwsQD0Uw4JXM6j2DOedpHeJCbalcEsES9P6/i107YlzUwZyDxXAGEfus8BYAbTQc6HxXSo1lqBUc+F8QwtcdU48GwocWbQjgz3vzmX4v2JYEBJN7ugw4jM+JjFEx8HYLexqrVG2PJ0NqSp+4S3cEazXo0fH8MZ3xGh1DKfv/463vfISiqvtcBgrQ0Y6w25ODFtC7LG0DWLLzgzig21Bn0fWUUumzo8i8YMk3vS3ImtjNB+2cloZcuJ4LVGP59pU9uyMZ5W3DuruFdVnszo8nrPjFxCflkH6xkA0mZGsvq1uVzZDG9eysvMtvL6miINHq5vmozVFJG/OwPKpW+QuzO7Q3jiKTilOjSbNIogtf/uY+Dm2bZ/vigevJkfiOUDBbUk6e9Ij8Wyp0aKtNCx0zAkPRpsdT8JRA1TlkJBcgktoKG59PSuo2s9f/2Fg+hPu1M9N878SvjTYMubB1mar0TI99Flsc1fz/JK3+eSzj4h97xSTg2Zxbw/t9i2jrbKm6BpSbush1ahXQJ+93niDpV8goStTyK2rr7Unn6xRKdwVw9acYTw+29XYKFqjp0xvyfDGQ7YttNho4cL52srcbVAeUTRWKJV6LjS+79jCEqWmgrIK09tN/g7mxPl20O7ysYm4dff220VL9ZBm7PzDCBiYwYvzItm6K5vkmFRUnyBmaIGaEgpLwc5xTMvvNyPObaYP4BjIQm+VhIVhrEjMZHfCJv5m6c8c15s8t02HyxsG9EfT2ZB8CrsAf1zMea+pOLfwGU3a+5rQs+sv+9HOePLWWBakU+W6G+PQ5vFmql3UnHbTXphvdnnH3113dc0aanVOnz7NtWvXmDBhAidOnOCTTz7h+PHjlJaWcv78ec6dO8fhw4fJyMjgwoULTJ8+ne+//54T7ewgqblu3hSPLb73mvlduM8+8xsc7MdgaWnJ999/zx/jtxO5/BXGPTSW4Votvw19hm+//Y5T//0vd911F5aWd2HRz4Ihgwe3GFdTaVhaWvLyi+HsSHyf7P2fcTQ/n9/99rf17//vf09TWvoVk1xcAHCdNIl9+z+r337nnXfSr18/+vXrh6Io9O/fv8Xn6gwdOpRlSxYxaNBAhg4dyiOPTOBscXH99itVV/jRj35U//iLEyeJ397ySMa2WFuPYNCgQfzkUTeuVFVRVm5ew6Ny550M1zYda3v5yhVee3U5Wu0wFEXh8JGj5BcU8FrkckbdfTd329jw+7D5/GPPpwBYWQ2if//+KJYKQwYPbnHdRVNpmPpdNBoNVlaDmqSpHa6l4uLFdsWpp8zxbXqniOWkMGKW+ONib4uTbyA+ttUUn2n5N9JMDWPdy0F4jrfFZpQjAYHe2JTn1c4JrlJYVI52ii+eoxRQdHj6TkFTVEL1/To06NmdmIHqEc6qYA8cbG1x8o1g1RwduYmJ5Pblm7VoX5ybOJPC9r3gM9sdLYDGGc/xBnYnppB/yYChPI/kqC0cVMHQ6J4aS2tfFkV4YzcAlNHeeNorMDGY5b72aFCw8/LAoeYU+S0ufK1nd1wiZVMXEDK+hRJG7V04TQZqK6DIvdp9n1qJihVKcQorQv3rR/4mHLqNKl1dQdGgUQALBc2Q2v9be7MoKoyAqY7Y6WxxCQrCzbKI3KMtD6mpvlSJaqnFxlqLRqPDyS+YAEfjSVmcnsRBx1BWBbtip9WgtfdmYdAY/rM3R0b9NdbGtay6hctZWUY8O7+d0iQfLUuPI7nan0VBt8ZdmN2inXEUnaNa+7Jonis2AxQUC9PnuzLEeNey5QANmiEttxqYvGbYB7HoSUiLSyErJZ4sXTALZ2hbTKsvMeTu53MLV37ycKO4qZWoaFBKUlk7/xfMnP4Ys59ZQXJuw9VV0Y7hodE6LL/ZQ2zkJj4bMovZbn0/Xu1joqwpuoaU23qIBhffUELmRRK/cw97NofhUBLP0qjaKSbNzCfL0hfj5elL8NocbOat5oWJtQ3w1QaqUZq+v/Zx3eif26I8Ms4VNyWb5MQ89AYD+oJ0Vr+RUb++qsntpn4HM+J8W2hvuc5U3Lp7+22ipXrIDQbqcBpnh9aimLS1kcQeG8Oc2c61HVCVqJWWaGoKSFhWO8ODfygrUgqMMzqYE+c20wfQcN94R+yGVVP4QRQrEsp55Cnfm3xzTwfLG0WJBHt5MStsC4WO4awLbr3DtSkTcW6ueXtfk13PYffRYbhNuxWu650s17UUh7aON1Ptoma0m/bGfPPGHotO+ulPf9ql6V28eBE3NzeGDh3Kvffey5EjRygpKWHAgAFcvXqVAQMG8PXXX/Poo48ycqRxcpQpU6ZQWFjYrs8ZMaiVKRTMea+V+ZWuB0bfX////54+TVVVFWHPL2zymqtXf+DCBT04mE7PnDSmuk3hkQkTWLo8kpciXmDw4IaOt737shmu1fLAA8b9cp08kfeS3ufEyZOMffBBs79XHQuLplffgQMH8dVXDR0Rd1nehao2XJoeuH80d9u0f5KQ7Tve4/vvv6e6uhqDwYCqqpwrLWXtuvV8//33fP99tfHfauP/fzLVrX7UX3PDhg1j2LCGdf+O5Rfwnaoy42d+TV73w9WrXL58mUGDBjVP4gbmpNHW79KSgQMHtHu9y57i0+wUUAZoGoZXW2jQDITqqpYGmRupJzJJ27Wf3GPFlKkV6GuGoaqAToPLFEdiP8wgK9ART62eg7v+hd7Om/sUoKaAYyfAaaFzk+HcdhOdsUnMI78cXHrP9bbLtTfOABiKSFibSPmUJUS71r1bi9+SSMqitxDhHw/WjvjNC8bndDR6q8Z3ZzX5dJQBYGnRKAMeqKBppfCuHohn61FH5ie4trwgr6JgCRgMQN0CuAYDhm5cpcyQHYX/6zm1w/4V3F5JZZWHNBT1OAugpoC0DGcWRSbyqs7AscRIlkZuwiYlCp9WprMQ5jGcL2BXRgYHD53irF6P/ooBh6qWK7SaaaEsPLCYNcFz2eXli5+fL572GkDl2Iki1EPR+HpFN33T/a6ocGOl4nbVxrXMsvni3iXprNl2HIeQ+IZ89HwGsQnl+Kxcj5MC9ILpQW6K9sRRdJrG1r7Rmhldcb6bk4aC09wF+GQuZmmiPSHb/G+LaQOP5x6Gcb9jXOMyVW0++NEnE1i47D1eHlnN8eQVrFz5FiOTVzHd6hTJy9bw35lvsv0JWyqOf8TWN+JYtlLL5rWzWpke9PZjsqwpuoaU23qMzbQg5tQ9sPXghXk57Hslk4PnfQkwM5+08Y0k0bmc4oI9JMeHEXYpmrgQRxRLBUsMNO3jMD5WBii3T3lE48GiqHLWbIzEP7karaMvIU/7UxZVgMbKjO2mfgdTcb5dtLdcZypu3b39dtBSPeQGBnJjFxNbGUp8kgea8jw+3BzNxvBoNNuW4DkELC1U9v1lPy4R6/k4SsuFnC0sfTWSWLtUljubirOJ9LWg3xtNRIoVy7cl4dK/hKzEGDauDSdW+y4Lb9JNPh0ub9wfREySB2Vn8tiVGMdziyvZst7frDUk24xz49GPLbb3Ndr3vBwKNc6E3ALtpJ0q17UYB1PHm6l2UaXt7b003+zyjr+u5lI7Eg3gjjvuQK/X4+TkxD33NEwXeeDAASoqKuo7/oYOHYqrq2u7Pme8zVj2nDrQoX0cb9P+DjKAa9euA5C0Yzs2Ol23pWEwGDh95gyDBg3i8uUrTbbty86m4uJFpvvMBOC6MTmy9u3vUMdfS67XJQqMGjWKki8b1qlTFKVdazHWeebp37T4/Nt/iG3/DjZz/fo1rEeM4K9/+bBb02jrd2lJ1ZUqRgwf3uF9utlaXSw4LZwFiQZ85oWyPNQZm28SCX62YbpgG68gPFNi2PpSIBurFHRjPVgXFYxd40RkKp96bd+fpicrOpIE1Zd1G72bNtxZuzL/DVfm1z1WM4hYa8fDdl1RmFLJ+ksmZZdgTaAXdZMDVxsMFM/1JSs4jvhpOrQcN071UVcI0eu5QMeujeZQHg0nMbkhYkoroxZEN9PqsFHscYsIxq3253YJDsQlZT25J8Gnfdm5aMRQEE/Ykkw0/qE8ExmBk3UBGwIWU9xaIVSxJSAqFc8T2exMT2XjvCTS5sWwJUgHKGh8V7NnifwgbbJu/Vrmad3odWoesa9uodh5CYkBDde5ssx0si6VY/nKLP5W+1x1lQGiZ+G1N4I963176IvcZObGUXSTrjjfzUjjmyIKqzRoLKoxVBmgG2/26R1K+KJQRefWbKomrQ6dMobJC+cyeSSAFRPmBDLhz+s5fBKmW6Tzkd6dZT8zngzDxv2cZVHVLAxJ4qOTs5jfNVW2W57JsmbwLdDKdSuQcttNo1jr0FKAWon5+aRiXNPLxtYeFys9s6IS2eW/Hj+NFhvras6eV6GuSbVGT5kehltrb6vyiGZ8EOuSguofGw5Es01ji90QM7ZXmfgdLNqO822jveU6U3Hr7u19XSv1kBtfl03abvCJ8zC2HemcmRMViX5uOMmZwXjOHoZ2mMIjc8IJcDbG0WZqMH7jM/jwaDFMNBFnU+kHKWSlZWPzZCouGgBbPOetx1IfyNLkbBaO9+7GILWuw+UNCwWtzhatzhanBxWKA+JIzvFl+VRT5V8TcXatW8qrjfa+WsUnizDYBrUwBWjv0/FyXStxMHm86Uy3i7axvbfmm72+46+xa9eu0a9fP2yajRBzcHCgpKSkU2n/ZPRE4j9P5dvv27eivHbgEKbYOnfoM+8ffR+KorB//z8JCpzdbWkkvJeITjeSV4J/zctLluHtNQ0bnY5z50o5feYs//daJOMbrVP3h7i3ydqXTdh849ST/fv3p6amaSthS8+Zw3WSCwnvJXHixEnGju2dtdSxDz7I21+/Q9GpU9iPGdNtabT2u7Sm4uJFxj/c8nqCt6yaInZ+WIBdcCoLfWsvx980fUlxejy5E5eQ/nIL55mFPQ728EFeHqqvR/1dIMV5eZRpxuDQfX1GtyCV/G1LeC1Hx8LNC3AzcTeuPiubfJ0r87ukjUTDjGVJuDTulSxNJ2JxHp5vRDF7tA4Gqrho4zl2VA+2xmOhLC+P8lEdu76axUKD9jYo1/d61s643BNPfoEKuobph7hhWhDRXrk70ylyDmdPqLexkdnMbFs71oOQsR74jA4jKC2D/NkRONjrUDP/Ra7B9dZYD+Bm0Tm2ei1zqrvuGopIXhZJmmUQW5Y1rZTZ+EWTPq3RxbKmiITnoygLfJfl3rdRpmZOHEU30XTB+W5OGuWkxaRCQDQxNdEs2JSIz7ZQHPry9aWmnJJSGDayWeFjxAQmjIrn+PFKZo+sXefPYAAsjflgFRhqqo3X8LqGmsFahmHAUNVzu9/bmSxriq4h5baeUaNHf0VL46Wx9CcKKLaww2YEMNBUPqmiVmnQNBpNpQxUsKypHX1jYY+ToyU78/JQ/Wrr0UV5HLviyGxHBZuJt2t5ROVAZg64RvJwiw3jzbabKq+YiPNto73lOlNx6+7tfVkb9ZAWXkz1D8APjZ6y0KIdAmqlChZ2uIzXsrvgFIYZddNzGmeAslQswcLORJxNpI+G6hqoblKHVdBqraC8svOx6KCOlDdU1YBG02y2LAvjWq4mmYqz8RPMaO9TOVusr72JpPfrWLmurTiYOt5uTNNUu2jj7TYjeme+2eVr/HWnfv36MW3atBumkxw+fDiPPPJIp9K+q78lzz/a8iiy1tzBHbzwkxAUizs79JmDBg0i9Jmn2fpOPH/P+Jjz57/hWH4BSckftPm+O+9UOH/+G65fv24yjXPnSnn/gxR+H/Y7Jk2ciNsUV9ZvfBMwTvM5dMgQvKdPQ6cbWf/nO2smX547x5mzZwG49557OHzkKCVffslXZWWtPmcOBwcHJk2ayIf/7y/888BB/ve///FVWRmf//sQHtMf4z95hwG4eOkSPrMeZ/uOxBYfnzxZiKpWUvrVVxw4+K92RN20yZMmMnnSRJZFvsa/D+XyzTffkL3/M/bszWr1Pcqdxkvv1+fPm5VGW79LawqLTvGgQ8P8r6qqotdXcPnyZa5fu45eX4FeX9HZr98hZaXlqB2ZCt3CEs1AKD6aQ+F5PcV5GcTGpFPYKGOvNlSjnikgt1SPXq9Hf0nFUL9dh98cb5S9m1iTnENhaQn5u2JY8UE5Tk8F4XYLl9kMqvH7qlcMUGNA1evR69UOrnhnoDAlkogUFZ/wCNwG6CkrLaestBy9CqCSnxBNbEYexaUl5O+KY8m2U7iFBHbZ3OmKVoeNrtGftRWWKGisdcYKrYUjfgH2/CdhE2kn9OhPpBObWsIjs/vO3aS3s7aPZ1v8nnImP34TO0/oUS+VsDsukYPW3sxwvMk7fovTDLCi+nQeB0r0lBXlkBYdx259o6uIpYIlKnrjhYDilCiWbsukUG/AoJaQf1JvvEPXAhz8g/E0ZLBmZQq5JXpUfQkH0xLZ2eKann1bm8dzG9cyBwugppydUUuILXVm4Uu+aPXl9ddj1QBotE2vlTotGgtQhuiw0d7CmVp7mYqj6FYdOd8tLRWo0nNBNS8NfUYMW0vdmR/oiFNwBI+rKWxI6dyNnL3etxVUGBSGDbZqtsGWmUHOHN++iU9O6qn8toRPtybxubUX08cBE7yYPmA/27bu58tKwFDOZ9tT+XyEOz/pnfdR3hQmy5qii0i5rSfod0UTPC+ShOwiysrLKdwbz4pteeieCMTTRN3JwQLUXesJejqShL0FFJeXU5yXwZq4TKrHe+BmDaAw1d8f7T/jic0sQV+eR0JcOpUe/vhYc/uURwxFpG2MIS2niLKSArISoth4aAzzgmsb101tN1leMRHn24XJOJWQ9rwvXuHpxjVC9Ct1AAAgAElEQVQsTcatu7cDBrW2/amSaqpRL9WW93vJ9H2tabOeYqoe0vx30LgyY0olaZvjOVhqAFSKd8WTVmTHDHd7QMElKAib/XG8lVmCquop3BlHWpE9nh62mL7OmEpfh6e3I8XJ0SQf1WMA1BPpbN1VgdO0mze83HR5o1kcy1OImBPGipQcCkvLKTuTQ9raeA4OnILPxNrraZvHm6k4m2rvq1OBvgK02o4vddaT2h1nU3EwebyZahc1sb2X5pu31Ii/7vbofS7McX6C5Ly/mvX630x8kkn3ju/UZwb/eg4ajYbtOxKJ3vgm9957D74zZ7T5nse8ppOw4z1mzfRh5I9/3GYa6994E6/p03iodnRdxAvPE/irX5O551Oy9mUzfZrnDR2pkydNZMjgwWTty2b0fffxVNAv+eLECeY+/SzjH3Zi86Y3W3zOXP5P+HHvPfdw9Fg+hw8fYZDVIBzHPXTDflj0t+COO1p+nHf4CJmf7mXgoIGMfdCMxRDbKfr1NWzd9g4rVq7CcPUq9mMeIHjunFZfP2rU3Yx7aCyrX19HWsoHJtNo63fx9pp+Q/qlX33FVYOB0aPvq3/ulVdXcCj3P/WPZ/n9HIDPsvZ0aPrUzvAPDCMgLp1F7T4dbAkID6VwYxzPBcahdfQiZF4YPksS61/h4O2LXUo8Lz4V33Cnj6LFwTecmJc90E5dwpaV8cTuiOa5bSqKtT2eodEsDLiVp/PRsyvSnzWH6h4XsNQ/AxRXVu1cj097Gy/OZxAbl4daAztXzmVno00OoUkkhtiitddydmskwdHVKKOc8QvfxPxpPdtKYjc7inUVUWyICCQWHY8ERPGqX1++o/R2Yfp41nov5s3KODZEBrLxkgbdeG+Wrw81zo0uOswpOIKQ0i2seToQdM74hYQz/1I49bexjHLG7f5UPohOxScplPsmuWKXkMqLc6K4UK3BZrwXy5c8aVx3y9qbVZth65ZEXns2jgtocZjkzRzn22F6vsZMH89tXcsMe+PYuFcPZLMhNLtRutoO5qN9l+QJN1EHznetswdOCVtYE+eN5zLXttO49C/eij+OU2gibhoAZ+aHeRC0Poa0aZsIGNWTX7YHVeqpRINV834/YNj0xaytjGPzyl8R+60G3cNevLz2WcYpAM7MX7+Y7bHv8tLs1VRi3L5s/bNMkDUvxU0g5bbup/WNZF1VHFu3RZBQWo1ibYeL32qWhzjWX4Xbyic1MxazriqerQmRJJToYaAOe/cFxIQ1rKeqOAYT87LKim3P4q/XYOcxl3XhHrfXGpmKFjvrCmKjw9hwyRKbse7MiwmjvrhhajumyysSZ6O246SABVg2ahI0Fbfu3l6cHEFgfFH9/hQ/708yGvzeyGB5r53SuO16imeOqXpI899Bi8+yaKq3xLFh3izKrsBwuykErI4mpG52yVH+rFupsmZzOL5rDWjsnHl8dRQho42b246z6fRtAqKI+WETb0UFs+18NYq1PW6B0Szy7811gmZx1AURHQlbE2JYsK0cFQ124714eeMCPGtHpJk83tqKsxntfQDUqKgqWFr1latPszibEYe2jzeNiXZRU9t7pzuuN16ATQCw7/TnbP7nDi63MneKleUgIn7yDFPv69woQyHM9V7S++grKnjxhYU3e1dukJ+fj1OjqWK7VgkJ8xZzNjCOVdPqBqMbKMuM5rm1ekJSNxFwO90pJ4QQQgghbrrKq9du9i7cFqzuvKUmKBJCCCGEEKJbfffddwAcOXLE5GtlxF8Lfnr/ZFxGObLn1AHyvjrO1+oFAEZqRuA8yhGvMVMZpMjtlaJnGK5e5aO//o23Yjbe7F3peTXllJVWopbrUWuMw6S5VEz+oSKq7/fG6VaYmFoIIYQQQgghhBBCCCGE6CEy4k+IXq64uIRPs/bxbEj71qDsKd074g/0hxJ5Kz6DgyWVoChYDtBi7+rPMyG+LS8ELYQQQgghRDeSEX89Q0b8CSGEEEII0aA9I/6k408I0Snd3fEnhBBCCCFEbyIdfz1DOv6EEEIIIYRo0J6OPylJ/3/27j2uqir///grsYMXTl6OOR2yoBpRU/OnaZr2xVC8Rdkw6UDjyAzFjA1OBpWXMXH8ekvMBtOBdKIojAaK79DYUBomopNDyWAKZmAlqEFjHrUOXjiF/v7grsA53ISO7+fjwUM5a+91Fp+991pr77X32iIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk5AA38iIiIiIiIiIiIiIiIiTkADfyIiIiIiIiIiIiIiIiJOQAN/IiIiIiIiIiIiIiIiIk6gY1sXoD07n5fH2X37+P6//+Waa66h409+Qpf/9//o1LdvWxdNREREREREREREREREpBYN/NXBduQI/33hBc7n51+WZomPp9OAAfzkiScw3HhjG5RORERERERERERERERE5HKa6vMSJf/+N0efeqrOQb9K5w8e5Gh4OGf27LmCJZOr1dGjx0h6M7mtiyEiIiIiIiIiIiIiIu2cBv5qKD18mK/XrOHC+fN2l71w7hxfr16N7ejR1i1TaSnzFy7i3vETefChX/DDDz8wftIU/vnuey36Pc9HvcDIMd58vCerRfOtT+6BA8S/nsBfYjYQ91r8FflOgIQ3Epl03wPNzqfwyBGCQ37H//j4smjxEvIPHWLkGG+Ki79ugVLCk3Pn88dFiwG4/vpeJPwtka+/rp33mTNn2PzPVB757SymPPBgi3xvU1mr/mdj60JffJdltmFpnIMlO5HFswLw9fHG2z+ExYm5NeJczpqbworQAHx9fPGdFsqKlPzLlmkR57J5LsCbkb+MJa+sCeki54rJSo1nxSx/vP2jyKpjP7li+7MzO5dLwryZ5fVGQBS7bXaWPxhLgE8oCceuSOmch739uczC7pgw/Cf74u03k/DYbCw1kh2p3ysVpYThPcaX8C32NqYTshNHaVtF26MI8fdlpI8fs5OLKUoMxTsghhz1Ay63ezkP3nsv4y/7uZ81NU69Sg68zZo5D/PgxIk8GDibNf84RElloq2QD6KeZMb9E5ky5SEeW5bEgZK6vuzq1pj6VVqClfQl/oz0DiWhuK3L4nwK4kMYOca71o93RBpVPQJH+huhFcfD1JmEb8ykqFYdbaNgSxQh03zx9vUnKCKRnHoOGKfujxzPZMO8meXnIAFhPJdaiK0x6Xb7K47H2ak1ul9nL26tnW6l4MNUNiwJwdcnhLgvm/n3XyH22sHGt5NW8pKXEeLvh7ePH/6hkWz+suIIsKYS7u19WT01cow3ARsrHqbJjcH/0vSpNc6f7KUDloMZJK+fR4CvL7NT2tvZgGPtoK04k7iIEPx8fRnp609QRDxZtf6U5u6vwPHsqu/w9pvJ7Kg0Cqoqqwa244+Cg/2N4gw2LKyIweQAQiJTa8SgPB9H42Cv3bss3ZHjoQ2064G/gwcPUlRU1KR1P/nkE06ePOn4Chcv8vVzz3HR5viOf+H8eb5eswYuXmxCCR2T9sF2PvroYza+GM2Lf3mBjh1bfnbWixcvkrFzFwP69yd9R8Zl6dEvbmTFs5F2P3PUlq3v8/bmf3LrLbcw7ec/Y/LEiU3Kpyk6de5Ep06dmp3Pa/Gvc/EivPnG64TN+UMLlKy2zp2qy9mpUyceuN+P/0v5R61lioqLeT9tG2fOnGnx72+s5zKuxp5rK8qPJXxhKkyOID4piZdm9SVvY0TtOH+ZSPjTsRzuG0LUqy8TFdKXw9svPZlrCTbyEmLY+q2xiekiwNFMktOyOWy1UVpX+hXbn52bJS2ejftNBMckkbg2iOGGti6Rk7KzPxckRrAgw0Twmk0kRv4c19QIFqdUnKE4Ur9XsqSxLu4Qhs6t+te0Ww3GUdpWWS4JMSlYx0SQ9EYsSyeZ27pE7dvwMDYm/I2Eqp/XWBnYF7f+ATw4tGKZw0ksnB/LkZ8+ysrYl1j5iBdHdmRSXNEOHnhlMes+H8jT8e/z3lurmHLuTZZEZ/JjumTT6hpTv0qLsGbGsC7Ths4CWkdpiRXXEaEkJSWRUvGTGDaWyu5dg+2kNYMXlqVgHRVO7BspxM8fizVlEQsSCqvyt+2LZf6abPrNiiHx1Qh8rJtYsDrj8kEAZ+6PlBUSF7GMrV0CiU5KIjbUi7z1YSyvrDfspWO/v+JwnJ1cY/t19uLW2umUFZL+9zRyiq3YfiznpPbawSa0k9YPY3gy3orPykR2bktk6YhC1i2quOHbOIFFb1TXTylJSSQt8cO9+zCm+3mVZ1Bixdp9LM/UXG5jCHe44Fg6VvZvSyU9/yQlZe2v1+NYO2hhy5pItrj48b8vJ5ESE8rggngWrEytGvxu9v5qy2XdU/NIto7i6fWbiF8ZiOmzNLKOV5Szoe34I+BQnM9l89zjq9l9fQCrNiWRGBlIrz1RhEfnVvWXHY6DvXavrnRHjoc20K4H/rp160ZWVlajBzYKCws5evQobm5uDq9zJiurSU/vlX75JWf372/0ehcuXHBouZMnT9K7d2/6efXF3dw6J9WfHjzId1YrMx4OIGPXrsvKdur06cvWqeszRxw+XMCOjJ38/MGp3DNmNDfccANm8w1NyqspenTvTo8ePepNd3i7nDpF//5emM030KtXr5YqXpXuPbrTo0f3qt8njB/Hu+9t4WKNQea+P/0pf3khioBfTG/x72+s3enZuvDQkrwCWLUxhqX+g3Dvbabf5FAeucdWI85W0uM2kd8/hKXhExjs4cHgyeHErg+in4udvBur8O88lwxTpw7EtSnpIgBe/qxau5bI6X3r2E+u4P7s5EpPn4TrBzHcy4y72YTG/VpJQ/tzWS5vpeRzZ1AoUweZcR/kz9wZfcl5K7X8hMJu/V7Jyu6NseQM8cfHdKX+sHbEXhyljZVgLTFwy5C78TSbMemqf8MMbtxwo7n6p/Mh3nvfxpTHA+jrAlDCrtc2cah/CAvn+DLwZg8GTnyCdVEzK9IL+eQ/xfSd+DOG9gTc+vLgdG/Yv5dDOh6qOVy/Souw5RIXvRP3qb54qr/WKqxWG4bennj2MeNe+WOq6N3ZayeNY1kUH0tU0Cg8zSY8xwTxiLcrBXsqn7Ky8WFKKiVjZ/PEBC/c+wwjONQft38lsbnWWIyT90cOprL5i4EEh/nRr7cJz7GhzJ3mxo6kVIocSbfbX3E0zk6u0f06e3Fr7XTAZRDBz68ler7fj6eOs9cONqGdLPg4G+sQP6YPMIKLkcEzAhh+MpvdxwAMmGrWT30ga/NOTDPDmdanfH2btQRbNw/61VzObKw6T7WXDkZ8Hl9N9NpQ7u3ezs5uHW4HTUxdEs+mJf4M9zDh7jWBx2YMw5adyX4btMT+akmNJfnkWJ5eEoLPADOeQ/xYunF11XZoeDu2c47GufMwHnk+lujwCQw2m3Af4s/cGYOwfJhWVc84Fgd77V596faPh7bQ4gN/ZWVllJW1zBmIu7s7Hh4e7Nixg++++86hdYqLi9m7dy+jRo3CYHC8UjiT1fQpLs/+5z92l8k/dIhxE6fwWV4eM34dzLgJkykrK+PixYtsSniDB/wfYtzEKYQ9NZdjx74C4JVX43lx40sUHjnCyDHevPRyXJ15N5RHzIa/8lDAw9i+/x6ALw8fZszYceTlVT9mmr5jJ0OHDGHEiOGcPHmKnNzcqrSHfvEw7/wzlc3/TGXkGG+enDu/zs8AEpPe4ql5C/ggfQe/efS3+E8PZOmKlVit1fPQ7M7MxGw2c/vtAxoZ5do2JbxBzIa/kvRWMidPngLg6LFj/P4PcwgO+R0P/+rX+E8LYMoDD+IzYTKLlywFwGTqicnUsyqfxKS3eHr+H3n3vS34TfUn7Km5QPk0mitXrWb8pCncN/VnrHpuDWfPngXg8bAn+XfmR6S8vZmRY7zJ3ru3zjI2lIe97WLqacLUs7oG8fT04CIXKSw80qy4tRbbpU/KuljJSV5GkL8v3j7+BEWm1f/kjiWXzesjCJrmh7e3L34zI0jIrZGfrZCt6+cR4O+Hd83HpX0iSK/Is+jDWBYE++PtXT79xo9/ikAj7h7GWr+b3GrE2ZZN+p5SRk/2xb3O9cunXA2JzyZ94zwC/Hzx9g1gdlw2luJMNiyciZ+vL74B80jIbShSFjav24TFN7SehqqhdCtZ8REETS2fBsz/8UiSG/wuuWrZ3Z/FEQVxIQRuzKf0y3iCvL0JiC2EMgtZyVGEz/THu2I6lxWXTk9US8PHra0wjXVPVUwlOjWEBXGaevEyx7LJsXgyfEh1G266YxDm4lxyLGC3fq9g27eJF3aaCQ4ZhfFqvLBvN47SMir6CwnZpK8Pw9/Xl6D48qdB6j3ebRks9pvHVquN9CW+tfpjl+VeXx5lxSSE+uK3rPpO5aKUMHynRbL7KugqHHhzE3vvmMmvBlacq9qy2ZVlY+REX+q+HdKNnj2hcP8BKuezKfnGgs3ohvHHcjHyinCsfpWWUZAYw+aO/syZbNLAaquwYS2x1rp2UYsD7aTBWPuujFIb0Nmt/IJ6WT45uaV4DRtYfYH9tmHcYcwnp8a5uLP3R2zFxVi6e3JLjVD1GzEMU342OVb76Xa3g4NxdnqN7dfZi1trp/9o2WsHG99OGk1GyM9mf2X/zFLMCXpiquM5G9uH8cQV+/LYzz2qPrOWWKF7/f0Ve+ntWaPaQaOx1k25NpsNurjh6kIL7K9WPszIxThmAmO6U6fGbMf2pjFxNnmYaz0VaHQzgq16lh5H4mCv3XO0XazreGgLLT7wl5aWRlpaWovlN2TIEDp06MD27ds5fPhwvcuVlZXx6aefsnv3bq677jquv/76Rn3P902cUhTA5uC6Z86cYdXq51n8zEK2vvsOLi4u/C3pTba+v41VK5bz5t9e5/YBA3hq/gLKysr4ZeAvCP71TPr0uZGt777DzBkP15lvQ3k8GvxrLpRd4G+JSQDEvPhXHrj/Pvr1q37MND0jgxEj7qR7t254efVle3r1dJ+JCfHcNWI49983hV3p21j97Io6P6uU+dHHfPTxHqLXvcArf32RY0e/IvW9LVXpR44cxePmmxoV37pMmjCBX8+cgbvZnTcSk7hw4QI39enDi39ZR1zsX/nb66+RkpzEe+/8g/S0LSxdUv6+vAEDBvDH+XNr5ZWbe4CtaduIj3uZNZHPAvCn/13O2bNnee2VWF7aEMM335zghfXRADy7fBnD7xyG35TJbH33He4YPLjOMjaUh73tMv0hfx584P5a+fXr68WnBw82O3atwbO/V61GzLo9hhe+GMScyJeJDh+GNXUtG/5VdxVt/Wwnu0sHEhwRQ+Ibawn2zGXdyviqOzLy4hex4t8mHlufws70JFb5e2AcEkRUTCjDXcC6J4rZi1Ip9QnjpU0vExXkQV50OAuSnegWurJ8du8rrY5zcSEFpZ70G9DwLfY58VGk9w4hdvM2EsO9yIuLIGhhKsZfxpC6NYm5Aw6xcW0KBfWsb90ew8YvRzEnZFidj9Q3lG7LjOFP8RZ8ViaxbXMsSyeYQc8ESl0c3J+lYZ4zoogO9sLVI5DozanEBnmAJZv0fa6MeWwZ8W9s4s/+bqSvieSteqrHBo9bSwbLw2I5PGQ2scmppKwJwJgWwfKr6nZlB1gsWDDTq+bdfyYzvSim6Hgdy19av1d8lrA+Fddftu3dgW2qsXGUZilIiiTZJZDod94ldoZHw8e7YSyLEiLw6Wxg9PwUtqVE4FPXxZqG8nAxMyMsCFNGLHH7bHAuk7iEQoaHhDDa2ZuCczv5x/s2xj/oTdU1hq8LOWLzoG//+q6+mBgf8igeWct5fP6LvLfrbda9doiRgfdx8xUq9o9SXfWrtIziFJ5LsjI1TLMztJ5SrGfBkrG6/CbOqQGELEkkq3Lipca0k2VW8rZEsSGzJw9MH1V+3lZmocjiSq+aj2y7mHA3wYnjFSMxV0F/xGB0w1Bi4UTN+45dXDGUnaTopP10u9vBkThfDRrdP7YTt9ZOdxb22kEH2klP/1CmdUnlyVkRbNiSQUJUEtZJgUy+7EkoC1v+vhPT5J/Xet2E1VpC6bFUFkzzw3uyPwGPR5J80OpwervV5HbQhmVfCs8lHMJzmj/DXWiB/bmQvGPgOahvC2zHdqZZ/Q0bWXtyoe8gbqlY124c7LV7DreLdR8PbaHFB/46dWqZd6hV+uKLL7hw4QJDhw7l4MGDvPfeexw4cIBjx45x/Phxjh49yt69e0lNTeXEiROMHz+e8+fPc7CRAyQXHZzisU6NeMLx0Ud+TT+vvri6unL+/Hn+GvsKEc/8kYG3D6CXycTvQh7h22+/49Dnn9OpUydcXTvh0sGF7t261RlXe3m4urry9JNhvBr/Ohk7d7EvJ4ff/+53Vet//vkXHDv2FXcNHw7AqLvuYsfOXVXp1157LR06dKBDhw4YDAY6duxY52eVevTowcL5c+natQs9evTgzjuHcrigoCr97LmzXHfddVW/f3rwM2JfqftJxob07n09Xbt25X/uGc3Zc+coKnbswqPh2mvpZapds505e5Y/LXoGk6knBoOBvZ/sIyc3lz9FPEOfG2/kRnd3/hD6GO9v+wAAN7eudOzYEYOrge7dutX53kV7edjbLkajETe3rrXyNPUycfLUqUbF6UqZ4Vf7DgbXu0KJmu/PcC8PBvsFMMmjlIIv695GxjGhrHo6EJ8hHrj3GcS0gAm4F2dXzEVtJS+/GNPdfvj0MYDBjI/f3RjzCym9zYwRC1vjU7GODWNp0Fj6eXgw2C+cpTPMZMXHk/VjvlmrhqLUWDZ/e3d1nK0lWHHDUJDI4hD/qicl4/bU7qQa7wlhkb8XRhdwH+fNcJdSvAIimDHICC4mfHyGwReHyKsrTtZs1sVkMjgkFJ+67hqyk156ugSrqwn33iaMRjODpwYxbZAuu0gdHNyfxQ6DEaMBcDFg7F7x/94TmLsslGljBuFp9mB4YCCjXfPJ2lf3yVRDx21ByiZ2DwphadAoPE1GTF4TmBPYl/9sz9RTfzVV3FFYa9IJAxiwld9pf4nL6negKCWGhFJ/5ga27d2BbaqRcZTmsfb2Y+6sUbh3NmBwsX+8G7qX37Xs2tmIsZ5pl+zWGV6BzP05JMckkp4YS7o5iDnt/upD89mydvKRyyj+546aV8VKsGLEUJjEysceYsr4iUx/ZDEJWdW1q8HUl9tvNeP6zTbWRaxlV/f7mD7a+ePVHHXVr9ISLGyNiadozGyCh6hv33qMDPcLIXhWBLGbt7FtfSj9CmNZsKxiikkH28milHn4+vgRtDIT91nLeWJExYXjUhulGGqvX/F75dM/V0V/ZOAoRhsySIjPxmKzYclNYfnzqVXvV7Wbbm87OBDnq0Jj+3X24tba6U7CXjvoUDvZxczggZ6YXApIXhnBuv19mTF92OUDTJZMtu7ryehxtfNyH+XPY0GzWbUplZ2JkcwwZbNu/lrSTzuW3j41sR3MjyfI15f7QqPJGxTGqqCKAddm768lWEtcMZblErewYqYN/xAWJ+ZWz4Tm6HZsV5rZ3/gykVe2w6Tp3lT1mO3EwV6753C7WM/x0BYuH7FopnvvvbdF8zt16hSjR4+mR48e3HzzzXzyyScUFhbSuXNnvv/+ezp37sx///tf7rnnHm64oXxylLvvvpu8vLxGfc+1vXpxroll7NiId7z99Nbbqv7/+RdfcO7cOUIfn1Nrme+//4ETJyzQz35+juQxZvTd3Dl0KAueieCp8Cfo1q164G37jgx6mUz89Kfl5Ro1cgSvbXqdg599xoD+/R3+uyq5uNQegu/SpStffVU94NPJtRNWa/UFx5/edis3ujd+crdXXn2N8+fPU1pais1mw2q1cvTYMVauWs358+c5f760/N/S8v//z5jRVU/9Xapnz5707Fn93r/9Obl8Z7Uy+f6ptZb74fvvOXPmDF27dr00i8s4kkdD26UuXbp0bvT7Lq+USZdcdzB0NlY/AeZixNgFSs+VXrpaFevBNJK37CRrfwFF1pNYynpitQJmI8PvHsS6t1JJDxiEj8nC7i3/xuI5gVsMQFku+w/C4Dm1nzjzHDEM9/hscopheNvXs81TmMKKjQfoFxxbHWcXoCyX5NRhzI2IZ5HZxv74CBZErMU9cRmTKgfiXFyrG3KDEYNLxbqVH3UxYMRKaSnUbvFt5GyKIt0cQrxfXReV7KWDcVwIcz6cx4qgmWzx9WPqVD98vNrXbfy2jGX4P5tZ8di/gdF/TGLp2Pbd9XFKju7P0iS247lsSU1l955DHLZYsJy10e9c3Se09R+3VvYfzMe6JxI/38jaK902Ciugy88VDAZcAZsNqHzZt82GDQOul74cvK76/Xgq6+KKmbRkNYMNgBNOq+WQxsRRms3o4VXjnRktcbw7koeBwTNnMyltHgvivQje6H9VTPd8IGsvDPw9A2ueMlW0g2+/N5Q5C1/j6RtKOZCwmCVLXuCGhKWMdztEwsIVfD7lz7zyoAcnD9exJwgAACAASURBVLzNhudjWLjExPqV99UzPehVrq76VVqE9cNYNuwbxGNxo+qcEURajvu4QGZU/uIxlidmZbLjj2nsPu7HNAfbSXe/COKHFVOQu42E2FBCT0cSEzwIg6sBV2zUHuMo/93Q2XD19EeMY5m7rJgVayLwTyjFNMiP4N/4U7QsF6ObA+n2toO9OF8tGtuvsxe31k53BvbaQYfaSRtZ6+axriSE2E1jMRZn89b6SNaERWLcOL/W+82s2ZnkGYcRfMn1N4OXH8GVE8519mJqWAjp0yLZumc+PhMMdtPboya3g7cFErVpLEVfZrMlPobfzisherU/ni2wv7q6WNnx950MD1/Nu8tMnMiMZsGiCNZ5JvHMKBzeju1Js/obtnziVsZTfPd8IkdVrm1nfy6z0+41ol2s73hoCy0+8NfShlc8iQZwzTXXYLFYGDx4MDfdVD1d5IcffsjJkyerBv569OjBqFGjGvU9ne+4g++2b29SGTvfcUeT1rtw4SIAm159BXezudXysNlsfPHll3Tt2pUzZ87WStuRkcHJU6cYP2kKABfLsyN9x84mDfzV5WJlpkCfPn0oPFL9njqDwdCodzFWeuQ3v67z8xf/sq7xBbzExYsX6H399fzj72+1ah4NbZe6nDt7jusbMcjc3tT7suDkMGbH25g0K4RnQobh/k08QY9WTxfs7huIT2IUG54KYM05A+YBY1m1LAjPmpk46xQz1mzWLYqmYNh84qfVOL5NZtwNXowOD2J0xcfDgwIYnriarM9gUuOqv8uVZfPW5kKspdEE+kZXfGaj1JbJb/0ymPbsbE40lB65iTlDPJi2LAmfgxlsTklizaxNJM+KIjrQq/7vvcIM94QRn1C9Zxra28uirxatvT9fxWy5sYTOT8PoH8IjEeEM7p3Lc9PmUVBfJ9VQ33FrBgwY/Zazbb42SIN6mzFxoHzaosqOvsXCCcz49K6xXD31e1FaCumni3H94328U/FZ6TkbRN6H7/Zwtq32u0J/SBtzNI7SSlrieHcgj2/yyTtnxOhSiu2cDdr5fcfNV8ineVbMoy+Zislkxmzoy8g5Mxl5A4AbQ2cEMPTN1ez9DMa7pPC2xZuF95cfDD0H/oyFy0qZE7yJtz+7j8da5pTNedTXf5YWkf73NIpOw4oAXypf+FFqs1Ew04/0oJjyqcalVRh6mzGRi7UEx9tJQ/k7vdw9vBjuZuG+ZfFs8V/NVKMJ996lHD5uhcpLqmUWiizQq7fpquqPGIcEsmpTYNXvtg8j2Wj0wLO7A+nn7GwHl4bjfNVobL/OXtxaO/3Hzl476Gg7ac0geStMihlbftOXeRgzlkVgmRlGQloQPoHV6xZ8lo/NI7BqSsV6GU24G6GgpJ5byeyltwNNbgddDJjMHpjMHgzub6BgWgwJmX48M6q5+6sVU08Dd84IY9qw8nT3MUFMHZLKW/sKYGChw9uxPWl6f8NCemQEcVY/Vq2ZUL0X2dmf+5U13O6tG9JwetJqv6rvcvh4uALa/cBfTRcuXKBDhw64X/KEWL9+/SgsLGxW3m5jxnAiLo6yb79t1Hode/bEbeTIJn3nbbfegsFgYOfOfxEYML3V8oh7LR6z+Qb+GPQrnp6/kAm+43A3mzl69BhffHmY//1TBENqvKfuLzEvkr4jg9DHyqee7NixI2WXTGda12eOGHXXcOJe28TBg58xYED7PEsd0L8/L/73JfIPHcKrb99Wy6O+7VKfk6dOMeSOut8n+KNVls/mt3LxDEpiTuWTY9/UXqQgJZasEfNJeXrY5eu7eNHPC97IzsbqN7bqLpCC7GyKjH3p1z7bL8fY8klYGEGyayDRCyfU7vL0Hsbwm2LJybWCuXq6Fi57/L+JXIYxNz6Jx2oc4tbty/jt5r6sej6Iwb2N0GB69eemAWMJHjCWSbeGEpicSs50Lwa3g8YPABcjpvbZl7y6tPb+fBXL2pxC/rAwtoVMKL/I7GCzfflxG04/LzPWtH+TZRvV5vPUt2vmQQw3xbJ/nwU8yiuYouxsivsMY3Dl06sN1O/uUyNJGVfjVpmyfOIeX0ZRwMs8M+HH3Kg1kiNxlFZibIHj3ZE8ikmOSoJpkUSVRTJ7bTyTNobQz5nrl7JiCo9Bzxsu6XxcP5ShfWI5cKCE6TdUvOfPZgNcy9vBc2ArKy2vwyv7UN1M9MSGralT1jirhvrP0iImL9zE8Jp3dB5LIXxeNj7PL2P6rVdRO9XayixYzpqo+Uony8FcClw8cb8e6GKvnbRiPWfEWONpKkMXA65lFU+NuHgxeJArm7OzsU6tOI/Oz2b/2UFMH2TAfcTV2h+x8mFaJoyK4I46z1kvSbfXX7ET56tGY/t19uLW2uk/ZvbawUa1kzZKfwB+qPGRiwlTd7CWWIHKusDK4QJLxc0Jtde3WMBkqhHT4nzyThu5xWx0IL19ako7aLXaMBpr/J1dDBhdyt/l2vz92ZPhQ0xszT2EbXLltJXl0+i6GlxxfDu2L03rb1jJ2TifP2WambN+NqNr1S8Nx8E9oOF2z90wqsH06n2/vuOhbbT4O/5aU4cOHRg3btxl00n26tWLO++8s3l5d+pE79DQxq10zTX0/sMfuKaJVya7du1KyCO/YcNLsfwz9V2OH/+G/Tm5bEp4o8H1rr3WwPHj33Dx4kW7eRw9eozX30jkD6G/564RIxh99yhWr/kzUD7NZ4/u3Zkwfhxm8w1VP373TeHI0aN8efgwADffdBN7P9lH4ZEjfFVUVO9njujXrx933TWCt/7v7/zrw918/fXXfFVUxEcf72Hs+In8J3svAKdOn2bSfQ/wyqvxdf7+2Wd5WK0lHPvqKz7c/e9GRN2+kXeNYORdI1gY8Sc+3pPFN998Q8bOXWzbnl7vOoZry/eB/x4/7lAeDW2X+uTlH6J/v+r5X61WKxbLSc6cOcPFCxexWE5isZxs7p/fJEXHirE2ZSp0F1eMXaBgXyZ5xy0UZKeyLiqFvBoXp0ttpVi/zCXrmAWLxYLltBVbVbqZqTMmYNi+lhUJmeQdKyRnSxSL3yhm8C8DGf1j7bOVFbN52XzWHRvGnKf8MFmKKTpWXCPOHkz95TByYtey+aAF6+lCtsbEs7v3BCYPaokCGDD2NuNurvFjdCsfKDObMBrspUNB4jIWbEwjz2LDZi0k5zNL+Z1+7WXQT64om7X8+LWetUGZDavFgsVirXgSuLX356uXsbMbpV9k82GhhaL8TJIjY9hqqVFZuxpwxYrFUj4Fd0PHbT//IHxsqaxYkkhWoQWrpZDdyfFs/rKN/rg21OD+7DKIqdO8+E/cWpIPWrAcTGFdUiF3TvcrfyG5vfrdaKpdt5pNGF3A0N2Mu+nH2qg1gb04SqtqyvHu6mqAcxZOWB3Lw5IaxYZj3jwWMIjBQeE8YE3kucTm3cjZ7n17kpM2Az27uV2S4MGUwGEceGUt731moeTbQj7YsImPevsyfiAw1JfxnXeyccNOjpQAtmJ2vZLER9d78z/t8z7KtmG3/ywtwWC65BygtxuuFecOpvZ7nfZHx7IlkqBZEcRl5FNUXEze9lgWb8zG/GAAPkbstpPWLasJ/E0EcdtzKSgupiA7lRUxaZQOGcvo3gAGxvj7Y/pXLOvSCrEUZxMXk0LJWH8m9ebq6Y/Y8kleE0VyZj5Fhbmkxy1jzZ6+zAqquHhuL91uf8VOnK8WduNUSPLjfviGpZS/w9Ju3Fo7HbBZK64/lVBKKdbTFf399jztrb120G47ecl2MI5i8t0lJK+PZfcxG2ClYEssyfmeTPauOZPTSSwnwWTqWbs8hX9nflAoK5IzyTtWTMG+NJ5bsol8L39mjDDYT4fa51plUHq2pPw8rA1vfLLfDl4Sx+JEwmeEsjix/O8s+jKT5JWx7O5yN5NGGGiJ/Xl4YCDuO2N4Ia0Qq9VC3uYYkvO98Bnr0Yjt2L40Os7YyEuMIDzRyqSwcEZ3tlTt3xYr9uNgr91zuF2s53hoIz+qJ/5am9vo0fQMDORkYqJDy5t+9Su6jhjRrO8M+tUMjEYjr7waT+SaP3PzzTfhN2Vyg+tM9B1P3Kuvcd+USdzwk580mMfq5/+M7/hx3F7xdF34E48T8PCvSNv2Aek7Mhg/zueygdSRd42ge7dupO/I4NZbbuGXgb/g04MHmfmbRxlyx2DWr/1znZ85yv/Bqdx8003s25/D3r2f0NWtK4MG3n5ZOVw6unDNNXX/nr33E9I+2E6Xrl0Y0N+BlyE2UuSzK9iw8SUWL1mK7fvv8er7U4Jmzqh3+T59bmTg7QNY/uwqkhPfsJtHQ9tlgu/4y/I/9tVXfG+zceutt1R99sdFi9mT9Z+q3++b+jMAdqVva9L0qc3hHxDKtJgU5g5p7JoeTAsLIW9NDL8NiME0yJfgWaFMmh9ftUS/CX54Jsby5C9jKa3sZBlM9PMLI+rpsZjGzCd6SSzrXo3ktxutGHp74RMSyZxpP95pZmzbY1iz3QJk8FxIRo0UU1WcTRPm8eeSGJ6LCGDNaSPmIRN4ZnVI+VzT7YD7XaPwjEviyRnLOFFqxH2IL8/M//lV8f4euZSFLRH+rNhT+XsuC/xTwTCKpZtXM8nY/vfnH6vBQeEEH4tmxW8CwDyMqcFhPHY6jKrbWPoMY/RtSbwRmcSkTSHc0tBx23sCS9fDhuh4/vRoDCcw0e+uCcwYdjVMz1eT/f3Zc/oyVp1cxnPhAazDzJ3TlrFoavldiY7U71KuoThKK2vC8W4aNpbBcdGsiJmAz8JRDedx+t+8EHuAwSHxjDYCDOOx0LEEro4iedxapvW5kn/sFVRioQQjbpeO+wE9x89jZUkM65c8zLpvjZjv8OXplY8y0AAwjMdWz+OVdS/z1PTllFCevnD1owzVOy+rqH4VZ2Lyi2DVuRg2bAwn7lgpht6eDJ+6nGeCB1XVwg21k8bJ81h1LpYNcRHEFVqgixkv79lEhVa/T9UwKIiop60s3vgo/hYjnmNnsips7NX17kaDCc/eJ1kXGcpzp11xH+DNrKhQqrob9tKx319RnMs1HCcDuIBrjUuC9uLW2ukFCeEExOZXlafgcX8SMDL1+VSeaadvPrDXDj5x3F47eel2MDFpYSSl0TE8N+s+is5CL8+7mbY8svq9fABlVqxWcHW7ZK/2CCRyCbwQF8Xs6GJsXcx4jZpJ9OOB5e+WtpcOkBtNYGgKJyrzjJnJfTHgPiOWlND2Omh1SRzNgURGwIa4KGZvLMaKEc8hvjy9ZjY+FU+kNXd/pY8/q5ZYWbE+DL+VNoyew3hg+TKCbwWHt+OPziVxPp7KuphsrGWweclMNtdYsl/IJuKDPa5MHOo7HtrINRdrvoBNALDu3Mnx6GgunK37vWsd3Nz4yeOP4zZ69BUumVytXtv0OpaTJ3nyiTltXZTL5OTkMLjGVLEtq5C4WfM4HBDD0nGVD0nbKEqL5LcrLQQnrWXa1XSnnIiIiIi0uZLvL7R1Ea4Kbtf+qCYoEhERERFpVd999x0An3zyid1l9cRfHYze3nQdNozvtm/n7Cef8P1//wvAtT/5CV2GDuW6cePo0LVrG5dSrha277/n7X+8wwtRa9q6KFdeWTFFx0qwFluwlpU/Rs3pAnL25FN62wQGt4cJk0VERERERERERERE2gk98SfSzhUUFPJB+g4eDf51WxelTq37xB9Y9sTzQmwquwtLwGDAtbMJr1H+PBLsV/eLoEVEREREWpGe+Lsy9MSfiIiIiEi1xjzxp4E/EWmW1h74ExERERFpTzTwd2Vo4E9EREREpFpjBv7UkxYRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECGvgTERERERERERERERERcQIa+BMRERERERERERERERFxAhr4ExEREREREREREREREXECHdu6AO3Zif2HKd79KSVfWeAacOvTC/fRt2Ma5NnWRRMRERERERERERERERGpRQN/dTj9eRG7n3mVE/sPX5a2NyqF64fexpiVwVzn+ZM2KJ2IiIiIiIiIiIiIiIjI5TTV5yWObNvLu79YWeegX6Vv9n5B6rTlHMvYfwVLJs7q6NFjJL2Z3NbFEBERERERERERERGRHzkN/NVw6rOj7Hr6JX44V2p32e/PnGdn+F/59ouiVi1TaWkp8xcu4t7xE3nwoV/www8/MH7SFP757nst+j3PR73AyDHefLwnq0XzrU/ugQPEv57AX2I2EPdafIvlG/3iBmI2bGx2PoVHjhAc8jv+x8eXRYuXkH/oECPHeFNc/HULlBLuHT+R5P9LAeD663uR8LdEvv66cXmfOXOGzf9M5ZHfzmLKAw+2SLmaauQYP1bsAbCxdaEvvssy27Q8zsCSncjiWQH4+njj7R/C4sRcrI1Ibwm24ly2JkQxe5ov3hEZLZy7XE0s2YksDq3YX6fOJHxjJkVll6S38v58VTiXS8K8meVxDIhit83O8gdjCfAJJeHYFSmd8zhXTFZqPCtm+ePtH0VW2SXpZRZ2x4ThP9kXb7+ZhMdmY6mR3Jj9vSglDO8xvoRvsbcxnZCdOErbKtoeRYi/LyN9/JidXExRYijeATHkXHo8COxezoP33sv4y37uZ02NU6+SA2+zZs7DPDhxIg8GzmbNPw5RUploK+SDqCeZcf9Epkx5iMeWJXGgpK4vE2tuCitCA/D18cV3WigrUvLVp2gFinPrK4gPYeQY71o/3hFpVPUIHOlvNND/BhsFW6IImeaLt68/QRGJ5NSzEZ25P+LQeUpDcbTbX3E8zk6t0f06e3FrXrqtOJO4iBD8fH0Z6etPUEQ8WRZH1y8mIcT7suNz5BhvRoYmUmQnvTW1/HUkKwUfprJhSQi+PiHEfXl5el7yMkL8/fD28cM/NJLNX9aoJ8qK2RpVY7vHZNQ6fuxuh3OFbF0/jwC/8j5nwFMxpBc3MTitwkr6En9GeoeS0GC57MWxWr317fHsqlh5+81kdlQaBVWLNG87tH8tE2f7x33ztpP9/K+8dj3wd/DgQYqKmjaw9sknn3Dy5EmHl7944SI7n36JstLvHV7nh3Ol7Jr7Mly82JQiOiTtg+189NHHbHwxmhf/8gIdO7b87KwXL14kY+cuBvTvT/qOyy/uR7+4kRXPRtr9zFFbtr7P25v/ya233MK0n/+MyRMnNimfuri6unLttYZm5/Na/OtcvAhvvvE6YXP+0AIlq61T58506uRa/v9OnXjgfj/+L+UfjcqjqLiY99O2cebMmRYvX2MZBwQwbVhbl8KJ5McSvjAVJkcQn5TES7P6krcxgucyrI6lt5AT2alszizkxDnnO8GTK8iawQvLUrCOCif2jRTi54/FmrKIBQmF5elXaH++GljS4tm430RwTBKJa4MY3vzmUOpyNJPktGwOW23UdatYQWIECzJMBK/ZRGLkz3FNjWBxSsUZSmP2d0sa6+IOYejcqn9Nu9VgHKVtleWSEJOCdUwESW/EsnSSua1L1L4ND2Njwt9IqPp5jZWBfXHrH8CDQyuWOZzEwvmxHPnpo6yMfYmVj3hxZEcmxRUXZQ68sph1nw/k6fj3ee+tVUw59yZLojNRD+0SXyYS/nQsh/uGEPXqy0SF9OXw9ksHO6TZFOcrorTEiuuIUJKSkkip+EkMG0tl967BdtJe/xuw7Ytl/pps+s2KIfHVCHysm1iwOuPyQQBn7o/Yi5MDcbTXX3E4zk6usf06e3FrXrqFLWsi2eLix/++nERKTCiDC+JZsDK1ajCy4fXNTF1ZfVymJCWREjcfH5OJSdMm4G4nvdW0xnWkskLS/55GTrEVWx11vPXDGJ6Mt+KzMpGd2xJZOqKQdYtiySsDsJETM4/n9nswJyaJlKggev17GeEbcyv6L/a2g5WtK8NYl+/FY1GbePfVZUwuS+NPEfEUtJP2xpoZw7pMG0Z7C9qJY5X66ltbLuuemkeydRRPr99E/MpATJ+lkXW8ohzN2g7tX8vE2f5x37zt5ED+baBdD/x169aNrKysRg9sFBYWcvToUdzc3Bxe56udOXz7ReMvJpw8eITijz5r9HoXLlxwLP+TJ+nduzf9vPribm6dk+pPDx7kO6uVGQ8HkLFr12VlO3X69GXr1PWZIw4fLmBHxk5+/uBU7hkzmhtuuAGz+YYm5VUXN6Mb111Xf1XgcNxPnaJ/fy/M5hvo1atXSxWvSo/u3enRo0fV7xPGj+Pd97ZwsRGDyH1/+lP+8kIUAb+Y3uLla6ypYYH0c2nrUjgRrwBWbYxhqf8g3Hub6Tc5lEfusbE7Pbu8YbaX3kLc/eYTvX4tjw3T6IE0g3Esi+JjiQoahafZhOeYIB7xdqVgT8Vdnldof74alJ4+CdcPYriXGXezCR25rcTLn1Vr1xI5vS+ul6aV5fJWSj53BoUydZAZ90H+zJ3Rl5y3UstPvBze363s3hhLzhB/fExX6g9rR+zFUdpYCdYSA7cMuRtPsxmT3bPwq5zBjRtuNFf/dD7Ee+/bmPJ4AH1dAErY9domDvUPYeEcXwbe7MHAiU+wLmpmRXohn/ynmL4Tf8bQnoBbXx6c7g3793JIx0MNVtLjNpHfP4Sl4RMY7OHB4MnhxK4P0nlKi1KcrxSr1Yahtyeefcy4V/6YKnp39tpJe/1vbHyYkkrJ2Nk8McEL9z7DCA71x+1fSWyudVnMyfsj9uJkL91uf8XRODu5Rvfr7MWtuekmpi6JZ9MSf4Z7mHD3msBjM4Zhy85kv82R9cHYu8Zx2ceMNT2FnEEhPDHO5FB6q2iN60gugwh+fi3R8/3wrKOOL/g4G+sQP6YPMIKLkcEzAhh+MpvdxwBbNm9tKcEnZDY+t5oweU1g7pwJlGxOIv002N8ORnxC1/LS8yH4eJkxeQwjeI4f5i92kt4eZqyx5RIXvRP3qb51xqYWO3EsV399a0mNJfnkWJ5eEoLPADOeQ/xYunE10/qUpzdvO7RzLRZne/ubvfUr1bedHMi/DbT4wF9ZWRllZS1zBuLu7o6Hhwc7duzgu+++c2id4uJi9u7dy6hRozAYHL/k9dXOnKYWk6925tpdJv/QIcZNnMJneXnM+HUw4yZMpqysjIsXL7Ip4Q0e8H+IcROnEPbUXI4d+wqAV16N58WNL1F45Agjx3jz0stxdebdUB4xG/7KQwEPY/u+/EnGLw8fZszYceTl5Vetn75jJ0OHDGHEiOGcPHmKnNzqv+ehXzzMO/9MZfM/Uxk5xpsn586v8zOAxKS3eGreAj5I38FvHv0t/tMDWbpiJVZr9Tw0uzMzMZvN3H77gEZGubYXN75E1Lr1PLt6DUuWLifzo48B6GbsRrdu3aqWe/sf7/DM4iW8t2UrflP9CXtqLlA+TebKVasZP2kK9039GaueW8PZs2cBeDzsSf6d+REpb29m5BhvsvfurbMM9eVx8eJFfvv72SxZtqJq2f9LeZv7f/YQ586fB8Bk6onJ1LMq3dPTg4tcpLDwSLPi0laCB11yrLlYyUleRpC/L94+/gRFptV/B6gll83rIwia5oe3ty9+MyNIyK1RK9oqHu3398O75hQJPhGkV+RZ9GEsC4L98fb2xTcgzAmmmjHi7mGs9bvJDWw2m4Pp5VOuhsRnk76xfFoEb98AZsdlYynOZMPCmfj5+uIbMI+E3KZGykpWfARBU8unXPB/PJLkJuclzs5grH1VuNQGdHarGJiytz+LIwriQgjcmE/pl/EEeXsTEFsIZRaykqMIn+mPd8V0LitSCxsYUG34uLYVprHuqYqpRKeGsCBOUy9e5lg2ORZPhg+pPgsw3TEIc3EuOeVXkBza3237NvHCTjPBIaMwXo0X9u3GUVpGRX8hIZv09WH4+/oSFF/+FEO9x7stg8V+89hqtZG+xLdWf+yy3OvLo6yYhFBf/JZV35lflBKG77RIdl8FXYkDb25i7x0z+dXAiv6zLZtdWTZGTvSl7tsh3ejZEwr3H6ByPpuSbyzYjG4YNdBSzZZN+p5SRk/2xb2ty+LMFOcrxIa1xFrrmkEtDrSTDfa/y/LJyS3Fa9jA6hvFbhvGHcZ8cmqci18N/ZGGz1PspNvbDg7G2ek1tl9nL27NTQcwGmvdJGmz2aCLG64uDnz/pSypbNgMD/zGjzqH9eylt5jmXkdqwjeajJCfzf7K/pulmBP0xOQGnC7GctaEe5/qSBuGjOJOssmqfH6moe0AGMweuNdcoLMbRkoptf+GrlZXkBjD5o7+zJlsapEbluuvb618mJGLccwExnSve91mb4d2rEXjbGd/c0SD7WIL5N/SWnzgLy0tjbS0tBbLb8iQIXTo0IHt27dz+PDhepcrKyvj008/Zffu3Vx33XVcf/31jfqe7wqPN7mM1sL/OrTcmTNnWLX6eRY/s5Ct776Di4sLf0t6k63vb2PViuW8+bfXuX3AAJ6av4CysjJ+GfgLgn89kz59bmTru+8wc8bDdebbUB6PBv+aC2UX+FtiEgAxL/6VB+6/j379vKrWT8/Ii/h/3QAAIABJREFUYMSIO+nerRteXn3Znl493WdiQjx3jRjO/fdNYVf6NlY/u6LOzyplfvQxH328h+h1L/DKX1/k2NGvSH1vS1X6kSNH8bj5pkbFty6/n/Vbwuc8zh/nPc2SxYsYNfIuAMZ634P3PffUWvbTTw/yfto24uNeZk3kswD86X+Xc/bsWV57JZaXNsTwzTcneGF9NADPLl/G8DuH4TdlMlvffYc7Bg+uswz15XHNNdewYO7TpG37gP05uZw/f56X417liT+E0rlTp/J1Fy3kp7fdViu/fn29+PTgwWbHpi1ceqO3dXsML3wxiDmRLxMdPgxr6lo2/KvuKtr62U52lw4kOCKGxDfWEuyZy7qV8VV3fuXFL2LFv008tj6FnelJrPL3wDgkiKiYUIa7gHVPFLMXpVLqE8ZLm14mKsiDvOhwFiQ70S10Zfns3leKZ3+vup/gqSc9Jz6K9N4hxG7eRmK4F3lxEQQtTMX4yxhStyYxd8AhNq5NoaAJRbJlxvCneAs+K5PYtjmWpRPMcPmzLyK1lVnJ2xLFhsyePDB9VN1TNdjb36VOnjOiiA72wtUjkOjNqcQGeYAlm/R9rox5bBnxb2ziz/5upK+J5K16qscGj2tLBsvDYjk8ZDaxyamkrAnAmBbB8qvqdmUHWCxYMNOr5lm9yUwviimqq6tZ1/5elk/C+lRcfxledRfnVaexcZRmKUiKJNklkOh33iV2hkfDx7thLIsSIvDpbGD0/BS2pUTgU9fJbEN5uJiZERaEKSOWuH02OJdJXEIhw0NCGO3sTw+e28k/3rcx/kFvquam+bqQIzYP+vavb7YaE+NDHsUjazmPz3+R93a9zbrXDjEy8D5uvkLF/lEoLqSg1JN+A5x9J2pjivMVUor1LFgyVpffxDk1gJAliWRVPp3RmHayrv53mYUiiyu9aj6y7WLC3QQnjleMxFxt/RF75yl1pdvbDo7E+WrQ6P6xnbg1N70WG5Z9KTyXcAjPaf4Md3Hg+y9RkJrCf271Y7rXZUkOpbeaJl5HagxP/1CmdUnlyVkRbNiSQUJUEtZJgUw2UT5A7mLhRM2nyspcwVDKieOX3ulVx3aogzU7m4IuXvRr/mXl5ilO4bkkK1PDWuhp94bq27JC8o6B56C+9W6nltsO7UxLx7mKY/vbZRxuF5uYfyto8YG/Tp060alicKMlfPHFF1y4cIGhQ4dy8OBB3nvvPQ4cOMCxY8c4fvw4R48eZe/evaSmpnLixAnGjx/P+fPnOdjIAZSLzXhK8cIPjk0fCfDoI7+mn1dfXF1dOX/+PH+NfYWIZ/7IwNsH0Mtk4nchj/Dtt99x6PPP6dSpE66unXDp4EL3bt3qjKu9PFxdXXn6yTBejX+djJ272JeTw+9/97uq9T///AuOHfuKu4YPB2DUXXexY+euqvRrr72WDh060KFDBwwGAx07dqzzs0o9evRg4fy5dO3ahR49enDnnUM5XFBQlX723Fmuu+66qt8/PfgZsa/U/SRjU3Tp0oUuXWpPhnz23DkinlmIydQTg8HA3k/2kZOby58inqHPjTdyo7s7fwh9jPe3fQCAm1tXOnbsiMHVQPdu3ep8r6K9PG679RYCfjGNtev+QuKbb3HzTTcxwXd81frXX3/9Zfmaepk4eepUi8WiLbneFUrUfH+Ge3kw2C+ASR6lFHxZ98Vh45hQVj0diM8QD9z7DGJawATci7Mr5qq2kpdfjOluP3z6GMBgxsfvboz5hZTeZsaIha3xqVjHhrE0aCz9PDwY7BfO0hlmsuLjyXKSm+iKUmPZ/O3dzPDzaFS68Z4QFvl7YXQB93HeDHcpxSsgghmDjOBiwsdnGHxxiLwmxKn0dAlWVxPuvU0YjWYGTw1i2qVPforUUJQyD18fP4JWZuI+azlPjKj7gpG9/V3qYTBiNAAuBozdK/7fewJzl4UybcwgPM0eDA8MZLRrPln76u7gN3RcF6RsYvegEJYGjcLTZMTkNYE5gX35z/ZMPfVXk638vX+1Jp0wgAFb+R3il6hrfy9KiSGh1J+5gVfxMdDIOErzWHv7MXfWKNw7GzC42D/eDd3L72Z17WzE2L3utt9uneEVyNyfQ3JMIumJsaSbg5gz2RnnkavNlrWTj1xG8T931IibtQQrRgyFSax87CGmjJ/I9EcWk5BVXbsaTH25/VYzrt9sY13EWnZ1v4/po50/Xo1iLcGKG4aCRBaH+FfNJBK3R61Ui1KcrxAjw/1CCJ4VQezmbWxbH0q/wlgWLEulCBxuJ+vtf5faKMVQe/2K3yuf/rma+iP2zlPqTbe3HRyI81Whsf06e3Frbnql/HiCfH25LzSavEFhrAqqGABr1HYrJD29gMHjvOt5ms9eeutp6nWkRuliZvBAT0wuBSSvjGDd/r7MmD6sPI7GYfgMsbE1PpGc0zZsxdkkLItmtxVsOLAdLnUul7iEbEx+Afi06TtHLWyNiadozGyCh7TMNbCG69sSrCWuGMtyiVtYMZOGfwiLE3OrZzprie3Q7rR8nAHH97c6ONQuNiP/1nD5iEYz3XvvvS2a36lTpxg9ejQ9evTg5ptv5pNPPuH/s3f/cVXW9//HH0UdUjlLPcY8zIJ+CDkhv5I/SBsOxVBZNDYdLCcbxT46XCb9kLIwP6EmamE6SReLwmiw+IzmRulwKjbNJlEKzsCVkAZlHnIdFDmGfv8ABRQ4h9+Iz/vtxk3PeZ/rfa7zuq7rfb2v6/2+3u/S0lL69OnDmTNn6NOnD19++SV33303gwfXDo5y1113UVRU1Krv6Te4mSEUHFnW7Piyt91S/5TXfz75hKqqKqIfmtfoM2fOfMvx4xbwsp+fI3mMH3cXd44cyRNPxfFozMNcf319w9u2HbkMMpm47bba9fIbO5rXNr7OwY8/Ztjttzv8u85zcmrcjN23bz8+/7y+wec65+uwWutvON526y18z631g4S8tOFlTlef5vTpaqpPn2ZK0D0Xnvq72ID+/Rk4sH4+vf0FhXxjtTLlRyGNPvftmTOcPHmSfv362f1+R/L49YMP8NOfhZP8yqu8mvx7u3n27dun1fNZ9lSGPsb6HnJORox9obqq+WfxrQdzyNy8k7z9JZRZK7DUDMRqBcxGRt3lzZo3s9ke5k2AycLuze9h8ZjMzQagppD9B8Fnnm+jHnkeo31xS82noBxGXe7XKaVZLN1wAK/IZIKaqim2lO7kXH+CMRgxOAENDlFDXwNGrLXDJLTyTGScGMW8XQtYGjGLzYHBhIQEE+B5efX8teXGE/rcHmr3TAPjnszg2QlqvOwsbsFxpPqWU1K4lbTkaKJPJJAU6d1417O3v0ur2Y4Vsjk7m917D3HYYsFyyoZXVdMV/OaPayv7DxZj3ZtAcGBC44Vu9cMKXX4h22MZDDgDNhtw/oLUZsOGAeeLL1Cb2t+PZbMmpZygxSvwMQC9dFgtu1oTR2k3o7tng3ksOuJ4dyQPAz6z5hKUs4AnUj2J3BB6RQwbeCDvQxj+G4Y3vGRyAmoKeeudkcxb+BqPDa7mQNoiFi9+kcFpzzLJ5RBpC5fyn6kv8Mp97lQceIv1zyexcLGJtcumNTM86BWoLo6Z2b48HpfK02Yb+1PjeCJuNW7p8QQ1MzyWtJLi3GXcJoYz8/wL9wk8PHsPO57MYfexYKY7eJ5stv7tbMAZG43bMGpfG/oYrrj6iL3rlGbT7W0He3G+UrS2Xmcvbu1NP+/WcBI3TqDs03w2pybx6wWVrFsRikdrttuxfPJKzPiMaKZ2ZC+9s7TnPpLDbOStWcCayiiSN07AWJ7Pm2sTWDU/AeOGWAJMJkJi4yhLWEdMaDK4ehMyO4KgTxKwuDS4d9Tcdmh0e9lK3roEMmuCeWFWVz862Zh1VzLr93kzJ6WZEYxay4Hy1tnJyo4/72RUzArejjdxfM86nng6jjUeGTzlR8dshx6mw+N8nkP7WxMcPS+2Nf9O0uENfx1tVN2TaABXXXUVFosFHx8fbryx/rneXbt2UVFRcaHhb8CAAfj5+bXqewaPvZ1P/vJem9Zx8FgHWuiacPbsOQA2vvoKbmZzp+Vhs9n45NNP6devHydPnmqUtiM3l4qvv2ZS0FQAztVmx/YdO9vU8NeUc+czBYYMGULpZ/Xz2BkMhlbNxXjeb2b/uh3rcxbXG27gL39+s1PzKC8vp7q6mquuuopTVVV286w6VcUNgwa1eZ16uub6kZRkzmduqo2g2VE8FeWL21epRDxYP1ywW2A4AemJrH80jFVVBszDJrA8PgKPhpn01vlNrPmseXodJb6xpE5v4vi2l96ZDO5Mj88g4GAum7IyWDV7I5mzE1kX3r2VsNYw3D2f1LT6PdPQzFML0kEMtXMKuLl7MsrFwrT4VDaHriDk/A2i7tyfeylbYTLRsTkYQ6N4IC4GH9dCVk5fQElzldRmj2szYMAYvIStsa2rX11xXM2YOFA7bNH5jicWC8cxE+Da4HPN7O9lOVlsP1GO85PT+Gvde9VVNkiYRuC2GLauCO6iH9LNHI2jdJKOON4dyOOrYoqqjBidqrFV2Wh1L6TLTin/LrJiHnfRUE0mM2bDUMbOm8XYwQAujJwZxsg/reDDj2GSUxZvWfxZ+KPag2Hg8B+zML6aeZEbeevjaczpmEu2y5/JjJvBk3ExEYyrK1ZHRYQxKn0FeR9DkE5fHUNx7jYGVzMmCrFW4vh5srn6t9GEm2s1h49ZuTBxR42FMgsMcjVdefURe9cpzaXb2w5OLcf5itHaep29uLU3/cL3GDCZ3TGZ3fG53UDJ9CTS9gTzlJ/j2812qJAiJ49mh/2zl94puuo+kjWXzC0QlDShtlOY2ZeZ8XFYZs0nLSeCgHAzuPox53k/5lxYJpuYZR7c4dGgJtTcdhhf/5myTfE8s9mZ+1fPZVQ3t1Vt/3MOZSdgaVgg5ye+qrbZKJkVzPaIpNopN1rBXnmb8dxQTAMN3DlzPtN9a3+82/gIQkZk8+a+Ehhe2jHboYfp6Dhf4MD+1hS722lF3Ryebcy/s/T4hr+Gzp49y9VXX43bRU+IeXl5UVpa2q683afcyQerMjld0brxbfu69ufGif+vTd956y03YzAY2Lnzn4SHzei0PFJeS8VsHsyTEb/gsdiFTA6ciJvZzJEjR/nk08P87zNxjGgwj93vkl5i+45coufUDgl6zTXXUHPRUKhNvecIvzGjSHltIwcPfsywYd1zlTrs9tt56cuXKT50CM+hQzstjxWrXuD+8DBstjMsX7GS1JQ/NDls6HkVX3/NiDuank+w16opZtObhXhEZDAvuK4C9VXjj5RkJZM3Opasx3wvXd7JEy9PeCM/H2vwhAu9QEry8ykzDsXrcm47sBWTtjCOTOdw1i2cfGnvenvpXcQ0bAKRwyYQdEs04ZnZFMzwxOdyaYh1MmK6gq63uo8Va5URY4PenIa+BpxrGvSi7CH7c2+TtymLYt/5bI2aXHuT2cHT9qXHdQxenmasOe+RZ/NjVM+9Puh+Zm9GmZLZv88C7rV7cll+PuVDfPE5f/Oohf3dLSSBrIkNusrUFJPyUDxlYX/gqcmX80mtlRyJo3QSYwcc747kUU5mYgZMTyCxJoG5q1MJ2hCFV28uX2rKKT0KAwdfdJa7YSQjhyRz4EAlMwbXzfNnswHOtcOMVYGtprq2DD9fx7rexEBs2Oz3LbxyuPoy6sZkCgqtYK4fzpBLhmuTdlGcu0aNBcspEw2nGLMcLKTEyQO3G4C+9s6TdurfTp74eDuzKT8fa0jddXRxPvtPeTPD24Db6CulPmLvOsVOur36ip04XzFaW6+zF7f2pgNWqw2jscE26GvA6FQ7t2ZrttvxknJsrt64NXMPxF56h+vS+0g2qr8Fvm3wlpMJU3+wVlqBS8sKy/ZcCsx+zKlrs2lxO5xfZlcCcxMP4RObxJwecNxMWbiRUQ2fbDiaRcyCfAKej2fGLa0vH+1d/5mcYNQIE1sKD2GbUjd8Z90wuc4GZzpiO/REHR1ncGx/a47d7dTO/DtLh8/x15muvvpqJk6ceMlwkoMGDeLOO+9sV97X9HFm7DO/aN1CV12F37OzcHK+tk3f2a9fP6Ie+BXrX07mb9lvc+zYV+wvKGRj2hstLnfttQaOHfuKc+fO2c3jyJGjvP5GOr+N/g1jRo9m3F1+rFj1AlA7zOeA/v2ZPGkiZvPgC3/B06by2ZEjfHr4MAA33XgjH360j9LPPuPzsrJm33OEl5cXY8aM5s3/+zP/3LWbL774gs/Lynj/X3uZMOkePsj/EICvT5wgaNq9vPJqapOv22PsmNGMHTOahXHP8K+9eXz11Vfk7nyXrdu2N7uM4draA/fLY8ccyiP7nc0cOXKU+8PDiPzlLE6ePEXaHzNaXK+i4kPc7uX406NWqxWLpYKTJ09y7uw5LJYKLJYKh5fvEZycMfaFkn17KDpmoSQ/mzWJWRQ1uDldbavG+mkheUctWCwWLCes2C6kmwmZORnDttUsTdtD0dFSCjYnsuiNcnzuD2dc99cJ2qamnE3xsaw56su8R4MxWcopO1r7Z7U5kN5BrCcsWCzlWGuAmkrKLBYsdV9Qkh7PExtyKLLYsFlLKfjYUtuT73Jp9JMuY928gvBfxZGyrZCS8nJK8rNZmpRD9YgJjHOly/bnK5GxjwvVn+Szq9RCWfEeMhOS2GJpEFRnA85YsVhqOz21dFx7hUYQYMtm6eJ08kotWC2l7M5MZdOn3fTjupHNWns+sp6yQY0Nq8WCxWKtfbLdyZuQ6Z58kLKazIMWLAezWJNRyp0zgmsnJLe3vxtNuJnNDf5MGJ3A0N+Mm+lyPam1gb04Sqdqy/Hu7GyAKgvHrY7lYclOZP1Rf+aEeeMTEcO91nRWprevI2eP998KKmwGBl7vclGCO1PDfTnwymre+dhC5X9L+cf6jbzvGsik4cDIQCb12cmG9Tv5rBKwlfPuKxm8f4M/P9DTfg24E3K/LwXJq9l00IL1RClbklLZ7TqZKd7dvW69ieLcFSybE4iYHUdKbjFl5eUUbUtm0YZ8zPeFEWDE7nnSbv0bA+NDQzH9M5k1OaVYyvNJScqickIoQa5cMfURe3GyG0e79RU7cb5S2I1TKZkPBRM4P6t2Dku7cWtnenk6MTOjWZS+h6Kj5ZR9uofMZcns7nsXQaMNDuRfz3KsHPoPxNhM/dReeodq932ki7cDYLPW3YerpJrquntEdffkjH5MuauSzLXJ7D5qA6yUbE4ms9iDKf6egJWClATWZOdTcrSUgs1JxG44xLjIsNrtbnc7gHVfMjHP5GD8SSxzRnBhfcss3XejwGAyNy4fXV1wxoDR1VzXWaO1cbRX3hoYFR6O284kXswpxWq1ULQpicxiTwImuLd/O/RQHR5nB/a3dm0nR/LvBpfVE3+dzf0eX+6Ivpf9SX+1/2Fg5MM/ZsiEO9r1nRG/mInRaOSVV1NJWPUCN910I8FTp7S4zD2Bk0h59TWmTQ1i8He/22IeK55/gcBJE/l+3dN1MQ8/RNjPf0HO1n+wfUcukyYGXNKQOnbMaPpffz3bd+Ryy803c3/4z/j3wYPM+tWDjLjDh7WrX2jyPUeF3hfCTTfeyL79BXz44Uf0c+mH9/DvX7IeTtc4cdVVzb9uj4TnlrJ+w8ssWvwstjNn8Bx6GxGzZjb7+SFDvsfw7w9jyXPLyUx/o8U8rJWVrF33Er/9zWz69KntGhbz8EMs+t/4C09bXuzo559zxmbjlltudvg3PPn0IvbmfXDh9bSQHwPw7vatbRo+tXu4M31+FEWrkvh1WBIm70AiZ0cTFFvfwOs1ORiP9GQeuT+Z6vMNfgYTXsHzSXxsAqbxsaxbnMyaVxP49QYrBldPAqISmDe9B3ddscO2LYlV2yxALiujchukmJielMXDx1pOf3xEB6xETS4rp8ex5UJP8gRCcwHvaLI2hOM2xg+PlAwemRnP8WojbiMCeSr2J1fE/DzSOsYpC1helcz6lDhSSi3Q14yn/1wSo2vnc7K3v3fI/nyF8omIIfLoOpb+KgzMvoREzmfOiflc6OYyxJdxt2bwRkIGQRujuLml49p1Ms+uhfXrUnnmwSSOY8JrzGRm+l4Jw/M1ZGFzXChL955/XcgTodlg8OPZTSsIMoLHjHiWV8SzMiaMNZi5c3o8T4fUnvu1vzuupThKJ2vD8W7ynYBPyjqWJk0mYKFfy3mceI8Xkw/gE5XKOCOAL3OiJxC+IpHMiau7djisrlRpoRIjLhe3+wEDJy1gWWUSaxf/nDX/NWK+I5DHlj3IcAOAL3NWLOCVNX/g0RlLqKQ2feGKBxmpOS8bMU1ewAuVSayMC2PVCSPmEZN5akVU7Vws0mEU585nCo5jeVUS6zfEkHK0GoOrB6NClvBUg3nnWjpP2qt/Axi8I0h8zMqiDQ8SajHiMWEWy+dP6Ni5lHo4u3FyII726iuKc62W42QAJ3BucEvQXtzalW4OJyEO1qckMndDOVaMeIwI5LFVcwno71j+5x3/rxX6GJvdnvbSO1L77yNduh1K0mIISy6uf/1QKGkYCXk+m6f8TAQtTKB6XRIrZ0+j7BQM8riL6UsSiPQEMGLyNHF4fRwRCdUYhvgSMn81cybWRcPedqgpJm1VKkVVQPoCwtIbrPLoWN5f3VOHHG5tHB3IckgoyxdbWbp2PsHLbBg9fLl3STyRtwC0cztctlobZ/vHfbu2kwPlSne46lzDCdgEgJK3/8V7i1/njLXpsVMM3+nLuCW/5KbJTQw9KNIGr218HUtFBY88PK+7V6XVCgoK8GkwVGzHKiVl9gIOhyXx7MTzgxDYKMtJ4NfLLERmrGb6ldRTTkRERES6XeWZs929ClcEl2svqwGKREREREQ61TfffAPARx99ZPezeuKvCR7TxuB2tzefvPUe5bv/TeXnxwFwGTIIt/HDueW+uzAY1b1SOobtzBne+stfeTFxFQAnT54kbGZEs5+/794f8esHI7tq9bpXTTllRyuxlluw1tQ+Rs2JEgr2FlN962R8NBGYiIiIiIiIiIiIiMgFeuJPpJuVlJTyj+07eDDyl929Km3SuU/8gWVvKi8mZ7O7tBIMBpz7mPD0C+WByOCmJ4IWEREREelEeuKva+iJPxERERGReq154k8NfyLSLp3d8CciIiIi0pOo4a9rqOFPRERERKReaxr+VJMWERERERERERERERER6QXU8CciIiIiIiIiIiIiIiLSC6jhT0RERERERERERERERKQXUMOfiIiIiIiIiIiIiIiISC+ghj8RERERERERERERERGRXkANfyIiIiIiIiIiIiIiIiK9gBr+RERERERERERERERERHoBNfyJiIiIiIiIiIiIiIiI9AJq+BMRERERERERERERERHpBdTwJyIiIiIiIiIiIiIiItILqOFPREREREREREREREREpBdQw5+IiIiIiIiIiIiIiIhIL6CGPxEREREREREREREREZFeQA1/IiIiIiIiIiIiIiIiIr2AGv5EREREREREREREREREegE1/ImIiIiIiIiIiIiIiIj0Amr4ExEREREREREREREREekF1PAnIiIiIiIiIiIiIiIi0guo4U9ERERERERERERERESkF1DDn4iIiIiIiIiIiIiIiEgvoIY/ERERERERERERERERkV5ADX8iIiIiIiIiIiIiIiIivYAa/kRERERERERERERERER6ATX8iYiIiIiIiIiIiIiIiPQCavgTERERERERERERERER6QXU8CciIiIiIiIiIiIiIiLSC1zT3SvQk50uKuLUvn2c+fJLrrrqKq757nfp+//+H9cNHdrdqyYiIiIiIiIiIiIiIiLSiBr+mmD77DO+fPFFThcXX5JmSU3lumHD+O7DD2P43ve6Ye1ERERERERERERERERELqWhPi9S+d57HHn00SYb/c47ffAgR2JiOLl3bxeumVypjhw5SsafMrt7NUREREREREREREREpIdTw18D1YcP88WqVZw9fdruZ89WVfHFihXYjhzp3HWqriZ24dP8cNI93PfTn/Htt98yKWgqf3v7nQ79nucTX2TseH/+tTevQ/NtTuGBA6S+nsbvktaT8lpql3wnQNob6QRNu7fd+ZR+9hmRUf/DDwICeXrRYooPHWLseH/Ky7/ogLWERx6P5cmnFwFwww2DSPtjOl980TjvkydPsulv2Tzw69lMvfe+DvnetrK2mGohMzqQ0MTCLlqbXqKqnLzsVJbODsU/NJG8moaJNrYsDCQwfk8rMrRSsiub9YujCAyIIuXT1qZ3DGthFkujwwgMCCRwejRLs4rt7D/SG1jy01kUHUZggD/+IbOI2bCHspqL0mfXpYdGsSi9UPtFW1QVkrZgVm0cwxLZbbPz+YPJhAVEk3a0S9au92ixfAZqLOxOmk/olED8g2cRk5yPpUFya/b3sqz5+I8PJGazvY3ZC9mJo3Svsm2JRIUGMjYgmLmZ5ZSlR+MflkTBxceDwO4l3PfDHzLpkr8fsarBpVflgbdYNe/n3HfPPdwXPpdVfzlE5flEWyn/SHyEmT+6h6lTf8qc+AwOVDb1ZaK6ZtdQnDtfSWoUY8f7N/rzj8vhQo3AkfpGC/VvsFGyOZGo6YH4B4YSEZdOQTMbsVfXR47tYf2CWbX7cth8VmaXYmtNut36iuNx7tVaXa+zF7fOTu+a+yMdze51Rmu3Q3ku6xdGERwYiP+UMKISsilpcADYyveQElebPjYwlIi4VPIaZdjOcqaqlC1rFxAWXFvnDHs0ie3lrY1Kx7Nfvl6kppwtiQ3inpR76eeP5V+IpX/wLOYm5tTG2ppNjL//JeeDseP9CdtQ7GD+Vooy44kKDcY/IJjQ6AQ2fdrzy/MOjbMjceyAcqOnxblHN/wdPHiQsrKyNi370UcfUVFR4fgC587xxcqVnLM5vkHOnj7NF6tWwblzbVhDx+RsM/bFAAAgAElEQVT8Yxvvv/8vNry0jpd+9yLXXNPxo7OeO3eO3J3vMuz229m+I/eS9HUvbWDpcwl233PU5i1/561Nf+OWm29m+k9+zJR77mlTPm1xXZ/ruO6669qdz2upr3PuHPzpjdeZP++3HbBmjfW5rn49r7vuOu79UTD/l/WXRp8pKy/n7zlbOXnyZId/f2utzL0Sa66d7MgeMnPyOWy1Ud0R+dWUsv3PORSUW7E1daK0l94RPk0n5rFkDg+NIvHVP5AYNZTD2+ycuOXyZ83lxfgsrH4xJL+RRWrsBKxZT/NEWmltenEyMQuzYUocqRkZvDx7KEUb4lSutIElJ5UN+01EJmWQvjqCUYbuXqNeyk75XJIexxO5JiJXbSQ94Sc4Z8exKKvuCrU1+7slhzUphzD06dRf02O1GEfpXjWFpCVlYR0fR8YbyTwbZO7uNerZRs1nQ9ofSbvw9xrLwoficnsY942s+8zhDBbGJvPZbQ+yLPlllj3gyWc79lBeV0c68Moi1vxnOI+l/p133lzO1Ko/sXjdHnr+LZsuprpm11Ccu0R1pRXn0dFkZGSQVfeXPn8C56t3LZ4n7dW/Adu+ZGJX5eM1O4n0V+MIsG7kiRW5lzbg9ub6SE0pKXHxbOkbzrqMDJKjPSlaO58l5+tl9tKxX19xOM69XGvrdfbi1tnpXXJ/pKM5cJ3Rqu1Qlc/Kh1aw+4Ywlm/MID0hnEF7E4lZV1hX/7CweVUCm52C+d8/ZJCVFI1PSSpPLMu+0JjYvnLGypZl81lT7MmcxI28/Wo8U2pyeCYulZLu3CYOlK+N2ShIWsDK/e7MS8ogKzGCQe/FE7OhsL4eZytkzaMLyLT68djajaQuC8f0cQ55xwDjZJ5+o/48kJWRQcbiYNz6+zIj2NOh/K27kngk1UrAsnR2bk3n2dGlrHk6maKevG93dJztxrH95UZPjHOPbvi7/vrrycvLa3XDRmlpKUeOHMHFxcXhZU7m5bXp6b3qTz/l1P79rV7u7NmzDn2uoqICV1dXvDyH4mbunIvqfx88yDdWKzN/Hkbuu+9esm5fnzhxyTJNveeIw4dL2JG7k5/cF8Ld48cxePBgzObBbcqrLQb078+AAQOaTXd4u3z9Nbff7onZPJhBgwZ11Opd0H9AfwYM6H/h9eRJE3n7nc2ca9DIPPS22/jdi4mE/WxGh39/a+3enq8bDx3NM5Tlq1eTMGMozh2Rn5M3kc+vZl1sMB5ObUhvNyvbUzZSfHsUz8ZMxsfdHZ8pMSSvjcCrU75PegzjBJ5OTSYxwg8PswmP8RE84O9Myd663oWeYSzfkMSzod64uZrxmhLNA3fbVK60QfWJCrjBm1GeZtzMJtTu10laKp9rCnkzq5g7I6IJ8Tbj5h3K4zOHUvBmdm2F3+H93cruDckUjAglwNRVP6wHsRdH6WaVWCsN3DziLjzMZkzG7l6fHs7gwuDvmev/+hzinb/bmPpQGEOdACp597WNHLo9ioXzAhl+kzvD73mYNYmz6tJL+eiDcobe82NGDgRchnLfDH/Y/yGHdDw0oLpm11Ccu4rVasPg6oHHEDNu5/9MdbU7e+dJe/VvbOzKyqZywlwenuyJ2xBfIqNDcflnBpsatQH08vrIwWw2fTKcyPnBeLma8JgQzePTXdiRkU2ZI+l26yuOxrmXa3W9zl7cOjudLrg/0gnsXWe0djv08eWB55NZFzMZH7MJtxGhPD7TG8uunLrPmwhZnMrGxaGMcjfh5jmZOTN9seXvYb8N2l/OGAmIXs3Lz0cR4GnG5O5L5LxgzJ/sZHt3jlhjt3y9iC2fNzdXEhA1l4BbTJg8J/P4vMlUbspge92tdUt2MpkVE3hscRQBw8x4jAjm2Q0rmD4EwICp4XlgCORt2olpVkxtugP5l/wrH+uIYGYMM4KTEZ+ZYYyqyGd3Tx75p8PjbCeOHVBu9MQ4d3jDX01NDTU1HXMF4ubmhru7Ozt27OCbb75xaJny8nI+/PBD/Pz8MBgcv+V1Mq/tQ1ye+uADu58pPnSIifdM5eOiImb+MpKJk6dQU1PDuXPn2Jj2BveG/pSJ90xl/qOPc/To5wC88moqL214mdLPPmPseH9e/kNKk3m3lEfS+t/z07CfYztzBoBPDx9m/ISJFBXVz2G4fcdORo4YwejRo6io+JqCwvohGX/6s5/z179ls+lv2Ywd788jj8c2+R5AesabPLrgCf6xfQe/evDXhM4I59mly7Ba68eh2b1nD2azme9/f1gro9zYxrQ3SFr/ezLezKSi4msAjhw9ym9+O4/IqP/h57/4JaHTw5h6730ETJ7CosXPAmAyDcRkGnghn/SMN3ks9knefmczwSGhzH/0caB2GM1ly1cwKWgq00J+zPKVqzh16hQAD81/hPf2vE/WW5sYO96f/A8/bHIdW8rD3nYxDTRhGlh/xvPwcOcc5ygt/axdcesstkZPylrIS4kjIqT20eeohGwOt1QkWArZtDaOiOnB+PsHEjwrjrTChvnZKNqUwNz7Q/Fv9Fh2KCvz6z5RmsOaR+uGtwueRczanEbDD/RaTlYKMuOJCA3EPyCUiIScTutpW5Yejf+CLAo2JzI3rPax9bC4LIpOlLJl7XzCgmsfpV+0qbTpxhpbPtv3VjNuSiBuzX1HbhIx99fuB4H3L2BldrEafnoJg7HxXeFqG9DHpa5hyoibe8N0IyaXi8sVsackJYrwDcVUf5pKhL8/YcmlUGMhLzORmFmh+NcN97L04uGJGrGSl1pbfo8NCCb0oQQyC+v7YzYqa0OieCJFQy9e4mg+BRYPRo2oP4eb7vDGXF5IgQUc3d9t+zby4k4zkVF+GK/EG/t24ygdo3bo8Ki0fLavnU9oYCARqbW9aJs93m25LApewBarje2LAxkbEMf2ZvbRZvOoKSctOpDg+PoesmVZ8wmcnsDuK+ARiAN/2siHd8ziF8PrrlVt+bybZ2PsPYE03R3ShYEDoXT/Ac6PZ1P5lQWb0QXj5XIzsis4UNeUDqA4dxEb1kpro3sXjThwnmyx/l1TTEFhNZ6+w+s7it3qyx3GYgoaXIv39vqIrbwcS38Pbm4QKq/RvpiK8ymw2k+3ux0cjHOv19p6nb24dXb6ZcvOdUYb6tcmdzONcnQxgq3BqCdGY6POpjabDfq64OxEh5QzBrM7bg2/oI8LRqqp7pBhsdqu5fsbFzlRjuWUCbch9amGEX7cST55HwNY2ZVbiHH8ZMb3byqDxmy7UkkpD2TOT9wdzB+MJiMU57P/fD3bUs5xBmJy/HmpbtGxcW7skjh2QLnRE+Pc4Q1/OTk55OTkdFh+I0aM4Oqrr2bbtm0cPny42c/V1NTw73//m927d/Od73yHG264oVXfc6aNQ4oC2Bxc9uTJkyxf8TyLnlrIlrf/ipOTE3/M+BNb/r6V5UuX8Kc/vs73hw3j0dgnqKmp4f7wnxH5y1kMGfI9trz9V2bN/HmT+baUx4ORv+RszVn+mJ4BQNJLv+feH03Dy8vzwvLbc3MZPfpO+l9/PZ6eQ9m2vX64z/S0VMaMHsWPpk3l3e1bWfHc0ibfO2/P+//i/X/tZd2aF3nl9y9x9MjnZL+z+UL6Z58dwf2mG1sV36YETZ7ML2fNxM3sxhvpGZw9e5Ybhwzhpd+tISX59/zx9dfIyszgnb/+he05m3l2ce18ecOGDePJ2Mcb5VVYeIAtOVtJTfkDqxKeA+CZ/13CqVOneO2VZF5en8RXXx3nxbXrAHhuSTyj7vQleOoUtrz9V+7w8WlyHVvKw952mfHTUO6790eN8vMa6sm/Dx5sd+w6g8ftnnUFn42i5FgeSSvHZ/YKXv59AjOHlJL3SQu3mT/eye7q4UTGJZH+xmoiPQpZsyz1Qo8j2951PLLmEF7zktm6PZuMhRNwM/nx+NoEHhgGWHJY9FACu4w/4dkNG3l58U8wvpfA3PicXn9D2rotiRc/8WZewh9YF+OLNXs16//ZeRXV6j3JrCz05vFXs9n6h1kMei+RuZEJ5N0STXL226RGmtm9Zh2bmwp8eSkl1R54DWvmsYATOax5LgdmJJK9JYvUmLsY5OSsJ5Z6mxorRZsTWb9nIPfO8KPJvaGmmN37qhuUK+IIj5mJrIv0xNk9nHWbskmOcAdLPtv3OTN+Tjypb2zkhVAXtq9K4M1mehjb9iTxTKqFgGUZbN2UzLOTzXD+mTZLLkvmJ3N4xFySM7PJWhWGMSeOJVdUd2UHWCxYMDOoYW9Vk5lBlFN2rInPN7W/1xSTtjYb5/vP9z68ArU2jtIuJRkJZDqFs+6vb5M8073l490wgafT4gjoY2BcbBZbs+IIaKrxqaU8nMzMnB+BKTeZlH02qNpDSlopo6KiGNfbnx6s2slf/m5j0n3+XLgH8EUpn9ncGXp7c3cFTEyKehD3vCU8FPsS77z7FmteO8TY8Gnc1EWrfVmwV9eUjqE4d5FqrKfAkruitoNlSBhRi9PJOz/wUmvOk03Vv2sslFmcGdTwkW0nE24mOH6s7mLuCqiPGIwuGCotHG84Z5mTM4aaCsoq7Kfb3Q6OxPlK0Or6sZ24dXZ6b3HxdUa769c28vYWwlBvbr6k7mfDsi+LlWmH8JgeyignOqWcsebnU9LXE6/231buGI7c3+jjgsHJwvGGA+fVOIOhmuPHrFBTStFR8PAe6sD9Dwub/7wT05Sf1E/rYS9/wCM0mul9s3lkdhzrN+eSlpiBNSicKZfLk9wdEedGmohjB5QbPTHOHd7wd911HTOH2nmffPIJZ8+eZeTIkRw8eJB33nmHAwcOcPToUY4dO8aRI0f48MMPyc7O5vjx40yaNInTp09zsJUNJOccHOKxSa14wvHBB36Jl+dQnJ2dOX36NL9PfoW4p55k+PeHMchk4n+iHuC///2GQ//5D9dddx3OztfhdLUT/a+/vsm42svD2dmZxx6Zz6upr5O78132FRTwm//5nwvL/+c/n3D06OeMGTUKAL8xY9ix890L6ddeey1XX301V199NQaDgWuuuabJ984bMGAAC2Mfp1+/vgwYMIA77xzJ4ZKSC+mnqk7xne9858Lrfx/8mORXmn6SsSWurjfQr18/fnD3OE5VVVFW7tiNR8O11zLI1PiIO3nqFM88/RQm00AMBgMffrSPgsJCnol7iiHf+x7fc3Pjt9Fz+PvWfwDg4tKPa665BoOzgf7XX9/kvIv28rC3XYxGIy4u/RrlaRpkouLrr1sVp64yM7iuh4T1PVIyS/CMiufxYF+8bvEkYGY0MzybP30Zx0ez/LFwAka44zbEm+lhk3Erz68dyxo4XnwIq4c/IX4mDE5GPKYEM6rmEIfxxNQHirJS2W4I5umFoYzzdMdrdChPxwZjyE3lzeJmv7ZXcB4TTWJsKKM83fEJDiPIvZqSTzvvJryzazCPx0zGow8YbplMgKcBRkfwVLAnRgx4BE7Aq+YQBU1NfG2txIoLhpJ0FkWFXni6M2VvXYWvysLxagODzGaMfYy4jQ4lcop7p/0W6XplWQsIDAgmYtke3GYv4eHRTd8wKstOZtN/76ovV8QxBiNGA+BkwNi/7v+uk3k8Pprp473xMLszKjyccc7F5O1r+pGa6hOVWJ1NuLmaMBrN+IREMN27tvwuydrIbu8ono3ww8NkxOQ5mXnhQ/lg255e38miVep6wDYadMIABmy1PRQv0tT+XpaVRFp1KI+HX8HHQCvjKO1jdQ3m8dl+uPUxYHCyf7wb+tf27nbuY8TYv+k6nt0ywzOcx38CmUnpbE9PZrs5gnmXzd2HtrPl7eR9Jz9+cEeDuFkrsWLEUJrBsjk/Zeqke5jxwCLS8upLV4NpKN+/xYzzV1tZE7ead/tPY8a43h+vVrFX15SOoTh3ESOjgqOInB1H8qatbF0bjVdpMk/E1w0x6eB5stn6d7WNagyNl697ff7poCuiPjLcj3GGXNJS87HYbFgKs1jyfPaF+VXtptvbDg7E+YrQ2nqdvbh1dnovccl1Rnvr15+m88o2CJrhT6MaSHEqEYGBTIteR5H3fJZH1DU0dnQ5U1VISlo+puAwAnrAnKOO3t/A6EvACBtbUtMpOGHDVp5PWvw6dlvBho3aIfSdMdYUkrKwbqSM0CgWpRc2MRfiHrbsG8i4ie6tyB/oa8ZnuAcmpxIyl8WxZv9QZs7wvSw6WndcnBtoKo4dUW70wDhf2mLRTj/84Q87NL+vv/6acePGMWDAAG666SY++ugjSktL6dOnD2fOnKFPnz58+eWX3H333QweXDs4yl133UVRUVGrvufaQYOoauM6XtOKOd5uu+XWC///zyefUFVVRfRD8xp95syZbzl+3AJe9vNzJI/x4+7izpEjeeKpOB6NeZjrr69veNu2I5dBJhO33Va7Xn5jR/Paxtc5+PHHDLv9dod/13lOTo27ffTt24/PP69viLjO+Tqs1vqi67Zbb+F7bq0fJOSVV1/j9OnTVFdXY7PZsFqtHDl6lGXLV3D69GlOn66u/be69v8/GD/uwlN/Fxs4cCADB9bP+7e/oJBvrFam/Cik0ee+PXOGkydP0q9fv4uzuIQjebS0XZrSt2+fVs932VWCzp/1Sw9QVO1ByJjWzUdpPZhD5uad5O0vocxagaVmIFYrYAa30X54vLGTTfmhzPN1piwnh7yaoUQOAbBSdLAc4whfvBqUpIZhd+HTN5uCj63g2Xt7oxr6GOt7ujgZMfaF6qpOHPOg0eFtwNAHnJ0aBL6vAWNzlUYnoKaQzGxfHo9L5Wmzjf2pcTwRtxq39HiCzME8HLmHJ54OJ2JMMCHBwYSMd+/UE6QtN57Q5/bUDVNhYNyTGTw74XKo+lye3ILjSPUtp6RwK2nJ0USfSCAp0rvxNi7NYumGA3hFJteXK9IutmOFbM7OZvfeQxy2WLCcsuFV1fSVnXFiFPN2LWBpxCw2BwYTEhJMgKcRsLL/YDHWvQkEByY0XuhWP6yANlcdgwFnwGYDzl+Q2mzYMOB88QVqU/v7sWzWpJQTtHgFPgagFw6r5ZDWxFHazeju2WD+mo443h3Jw4DPrLkE5SzgiVRPIjeEXhHDBh7I+xCG/4bhDetUdXWkt94ZybyFr/HY4GoOpC1i8eIXGZz2LJNcDpG2cCn/mfoCr9znTsWBt1j/fBILF5tYu2xaM8ODXoHs1TUdGD5LHKA4dxm3ieHMPP/CfQIPz97Djidz2H0smOkOniebrX87G3DGRuM2jtrXhj6GK6c+YpzA4/HlLF0VR2haNSbvYCJ/FUpZfCFGFwfS7W0He3G+UrS2Xmcvbp2d3hs0dZ3Rnvq1rZiUZamU3xVLgt9F99huDSdx4wTKPs1nc2oSv15QyboVoXh0aDljJW9dApk1wbwwy7OlD3YZh+5vAGAiJDaOsoR1xIQmg6s3IbMjCPokAYtLbSydnazs+PNORsWs4O14E8f3rOOJp+NY45HBU371OVrz91Bk9CXSvTX528hbs4A1lVEkb5yAsTyfN9cmsGp+AsYNsT1+/taOjPN5Tcax3eVGz4xzhzf8dbRRdU+iAVx11VVYLBZ8fHy48cb653p37dpFRUXFhYa/AQMG4Ofn16rv6XPHHXyzbVub1rHPHXe0abmzZ88BsPHVV3Azt66hpDV52Gw2Pvn0U/r168fJk6cape3IzaXi66+ZFDQVgHO12bF9x842Nfw15dz5TIEhQ4ZQ+ln9PHUGg6FVczGe98Cvftnk+y/9bk3rV/Ai586dxfWGG/jLn9/s1Dxa2i5NqTpVxQ2taGTuVq2Yb6Qkcz5zU20EzY7iqShf3L5KJeLBBsMFewYzfUQ2KYnRbK+yYTB7M31ZLCGuDXO5aEjIK3i+kx7bP81kxs3gybiYCMbVFVWjIsIYlb6CvI8hyM+IT8RqsqYUsn1TFmnLHiTFN4aX44M77Uag4e75pKbVR8zQzFML0kEMtXMOuLl7MsrFwrT4VDaHriDk/A0iaz5rnl5HiW8sqdPbdk6UxmyFyUTH5mAMjeKBuBh8XAtZOX0BJc1dVBncmR6fQcDBXDZlZbBq9kYyZyeyLtwMGDAGL2FrbOvqV1ccVzMmDtQOl3P+QsJi4ThmAhqet5rZ38tysth+ohznJ6fx17r3qqtskDCNwG0xbF0R3EU/pJs5GkfpJB1xvDuQx1fFFFUZMTpVY6uywWXR77g9Svl3kRXzuIuGcjKZMRuGMnbeLMYOBnBh5MwwRv5pBR9+DJOcsnjL4s/CH9UeDAOH/5iF8dXMi9zIWx9PY07HXLJd/uzWNbt39XoNxbnbGFzNmCjEWonj58nm6t9GE26u1Rw+ZoXzXUlrLJRZYJCr6YqqjxhHhLN8Y/iF17ZdCWwwuuPR34H0KjvbwanlOF8xWluvsxe3zk6/3DV3Xd3m+rWF7QlxpFiDWb5q8qUdwJwMmMzumMzu+NxuoGR6Eml7gnnKr33lTMaK4AvfVbYpnmc2O3P/6rmM6il9++3d32jI1Y85z/sx5/xrazYxyzy4w8MADMQ00MCdM+cz3bf2x7mNjyBkRDZv7isBv/qGzpKPi7G5h1861GpL+VtzyNwCQUkTauNp9mVmfByWWfNJy4kgILyH33vpsDjXazKO7S03rLk9Ms4dPtRnZzp79ixXX301bhc9Iebl5UVVVVuf16vlMn48Ttdf3+rlrhk4EJexY9v0nbfecjMGg4GdO//ZpuUdzSPltVTM5sEsi1/MH1JevTAs5pEjR/nk08M8E/cUr7+awuuvppD2WgqBEwPYvqN+nr9rrrmGmouGM23qPUf4jRlFWVk5Bw82MbNmDzHs9tv54ssvKT50qFPzaG67NKfi668Z0L+Hd6E0u+NGCUUHGz6Q3mDi34vVFLPpzUI8IuKZF+yLW1Mn8H0ZpJRPZvmrG8nKzCBjbRwzfc+f/o14eZqxFuZT0HDc/cJ8Ck6Z8RnaU2oEgqsvo24spaCwwb5RXXuTr2Hbv8HVm6CoOFJXhePyzyy2lHbiOjkZMZlMF/6MV3CDceeyYr3oFG3oa8C5pkFvKVsxaQvjyHQOZ/nCJi4mpE3yNmVR7BtFQtRkfMyO31A3DZtA5MIkXp7tTkFmNgU1dWXt3vfI67G9C3oIszejTCXs31c/5FlZfj7lQ7zxOX8Kb2F/dwtJICszg/SNG2v/Xo0jxNXAqNl/IDV2cpf+lG7lSBylk3TE8e5IHuVkJmbA9AQSZ0Dm6lSKenv5UlNO6VEYOPiis9wNIxk5pJQDByrr37PZAOfaOpINbDXVjXvCX29iIDZs7bsE7l0crGtKOynOXaPGguWiMd4sBwspcfLA7QYcOE/aqX87eeLj7UxBfn79UHLF+ew/5c0ob8MVXB+xsitnD/hN4I4mrw0vSre3HezE+YrR2nqdvbh1dvrlrKXr6jbVr60UbIjlmT1m5j03l3EXfc5qvajy1teA0al2jtL2ljPn192yK4G5iYfweSyeOT1i+zhwf8MOy/ZcCsx+jHMHnDwYNcJESeGhBh35a0fScjY4N/rewyWWuk4grcgfG9XfAt82+ICTCVN/sFY2PQVIz9DBcW6Qb5NxbHe50TPjfFk1/F199dVMnDjxkuEkBw0axJ133tm+vK+7Dtfo6NYtdNVVuP72t1zVxhpuv379iHrgV6x/OZm/Zb/NsWNfsb+gkI1pb7S43LXXGjh27CvOnTtnN48jR47y+hvp/Db6N4wZPZpxd/mxYtULQO0wnwP692fypImYzYMv/AVPm8pnR47w6eHDANx04418+NE+Sj/7jM/Lypp9zxFeXl6MGTOaN//vz/xz126++OILPi8r4/1/7WXCpHv4IP9DAL4+cYKgaffyyqupTb7++OMirNZKjn7+Obt2v9eKqNs3dsxoxo4ZzcK4Z/jX3jy++uorcne+y9Zt25tdxnBt7T7w5bFjDuXR0nZpTlHxIW73qh//1Wq1YrFUcPLkSc6dPYfFUoHFUtHen98mZUfLsdoAkx8hdzuzfUM8aXtKKfs0n01JCbxR2Eyp7OSMsS+U7NtD0TELJfnZrEnMoqjhjY2aamyWYgqKy7FYLFhOWGlYt/AKjSCg6s8sScgi79NySvKzWJLwZ2x3RzBjWGf+6s5ls1qwWCxYT9mgxobVYsFisbb9iT6btS5+lVRTjfVEXX41Dqa3mzsh9/tSkLyaTQctWE+UsiUpld2uk5niDbZ9yTyxOJXdpVZsNitFB4qxOJtxa31/DOlhrJtXEP6rOFK2FVJSXk5JfjZLk3KoHjGBca5ATTmb4mNZc9SXeY8GY7KUU3a0vL5ckTYz9nGh+pN8dpVaKCveQ2ZCElssDYLqbMAZK5a6O0sl6fE8sSGHIosNm7WUgo8ttT1DnerKWls2Sxenk1dqwWopZXdmKpuamtOzl2uxfHbyJmS6Jx+krCbzoAXLwSzWZJRy54xgvJywv78bTbiZzQ3+ajslGPqbcTP1hAvdLmIvjtKp2nK8OzsbaufrtTqWhyU7kfVH/ZkT5o1PRAz3WtNZmd6ZvX16gP9WUGEzMPB6l4sS3Jka7suBV1bzzscWKv9byj/Wb+R910AmDQdGBjKpz042rN/JZ5WArZx3X8ng/Rv8+YGe9mug5bqmdBTFuStYNicQMTuOlNxiysrLKdqWzKIN+ZjvCyPAiN3zpN36NwbGh4Zi+mcya3JKsZTnk5KUReWEUIJcuXLqI7ZiMlclkrmnmLLSQranxLNq71BmR9TNyWQv3W59xU6crxR241RK5kPBBM7Pqp3D0m7cOjudLrg/0gnsXWe0ejvYKEqPIybdStD8GMb1sVzIz2IFytOJmRnNovQ9FB0tp+zTPWQuS2Z337sIGm2gI8oZ675kYp7JwfiTWOaM4ML3l1m670aB/fL14jhaKUhJYE12PiVHSynYnETshkOMiwy7UE6MCg/HbWcSL+aUYrVaKNqURGaxJwETGrZYVWCpAJNp4MVr1HL+Rj+m3FVJ5tpkdqZz/WoAACAASURBVB+1AVZKNieTWezBFP+eMWxqUzo+zuc1F8d2lhs9NM49fqjPruQybhwDw8OpSE936POmX/yCfqNHt+s7I34xE6PRyCuvppKw6gVuuulGgqdOaXGZewInkfLqa0ybGsTg7363xTxWPP8CgZMm8v1htVeFMQ8/RNjPf0HO1n+wfUcukyYGXNKQOnbMaPpffz3bd+Ryy803c3/4z/j3wYPM+tWDjLjDh7WrX2jyPUeF3hfCTTfeyL79BXz44Uf0c+mH9/DvX7IeTtc4cdVVTb/O//Ajcv6xjb79+jLsdgcmQ2ylhOeWsn7Dyyxa/Cy2M2fwHHobEbNmNvv5IUO+x/DvD2PJc8vJTH/Dbh4tbZfJgZMuyf/o559zxmbjlltuvvDek08vYm/eBxdeTwv5MQDvbt/apuFT2yM0LJrpSVk8PsJE0IJErKtXk/L0LDY4uzMuNIp5IeWsb3JJd6bPj6JoVRK/DkvC5B1I5OxogmJT6z/iPZkgl/mseSiMlRd6exgY5BvO/8ZHMcp1Ms+uhhfXpfLEg4nY+rrjExjLujmX81NDFjbHhbJ07/nXhTwRmg0GP57dtIKgNjzIWJIWQ1hycf3rh0JJw0jI89k85Wc/vSOYJi/ghcokVsaFseqEEfOIyTy1Iqp2PHd3f8Zdn8yLD6VScgIGufsyPW6+5gnpBYxTFrC8Kpn1KXGklFqgrxlP/7kkRtfO52TblsSqbRYgl5VRuQ2WNNWVK9204r2AT0QMkUfXsfRXYWD2JSRyPnNOzOdCN5Yhvoy7NYM3EjII2hjFzWP88EjJ4JGZ8RyvNuI2IpCnYn9SO9yu62SeXQvr16XyzINJHMeE15jJzPS9Eobna8h++ewxI57lFfGsjAljDWbunB7P0yG1Q3tof3dcS3GUTtaG493kOwGflHUsTZpMwEK/lvM48R4vJh/AJyqVcUYAX+ZETyB8RSKZE1czfUhX/tguVGmhEiMuF7f7AQMnLWBZZRJrF/+cNf81Yr4jkMeWPchwA4Avc1Ys4JU1f+DRGUuopDZ94YoHGak5Lxtpsa4pHUZx7nym4DiWVyWxfkMMKUerMbh6MCpkCU81mNeopfOkvfo3gME7gsTHrCza8CChFiMeE2axfP4ErqgxcwwmPFwrWJMQzcoTzrgN82d2YjQXqhv20rFfX1Gca7UcJwM4gXODW4L24tbZ6V1xf6SjOXKd0artcCybNUn5WGtg0+JZbGqQo1fURlIjw0mIg/UpiczdUI4VIx4jAnls1VwC6u7jtGv/rykmbVUqRVVA+gLCGt6qHx3L+6u7Z8hh++XrxfuzEZOnicPr44hIqMYwxJeQ+auZM7FBFIaEsnyxlaVr5xO8zIbRw5d7l8QTeUuDL66xYrWCs8vF0bOXv4mghQlUr0ti5explJ2CQR53MX1JApE9t92vc+IMLcSxveVGz4zzVecaTsAmAFh37uTYunWcPdX0vGtXu7jw3YcewmXcuC5eM7lSvbbxdSwVFTzy8LzuXpVLFBQU4OPj00m52yhY+yBLvo0hOcb3QmFrK80i9sF1OMe+zfLJusIUERERka5TeeZsd6/CFcHl2stqgCIRERERkU71zTffAPDRRx/Z/aye+GuC0d+ffr6+fLNtG6c++ogzX34JwLXf/S59R47kOxMncnW/ft28lnKlsJ05w1t/+SsvJq7q7lXpBtWUHSmn0rl2KAGjEaixUJSfT4mzL5E9YnxvEREREREREREREZGeQU/8ifRwJSWl/GP7Dh6M/GV3r0qTOveJP6A8l/VrN/LXfeVUOxlwdnLB7D2BGZERBN2ihj8RERER6Vp64q9r6Ik/EREREZF6rXniTw1/ItIund7wJyIiIiLSg6jhr2uo4U9EREREpF5rGv5UkxYRERERERERERERERHpBdTwJyIiIiIiIiIiIiIiItILqOFPREREREREREREREREpBdQw5+IiIiIiIiIiIiIiIhIL6CGPxEREREREREREREREZFeQA1/IiIiIiIiIiIiIiIiIr2AGv5EREREREREREREREREegE1/ImIiIiIiIiIiIiIiIj0Amr4ExEREREREREREREREekF1PAnIiIiIiIiIiIiIiIi0guo4U9ERERERERERERERESkF1DDn4iIiIiIiIiIiIiIiEgvoIY/ERERERERERERERERkV5ADX8iIiIiIiIiIiIiIiIivYAa/kRERERERERERERERER6ATX8iYiIiIiIiIiIiIiIiPQCavgTERERERERERERERER6QXU8CciIiIiIiIiIiIiIiLSC6jhT0RERERERERERERERKQXUMOfiIiIiIiIiIiIiIiISC+ghj8RERERERERERERERGRXkANfyIiIiIiIiIiIiIiIiK9gBr+RERERERERERERERERHoBNfyJiIiIiIiIiIiIiIiI9AJq+BMRERERERERERERERHpBdTwJyIiIiIiIiIiIiIiItILXNPdK9CTHT30NZ/sP86Jr07BVTDAtS+33nED37u1f3evmoiIiIiIiIiIiIiIiEgjavhrwrEjVrKSPuLz/5y4JG3rGx9zk9cAQueOxGTu1w1rJyIiIiIiIiIiIiIiInIpDfV5kYP/+oLfP/luk41+531W9DXrY3dSnP9lF66Z9FZHjhwl40+Z3b0aIiIiIiIiIiIiIiJymVPDXwNflHzDm6s/wFZdY/ez1VXf8qcXPuCro9ZOXafq6mpiFz7NDyfdw30//Rnffvstk4Km8re33+nQ73k+8UXGjvfnX3vzOjTf5hQeOEDq62n8Lmk9Ka+ldli+615aT9L6De3Op/Szz4iM+h9+EBDI04sWU3zoEGPH+1Ne/kUHrCX8cNI9ZP5fFgA33DCItD+m88UXrcv75MmTbPpbNg/8ejZT772vQ9arrRoeBZb8dBZFhxEY4I9/yCxiNuyh7MIhZWPLwkAC4/d0w1peZqrKyctOZensUPxDE8lrVCy1L45lWfP5/+zde1hU1f748XdSgyjjbczTkAVdxErIX4SKWhAKeZmipiwojxRFXw2PCuUtFTNRE7VQPJKeSAqjA8mJskNqmIgdjY4cvIARmAmJTJlj1uBlxsb5/QFyUWAG5e7n9TzzPMysvddsPrP32mvttfba3sP9iNhiqs5Rl01CZCgaPz+G+GkJjkwkR391/4IQFzVcLlSmT6xM14YyPzmf5j27dlBn80maOaEijoEx7DZZWb4gnkDfMJJKW2TrOo4Gy2fArGd3XDja0X54ayYQEZ9LzeK0Mft7XeX1NcNKHEXrKtseQ6jWjyG+Gian6ihLDsM7MI48682oa8/uRTz20EOMvOz1CCtqNL3KD37KiqnP8NjDD/NY0GRWfHaI8ouJphK+inmF8Y88zJgxTzIpKoWD5XV9mTDkp7E4LBA/Xz/8xoWxOK1I6hTNQOLc/IoTQxky3LvWyzsyg6oagS31jQbq32CieEsMoeP88PbTEhyZTF49P2KHro8cz2btzAkV+3JgOMvTS6j1X1qrj1itr9ge5w6t0fU6a3Fr7nQDxbvSWbsgFD/fUBJ+vMr/v4VYa2c0vt1tPU6FqVGEajV4+2rQhkWz6ccaR1B+HNpLyrEhAZe0n47nVl2L8tZMYHJMBsUmAB1Jod6XlYNDhnszJCz5qmN1NayXr5cw69gaU2P/j8u6fPl642BDui6LtXMq00YHEhqdXntdW36HNqjNxbkdlgttuuOvoKCAsrKyK1p33759nDx50ublLRYLG1fl8uf5CzavYzKaSY3di8VyJVtom4yvtvPtt/9l3TtreOfvq7j++qafndVisZC182vuvusuMndkXZa+5p11LH4z2upnttqy9Us+3fRvbr/tNsY98TijH374ivKpi729PTfcoLjqfD5I/BCLBT7+6EPCp/6tCbasts4ODnTubF/xd+fOPPqIhn+lfdaoPMp0Or7M2Mbp06ebfPsaa3lW5ZnfkMWqqDQMXhHEf5RG4iwfDGnzmJ1U0rob2B4dzSY1I5cjBhPGpsxXn0FswiEUDrU+ZMuKaLbYaXjjvRTS4sJwL05k9pJ0ucgqrp61cqEonog56TA6ksSUFN6d2I/CdZHV5YqwmT4jkXUHVITEpZC8MhjPqz8dirpYKZ+LkyOZnaUiZMUGkqOfwD49kvlpuorExuzvdZbX144G4yhalzmfpLg0DMMjSfkonoWj1K29RW2bZzjrkv5JUtXrA5YE9cPxrkAeu69ymSMpzJkVz093vsiS+HdZ8oIrP+3IRld5seLg+vnE/jCA6YlfsnnjUsac/ZgFa7LpgJfgr86PyURMj+dIv1Bi3n+PmNB+HNlu5SKRaDyJc4swlhuwHxRGSkoKaZWv5HAfLlbvGjxP2tAuN+2PZ9aKXPpPjCP5/Uh8DRuYvSzr8k6AjlwfMZeQEBnF1i5BrElJIT7MlcLV4SyqUS+zVh+xlm5znDu4xtbrrMWtudMxl5D5SQZ5OgOm9lK2WWtnXEG721qcDLvieCXRgO+SZHZuS2bhoBJi58VTeDFm5QYMPXyY+1F1OZa2LpR77S5+QT6xr84k1eDF9NUbSFwShOr7DHKOA6gJWFJjvZQU0hJm4atSMWqcf/PE0BaNvu5pIi9uJssPODM1LoW0mGB6fxNFxLr86npcg3Gwkn42l+VTlrH7xkCWbkghOTqI3ntiiFhTI39rv0Nb1NbiDO2yXGjTHX/du3cnJyen0R0bJSUlHD16FEdHR5vXKco9fkV37+mO/M6R/BONXu/CBds6GE+ePEmfPn3o79oPJ3XzNKq/KyjgD4OB8c8EkvX115dt22+nLp/2tK7PbHHkSDE7snbyxGMBPDB8GDfddBNq9U1XlFddHJWOdOumrDfd5rj/9ht33eWKWn0TvXv3bqrNq9KzRw969uxZ9d5/5Ai+2LwFSyN6kfvdeSd/XxVD4NNPNfn2NdbuzNyKglTpw7zEeGKCvXBRq3AZHswL3vYU75FR+o3mqmXpypVEP9UP+ybL1MDudfHkDdTiq6r5uYqABYlsWKDF01mFk6s/k8Z7YMrN5oBcURJXy1q54BrI0nVxLNS64dRHTf/RYbzwgKm6XBE2M546CTe64emqxkmtQvr9mklD5bM5n41pRdwfHEaAmxonNy0zxvcjb2N6RQPY5v29vvL6GmEtjqKVlWMoV3DbwKG4qNWo6q96CwCFIzfdrK5+ORxi85cmxkwJpJ8dQDlff7CBQ3eFMmeqHwNudWbAw9OIjZlQmV7Cvv/p6Pfw49zXC3Dsx2NPecOBvRyS46EGA5kJGyi6K5SFEf64OzvjPjqC+NXB9G/LF7baHYlzSzEYTCj6uODSV43TxZeqsnZn7TxptV1uYldaOuU+k5nm74pTXw9CwrQ4/ieFTbX6Yjp4faQgnU2HBxASrqF/HxUuPmHMGOfIjpR0ysB6nK3WV2yNcwfX6Hqdtbg1dzpg50bIWytZM0uDS3sp26y1Mxrd7rYep+L/5mIYqOGpu5Vgp8R9fCCeJ3PZXTmjjMlQjqm7M/1rlmNqZVU7VZ8eT+pJH6YvCMX3bjUuAzUsXLeMcX0r0pV9aqzXV40hM408t1CmjWjFAqmx1z1NuWzcUo5v6GR8b1ehcvVnxlR/yjelkFl5ad1aHBpMd/DghbfiWRPhj7tahdNALTPGu6HflVF1fFn7HdqkthZnaJflQpN3/JnNZszmpmmBODk54ezszI4dO/jjjz9sWken07F37168vLxQKGzfhQ/tPW59oatYt+jQIUY8PIbvCwsZ/1wII/xHYzabsVgsbEj6iEe1TzLi4TGEvzqD0tJjAKx/P5F31r1LyU8/MWS4N+++l1Bn3g3lEbf2HzwZ+Aym8+cB+PHIEYb7jKCwsKhq/cwdO7lv4EAGDfLk5MnfyMvPr0p78uln+Pzf6Wz6dzpDhnvzyoxZdX4GkJyykVdnzuarzB08/+JLaJ8KYuHiJRgM1fPQ7M7ORq1Wc889dzcyyrW9s+5dYmJX8+ayFSxYuIjsb/8LQHdld7p371613Keffc7c+QvYvGUrmgAt4a/OACqmyVyydBkjR41hbMDjLF2+gjNnzgAwJfwVvsn+lrRPNzFkuDe5e/fWuQ315WGxWHjp5cksiFpctey/0j7lkcef5Oy5cwCoVL1QqXpVpbu4OGPBQknJT1cVl9ZiMlVXExTK2ld/jCbAwbH2CcXOQF5qFMFaP7x9tQRHZ9Q/QlSfz6bVkQSP0+Dt7YdmQiRJ+TWqJaYStq6eSaBWg3fN29Z9I8mszLNsVzyzQ7R4e1dM39FhpqJpTBwB0/4NrNqpJiTUC+Wlyylrn/RNJhN0ccS+8mRmKslg+ZRA/LwrpoSYvS6LMumVETZquFxQ4uRcM12JyrF2uSKsK04IJWhdEcYfEwn29iYwvgTMenJSY4iYoMW7cjqXxZdOX1SLgZzESIIDKqbv006JJjW/urQ0lWQQ+2rlVKIBocxOkEEdlynNJU/vgufA6sao6l431Lp88vRg6/7eYHl9LbAaR9E0KqYOD03KJXN1OFo/P4ITK0bR1nu8m7KYr5nJVoOJzAV+tepbl+VeXx5mHUlhfmiiqkeMl6WF4zcumt0dooLWsIMfb2DvvRP464DKmpcpl69zTAx52I+6h0M60qsXlBw4yMX5bMp/1WNSOqJsJxcdWoQpl8w9RoaN9sOptbelI5M4txAThnJDrWsGtdhwnmyw/m0uIi/fiKvHgOo24B0e3KssIq9GW7uj10dMOh36Hi7cViNU/Qd5oCrKrZjO0FqcraXbGOcOr7H1Omtxa+70dstaO6OR7W4b4qRUKaEolwMX6296HSfoharyPhxDuQF61FdfMbArKx/lcH+G97Dh39Ons3YTPPq8htYeh2DTdc+LTunQn1Hh1Lc6VTHQi/vJJed7sB4H63FSOaup9cs6KsFUPTtNw79D29XW4tweNXnHX0ZGBhkZGU2W38CBA+nUqRPbt2/nyJEj9S5nNpv57rvv2L17N926dePGG29s1PfodVc+XaKt654+fZqly95i/tw5bP3ic+zs7Phnysds/XIbSxcv4uN/fsg9d9/Nq7NmYzabeTboaUKem0Dfvjez9YvPmTD+mTrzbSiPF0Oe44L5Av9MTgEg7p1/8OgjY+nf37Vq/cysLAYNup8e3bvj6tqP7ZnV030mJyUyeJAnj4wdw9eZ21j25uI6P7so+9v/8u1/97AmdhXr//EOpUePkb55S1X6Tz8dxfnWWxoV37q8PPElIqZO4bWZ01kwfx5eQwYD4OP9AN4PPFBr2e++K+DLjG0kJrzHiug3AXj9jUWcOXOGD9bH8+7aOH799QSrVq8B4M1FUXje74FmzGi2fvE597q717kN9eVx3XXXMXvGdDK2fcWBvHzOnTvHewnvM+1vYTh07lyx7rw53HnHHbXy69/Ple8KCq46Nq3B5S7Xywtes4HCLTGsze7Fo0951ToJGbbHseqwG1Oj32NNhAeG9JWs/U/dlQ3D9zvZbRxASGQcyR+tJMQln9gliVUjVwoT57H4GxWTVqexMzOFpVpnlAODiYkLw9MODHtimDwvHaNvOO9ueI+YYGcK10QwO7X9D7FrTBwxF5G0Oh37ZyOqR6vUyYR+fxrLkw7hMk6Lpx2Ajo3R0ezuE0pi+jbS3pqAuwO07eFBok1qoFyoXqaI3fuNdZcrol4u42NYE+KKvXMQazalEx/sDPpcMvfbM3xSFIkfbeBtrSOZK6LZWE/xZ8qO4/VEPb5LUti2KZ6F/mq4eE+bPotF4fEcGTiZ+NR00lYEosyIZNE1NVzZBno9etT0rtkaVanpjY6yusaJ1bW/21xed2CNjaO4KsUp0aTaBbHm8y+IH+/c8PGu8GFeUiS+DgqGzUpjW1okvnVdRGgoDzs148ODUWXFk7DfBGezSUgqwTM0lGEd/e7Bszv57EsTIx/zpmpump9L+MnkTL+76putRsXI0BdxzlnElFnvsPnrT4n94BBDgsZyawttdrugK6HY6EL/uzv6TtTKJM4txIjhDOizlhGo8cM7IJDQBcnkXJx4qTHnybrq32Y9ZXp7ete8ZdtOhZMKThyv7Im5BuojCqUjinI9J2qOK7azR2E+SdlJrMfZWrotcb4WNLp+bCVuzZ3eUVhrV1tNtx4nF20Y47qk88rESNZuySIpJgXDqCBGV/7WBkM5xtJ0Zo/T4D1aS+CUaFILKnsJzSUUloKLWz+b2v3F6Wn873YNT7laX7bF2HJ9w8ERhZ2eEzUnzjPbg8LIieMG63FoZJzARM6efOjnxm2VdfQGf4f2oE3GuX1o8o6/zp0707myc6MpHD58mAsXLnDfffdRUFDA5s2bOXjwIKWlpRw/fpyjR4+yd+9e0tPTOXHiBCNHjuTcuXMUNLID5cKFK39Qn63TRwK8+MJz9Hfth729PefOneMf8euJnPsaA+65m94qFf8X+gK///4Hh374gc6dO2Nv3xm7Tnb06N69zrhay8Pe3p7pr4TzfuKHZO38mv15ebz8f/9Xtf4PPxymtPQYgz09AfAaPJgdO7+uSr/hhhvo1KkTnTp1QqFQcP3119f52UU9e/ZkzqwZdO3ahZ49e3L//fdxpLi4Kv3M2TN069at6v13Bd8Tv77uOxmvRJcuXejSpfbk82fOniVy7hxUql4oFAr27ttPXn4+r0fOpe/NN3OzkxN/C5vEl9u+AsDRsSvXX389CnsFPbp3r/O5itbyuOP22wh8ehwrY/9O8scbufWWW/D3G1m1/o033nhZvqreKk7+9luTxaIljdc413pfljYTP18NwUuycZq4iGmDahfL9oPDiJmlxdPVGXdNIKOcjRT/WPfFY+XwMJZOD8J3oDNOfd0YF+iPky63co5lA4VFOlRDNfj2VYBCja9mKMqiEox3qFGiZ2tiOgafcBYG+9Df2Rl3TQQLx6vJSUwkpz0P5qJxcSxLiyPJqGVGkHOd6QAUJRLs58fYsDUUuoWzNLiyAmg2YDgFyj7O9FYqUN3uw/hgHxnlKxrFWrlQtVx6PJt+H3pZuSKsUChRKgA7BcoelX/38WdGVBjjhrvhonbGMyiIYfZF5Oyvu5JvPFWOwV6FUx8VSqUa94BgxrlVVHuL0zaw2y2UhcFeuKiUqFz9mRrUj/9tz5a7/mqqHFlZa9IJBSgwVYxQvERd+7tN5XVH18g4iqtj6KNhxkQvnBwUKOysH++KHhWzBNg7KFH2qLtpbLXMcA1ixhOQGpdMZnI8mepgpo5u7fHbzc+Us5Nv7bx48N4acTOUY0CJoiSFJZOeZMzIh3nqhfkk5VSXrgpVP+65XY39r9uIjVzJ1z3G8tSwjh+vRjGUY8ARRXEy80O1VTOFJOyRs1STkji3ECWemlBCJkYSv2kb21aH0b8kntlRlVNQ2nierLf+bTRhRFF7/cr3F+/+uSbqIwO8GKbIIikxF73JhD4/jUVvpVc9X9VqnK2l2xDna0Jj63XW4tbc6R2EtXa11Xa3LXHqosZ9gAsqu2JSl0QSe6Af45/yqOo4cfLSMil4Mks3pLMzOZrxqlxiZ62snHqxHEO5PUpzPglzKmeI0IYyPzm/jhm6SsjMLMZ9hHer3+13ka3XN1B64DvQxNbEZPJOmTDpckmKWsNuA5gwYT0OjYkT8GMy67fDqKeqY9Xw79C2tdk4txOX92hcpYceeqhJ8/vtt98YNmwYPXv25NZbb2Xfvn2UlJTg4ODA+fPncXBw4JdffuGBBx7gppsqJkcZOnQohYWFjfqe7qor76zsprL9Kcd33l59l9cPhw9z9uxZwqZMrbXM+fN/cuKEHvpbz8+WPIYPG8r9993H7LmRvBoxje7dqzvetu/IordKxZ13VmyX15BBfLDhQwq+/56777rL5v/rIju72kN+u3TpyrFj1R0Rne07YzBUHzJ33nE7Nzs1vvvgnXXvcs54jnPnjBjPnWP0qIer7vq7VM8ePejVq/p5egfy8vnDYGD0IwG1lvvz/HlOnz5N165drX6/LXm89OILPPl0EPHr3+f9+H9YzbNLF4dGP8+yrRh1yZnXSRNJooeO4vxtJMWHEXYqmrgQt6qTv8JBWT1Cw06JsgsYzxqpj6Egg9QtO8k5UEyZ4SR6cy8MBkCtxHOoG7Eb08kMdMNXpWf3lm/Qu/hzmwIw53OgANynetQaEeIyyAOnxFzydODZjtsxNsfxeDqxCTpGLViGuwKob5qWO4KI2eBD2Y+5bEmM46WZ5axZpsXFzpWnpj5BTlQY2lwfHtVoCBjtgVM7HwZjyopC+2Z25fQHCoa9lsJCn3b+T7Vh1soFAErSWLzuIP1D4i8rV8SVMR3PZ0t6Orv3HOKIXo/+jIn+Z+tu0CpHhDJ110wWB09gi5+GgAANvq5KwMCBgiIMe6LR+EXXXukOLwzQZhpgrU6hwB4wmYCL1UOTCRMK7C+tLta1v9taXnd0jYmjuGpKZ9caz6loiuPdljwUuE+YzKiMmcxOdCVknfaaGFB0MGcvDHiZATWbTHaAOZ9PN9/H1DkfMP0mIweT5rNgwSpuSlrISMdDJM1ZzA9j3mb9Y86cPPgpa9+KY84CFauXjK1netBrUGUcU9M9mBGZyDy1iQOJkcyOXIlTchSjOtC0Ta1K4txinEYEMf7iG2cfpk3MZsdrGew+rmGcjefJeuvf9grsMVG7j6PivcJBce3UR5Q+zIjSsXhFJNokIyo3DSHPaymLykfpiA31ESvp1uJ8rWhsvc5a3Jo7vSOw1q62pd1tNU4mcmJnElseSvwGH5S6XDaujmZFeDTKdbPwVYHCVUPIxTv0HFwJCA8lc1w0W/fMwncE2NsZ2PHJTjwjlvFFlIoT2WuYPS+SWJcU5nrV+C2O55JTrMZ9YNtpddp0fQMAFQGzIimLXkOENh76uBEwMZhRh6PRO1Zc0WswDoMaESdTEQlLEtENnUW0V/VV0AZ/B/+2vc+3VnbCrQAAIABJREFUyTi3I03e8dfUPCvvRAO47rrr0Ov1uLu7c8st1dNF7tq1i5MnT1Z1/PXs2RMvL69Gfc9tbr3Zl1V6Rdt4u1vvK1rv4l2GG95fj5Na3Wx5mEwmDv/4I127duX06TO10nZkZXHyt98YOWoMAJbKGx8zd+y8oo6/ulgs1XdT9u3bl5Kfqp9jp1AoGvUsxotenvjSVWzPBfrceCOffbKxWfPQ6XQYjUauu+46zpw9azXPs2fOcmPvK9uX2hxFxdzhTs6ueDrqGRuVyBbtMgIaaAjWN66qODWcyYkmRk0MZW6oB06/JhL8YvV0wk5+Qfgmx7D21UBWnFWgvtuHpVHBuNTMpJ3NY3016opjWUYamad02L82ls8rPzOeNUH0WPy2R5CyrHKOdDsFKrUzKrUz7ncpKB4XR1K2hrnDFai8wohP1ZKTkUZqciRByd68sXpWu37Iu+KBcBKTajyfsp67FkQTsVYuGHKJnbeGYo9ZJI67snOiqM2UH0/YrAyU2lBeiIzAvU8+y8fNpLi+izcKZ8ZFpeBbkMWmtBRWTNxA6sQY1gSpAQVKzSK2zWpc/eqa00eNioMV0xZdHFii13MCNb59aixXz/5urbzetkzTQv9IK7M1jqKZNMXxbkMevxZReFaJ0s6I6ayJjj+HeAnfFRpQD7tkCiGVGrWiH0OmTmDITQCO3Dc+kPs+Xsbe72GkXRqf6r2Z80jFwdBrwOPMiTIyNWQDn34/lklN02Rr/1RqnBSuDIsIZlhlseoZHIhn8jJyvodRcvpqGhLnVqPoo0ZFPoZybD9P1lf/Vqpw6mPkyHEDXBxKatZTpofefVTXVH1EOTCIpRuCqt6bdkWzTumMSw/grJU4W0u3azjO14zG1uusxa2509s7a+1qW9vd1uJkyCJ1K4yK86m4nqT2YHxUJPoJ4SRlBOMbVEfeShVOSiguNwC9UPVScP/4cMZ5VOTvNDyYgIHpbNxfDF7Vc3qaDuVTaOfStqYdbsx1zz5eTHrLi0kX3xvSiVjiwr0uCqzGYZCtcdKTGR1JgkHD0hX+DQ/Uq/U7tPF9vs3FuX1p8qk+m9OFCxfo1KkTTpfcIda/f3/O2tCx0pABQ53o2q3xjU1lz87cNejKxlnecfttKBQKdu78zxWtb2seCR8kolbfxJKoBbyX8D5luoo78I4eLeXwj0d4PXIuH76fwIfvJ5D0QQJ+I3zJ3FH9nL/rr78es7n2VcK6PrOF12BPysp0FBR83+h1m8rdd93Fz7/8QtGhQ82ax7IVb/NsUCDPBgWydNly/vzzzwbzPPnbb/Ts0d6HSBowXHIoKroosDdfOkrIRuYiNm3MxyU4iqkaD5zquKO7OC2enEGzSNmQQnrqBuKjQqsaodi50t8VinJza92aXZybS5myH/2vkb4Fp4Bo0lJTSN6woeL1fiQBfRR4TnyPxFkVFQKD4ZIfqIsCpV3FsyWqOKjxDAhj6XuRjDqbQequ9nzDO2CnRKVSVb3a24OO2w8bygVTEUlzIkm1D2LpHCuVVGGznE1pFHmEEh3qj7va9jqO6m4fQubE8e5EZ/JS08kzK+nvqsaw55t2P0Vys1O74akq5sD+6inPynJz0fV1w/3iKb6B/d1aeX3NsCWOopk0xfFuSx46UmNSYFw0MU9B6spECjt6+WLWUVIKvW665Cx3433c17eEgwfLqz8zmQD7ium1TGAyG2vfcdNdRS9MmK6uCdyx9PHA85YS8vJr1E+NFR3KVzDOVNRH4twyzHr0lzS19AX5FNu54HQjNpwnrdS/7Vxxd7Mnr2Y7uSiXA2fc8HRTXMP1EQO7MrLBy4d77bAeZ2vpVuJ8zWhsvc5a3Jo7vT2z1q5uTLvbapxMGP8Eal7qtFOh6gGGcgNgQq+/pHKnK6LwlBIntRLsXPAcqKI4/1CNAewV07/aK+xrrXaiWIepjxqnNnHN5uqve+ozs8hTezHMGetxsClOBvLWzeL1bDVT35zMsFrHlZXfoc1qi3Fuf9pVx1+nTp0YMWLEZdNJ9u7dm/vvv/+q8lbY2/HoS/c2bqXr4LFJ93L9DVcWxq5duxL6wvOsfTeef6d/wfHjv3IgL58NSR81uN4NNyg4fvxXLBaL1TyOHi3lw4+S+VvYywweNIhhQ71YtuJtoGKaz549euA/cgRq9U1VL83YMfx09Cg/HjkCwK233MLeffsp+eknjpWV1fuZLfr378/gwYPY+K9P+M+u3fz8888cKyvj2//uwWfkw/wvdy8Av506xaixj7L+/cQ631+NIYMHMWTwIOZEvs5/9+Tw66+/krXza7Ztz6x3HcUNFSf/X44ftymP9M1bOHq0lGeDAgl5bgKnT58h6Z8pDW5XYdEh7upvw/yulQwGA3r9SU6fPo3lggW9/iR6/Umb129KZaU6DCYwbFlG0PORJGzPp1inozg3ncVxGRgH+jDsSkbo29mj7ALF+7MpPK6nODed2Jg0Cmtc+DCajBh+zCenVI9er0d/yoCpKl1NwHh/FNtXsjgpm8LSEvK2xDD/Ix3uzwYxrA3X6UyGiv/HcMYEZhMGvR693lDvnZENUqpwUqtrvCo6uRQ91DipFKBLJmJ8GPOTsyks1VH2YzapS+LZ3WUoowYp4FQ2y+dEk5qrw2AyoS/K54hBiVOf9nvyEy3Harlg1rEpahaxpR5MfVWDSq+jrFRXVa6IK6d0cMR4OJddJXrKirJJjY5ja81Kv70CewzoK68sFSdHMXtdBoV6EyZDCXnf6ytG6NpBf20wvqZ0Fi9IJqdEj0Ffwu7URDb92Er/XCtqsHy2cyNgnCv/S1hJaoEefUEasSkl3P+Uhv52WN/frZXX1wprcRTN6kqOd3t7BZzVc8JgWx769BjWlnozKdAN9+AIHjUkszy5pPn/udb0+0lOmhT06u54SYIzY4I8OLh+JZu/11P+ewlfrd3At338GDkAuM+PkQ47Wbd2Jz+VAyYdX69P4dsbvXlQ7varwZmAZz3Ii1/JpgI9hlMlbI1LZHcff0a7tfa2dSQS55ag3xJN8MRIErKKKNPpKNwez/x1uagfC8RXidXzpPV2uYLhWi2q/8QTm1GCXpdLQlwa5T5aRvXh2qmPmIpIXRFDanYRZSX5ZCZEsWJPPyYGVz6jzFp9xGp9xUqcrxVW41RC6hQNfuFpFc+wtBq35k4HTIbK60vlGDFiOFVZ32/L095aa2dYbXc38ndQejF6aDmpq+PZXWoCDBRviSe1yIXR3q5Q8gmzgsNYnFpxnal4fwbLF2ygyFXL+EEKQIFnUBBOO+NYlVGCwaCncFMcqUWu+PrUfiaP/rgOevRqE4O1rZevl8bRQF5CNLHpuRSXlpC3JY5Z6w4xLCSwqpxoOA7W0k0UJkcSkWxgVHgEwxz0Vb+r3oANv0Pb1PbiTLssF9r8VJ8t6R4vNQ+Nc2VHapFNy/sF3YWrx1+u6juD/zoepVLJ+vcTiV7xNrfeeguaMaMbXOdhv5EkvP8BY8eM4qa//KXBPJa99TZ+I0dwz90VrcKIaVMIfOavZGz7iswdWYwc4XtZR+qQwYPo0b07mTuyuP2223g26Gm+KyhgwvMvMvBed1avfLvOz2ylfSyAW2+5hf0H8ti7dx9dHbviNuCey7bD7no7rruu/vdXI/rNxaxd9y7zFyzEdP48rv3uJHjC+HqX79v3ZgbcczeL3lxKavJHDeZhKC9n9Zp3+NvLE3FwqJiwPGLaFOa/EYW/34g6p2QtPXaM8yYTt99+m83/w2vz5rMn539V78cGPA7A15nbrmj61KuhDQxjXFwaM0bPZOnZeNYmRJJQoocualy9JxMTdqXPbXFmXHgohSvieCkwDpWbHyETwxg1q7oDuL+/BpfkeF55Nh7jxcJWoaK/JpyY6T6ohs9izYJ4Yt+P5qV1BhR9XPENjWbquLb8cD89WyK1LN5z8X0+s7XpoPBi4aZljGrqQTnqIKIjYW1CDJPX6TCgxGWgH9NXTMa3B2Duh++9WSSsCCW21AAqV4aFRDG1nc5xLVqW0kq5YNoex4rteiCL5aFZNdZUVZQrA1tpwzsA9+AIQkrXsPj5QFB7EBASzqRT4VQNc+nrwbA7UvgoOoVRG0K5bbAXLgkpvDI+ihNGJU4D/Zg764mK8ruPPwtXw9o1ibz+YhwnUNF/sD/jPa6F6flqsl4+uzwVxdKTUSyPCCQWNfePi2JeQMW5X/Z32zUUR9HMruB4V3n44J6whsVx/vjO8Wo4j1PfsCr+IO6hiQxTAngwKcyHoGUxpI5Y2bamcWpK5XrKUeJ4ab8f0GvkTJaUx7F6wTPE/q5Efa8f05e8yAAFgAeTls1kfex7vPrUIsqpSJ+z7EXuk2de1qLyn8nb5XEsjwxkxSkl6oH+zF0WWvGMMtFkJM7NT6WJZOnZONauiyCh1IiijwueAYuYW+O5Rg2dJ63VvwEUbsHETDcwf92LaPVKXHwmsDTch7Z8/0eTU6hw6XOS2Ogwlp+yx+lubybGhFGzumGtPmItXeJcoeE4KcAO7GtcErQWt+ZOL06KIDC++tpw8RQtSSgJeCuduW10SmNr7Yxpx621Qxr7O6gYNSca45o4lk8cS9kZ6O0ylHGLoiufJxdE9AJYlRDD5DU6TF3UuHpNYM2UoOpnS/fVsnSBgcWrw9EsMaF08eDRRVGE3F77fzvxuwEclG3iuLFevl4aRyUqVxVH1kYSHG1E0deDgPCVTBpR47+xFoeG0o+nExuXi8EMmxZMYFONbe0fuoHEEBt+hzaozcWZ9lkuXGep+QA2AUDermN8/o8DnDtT99SMDl1v4LGXB3LPELn4IJrGBxs+RH/yJK9Mm9ram9JoeXl5uLu7t9K3l5AwcSZHAuNYOOLiJAUmyjKieWmJnpCUlYy7lkbSCSGEEEKIZld+/kJrb8I1wfEKZ9YRQgghhBCiI/rjjz8A2Ldvn9Vl5Y6/OrgPv5k7/18f9mUd5fD+E5z6teIhVz37dOGOgTfy/3z60rnLDa28laKjMJ0/z6effc6qmBUAnD59msDxwfUu/9ijj/DSiyEttXltm1lHWWk5Bp0eg7nyGW2nisnbU4TxDn/c5UFhQgghhBBCCCGEEEIIIa4hcsefEK2suLiErzJ38GLIc629KVekde/4A/2eRFbFp7O7pBwUCuwdVLh6aXkhRFP3g6KFEEIIIYS4CnLHX8uQO/6EEEIIIYSo1pg7/qTjTwhxVVq7408IIYQQQoiWJB1/LUM6/oQQQgghhKjWmI4/qUkLIYQQQgghhBBCCCGEEEII0QFIx58QQgghhBBCCCGEEEIIIYQQHYB0/AkhhBBCCCGEEEIIIYQQQgjRAUjHnxBCCCGEEEIIIYQQQgghhBAdgHT8CSGEEEIIIYQQQgghhBBCCNEBSMefEEIIIYQQQgghhBBCCCGEEB2AdPwJIYQQQgghhBBCCCGEEEII0QFIx58QQgghhBBCCCGEEEIIIYQQHYB0/AkhhBBCCCGEEEIIIYQQQgjRAUjHnxBCCCGEEEIIIYQQQgghhBAdgHT8CSGEEEIIIYQQQgghhBBCCNEBSMefEEIIIYQQQgghhBBCCCGEEB2AdPwJIYQQQgghhBBCCCGEEEII0QFIx58QQgghhBBCCCGEEEIIIYQQHYB0/AkhhBBCCCGEEEIIIYQQQgjRAUjHnxBCCCGEEEIIIYQQQgghhBAdgHT8CSGEEEIIIYQQQgghhBBCCNEBSMefEEIIIYQQQgghhBBCCCGEEB2AdPwJIYQQQgghhBBCCCGEEEII0QFIx58QQgghhBBCCCGEEEIIIYQQHYB0/AkhhBBCCCGEEEIIIYQQQgjRAUjHnxBCCCGEEEIIIYQQQgghhBAdgHT8CSGEEEIIIYQQQgghhBBCCNEBSMefEEIIIYQQQgghhBBCCCGEEB2AdPwJIYQQQgghhBBCCCGEEEII0QFIx58QQgghhBBCCCGEEEIIIYQQHcD1rb0BbdmJI0X8/P0BTuuPA+DY+y/cdPdAVM53tvKWCSGEEEIIIYQQQgghhBBCCFGbdPzV4feyn/gm8e/oiw9dnvjph9x4x114BU+h21+cWn7jhBBCCCGEEEIIIYQQQgghhKiDTPV5iaP7stkSPavuTr9Kvx7+ni1vTudY3v9acMuEqNvRo6WkfJza2pshhBBCCCGEEEIIIYQQQohWJh1/NfxWWsyu997mT+M5q8ueP3eW/8Sv4HfdUZvz//nnn5kT+Tqjxj6Kr/9oXp05m8Ki6g7GV2bMYvqs165o2xuj6NAhhgz3Rqf72eZ1/pX2KU8GPoPPyId54aWJ7D+QVyt95Kgx/PuLzU29qc2u5KefCAn9Px709WPe/AWtvTk2e2XGLF6bNx+AG2/sTdI/k/n5Z9t/z6Y2ZLiGxXsATGyd44dfVHarbUtHoc9NZv7EQPx8vfHWhjI/OR9DzQVMJWxdEY52tB/eflqCFySTZ6gvtythoDAtmsmBGry9/dCERJKwR197if3JzA7R4u3rh2ZiFKn5TboBogPR5yYzP6xyfw6YQMS6bMrMl6Q3tL8L25zNJ2nmhIo4Bsaw22Rl+YJ4An3DSCptka3rOM7qyElPZPFELd7aGHLMl6Sb9eyOqyyfNROIiM+lZunZmP29LC0c7+F+RGyx9mN2QFbiKJqZPoPZARqCE4ua/avKksPwDowj79JjqQ55qwMZMty77pd3Gy3Pdi/isYceYuRlr0dYkVO9WPnBT1kx9Rkee/hhHguazIrPDlF+MdFUwlcxrzD+kYcZM+ZJJkWlcLC8ri+7tkl9ouUY8tNYHBaIn68ffuPCWJxWJLFuYsWJoZeVc96RGVTVCGypbzRQ/wYTxVtiCB1X2Z6MrL892dHrIw3uz9bqI1brK7bHuUNrdL3OWtyaO91A8a501i4Ixc83lIQfr/L/byFWj3tr7ZhLmXVsjanxu8VlNbIcacI4FyUS7OuN35LWv95ovXy9hJU4mnTZJESGovHzY4ifluDIRHLqOUDqLI/z49BeWi8OqP59G5N/W9LycW6C8+Lx3Krv8NZMYHJMBsWteOps0x1/BQUFlJWVXdG6+/bt4+TJkzYvb7FY2PXe25jPn7d5nT+N59i1fiVYLFaXNZlMTAl/lXPnzhH95mJWvrWcG2/sTdTiN7FUrj/Y05NB999v8/e3lNR/pfFB4ofMmv4KH214H7cBA3hlxkx+++231t60q/ZB4odYLPDxRx8SPvVvrb05NnPo3JnOnTsD0LlzZx59RMO/0j5rte1R3h3IOI9W+/qOpyieiDnpMDqSxJQU3p3Yj8J1kSzPqj7j5MXPY/khN+YmbWNnWjQBZ1KYvTqbpjqflKVGMjmpnGERcaSlvsf0QXqSIqPZpKtc4Hg68+ekYBwVRXLqBt4YridhVhSbjjfRBoiOw5DFqqg0DF4RxH+URuIsHwxp85idVFKRbsP+Lmyjz0hk3QEVIXEpJK8MxlPR2lvUQR3NJjUjlyMGE8Y6kouTI5mdpSJkxQaSo5/APj2S+WmVhWdj9nd9BrEJh1A4NOt/02Y1GEfR/JTOeA73Ytgdqtbeklrcn48jLTWFtNQUEsPcQOnD3JSK92kpUTylbu0trINnOOuS/klS1esDlgT1w/GuQB67r3KZIynMmRXPT3e+yJL4d1nygis/7chGV3mx4uD6+cT+MIDpiV+yeeNSxpz9mAVrmq7e1yFIfaLl/JhMxPR4jvQLJeb994gJ7ceR7VYuxolGM5YbsB8URkpKCmmVr+RwHy5W7xo8T1qrfwOm/fHMWpFL/4lxJL8fia9hA7OXZV3egdvR6yNW9mdr9RFr6TbHuYNrbL3OWtyaOx1zCZmfZJCnM2BqL2WbDce9tXZMbSby4may/IAzU+NSSIsJpvc3UUSsy6+qf7RcnHWkrk6hzL4NNHBtiXMt1uKoZ8uKaLbYaXjjvRTS4sJwL05k9pL0yzvH6yuPyw0Yevgw96Pq80XaulDutWtk/m1JK8T5qs+LpnxiX51JqsGL6as3kLgkCNX3GeS05nVSSxt27Ngxy2effWYpLy9v1HrFxcWWTZs2WYxGo83rlB7YY/lw4uNX9NIV7Lea/779+y2Dhz1oOXFCX+vzxmxjUyksKrIMHvagpaxMZ9PyZ86csfz8yy9V781ms8V/jMayeeuXVZ+NeHi05d9fbG7ybW1u016Zbnlz2fKrysNsNjfR1tj+Xcveetuy6u9rqj4/cqTYMvbRxy0XLlxosW256MCBA5ZVeRf3Y6Nly2sjLSMXftPi29Gx/GE5VvxHrfdb5o21jHx9h6Ui0sWW9c+PtIR9cqJ6kf+9bRn79BrLgT+baBPOlFmO/FLj/Z//syx7fKQl7LOK7Tr24YuWB//vn5ZjVQuUWTa+PNLy9NrCJtoA0ZEY/6i5PxstuxaPtTz4t08sFXuwtf1d2OrYBy9aHvzru5bvbS0HvnvX8vRDL1s+PNqsm9VhnfhkmuXBx9+27KkZ7z/zLMueHGkJ/3d1+Xxi4zTLg89c/F1s3d//sOxa/LRl7Lx3LYueHmkJ33yNHQ1W4yg6kmP/fNny4BXUYf74bIblwVFLLbtaYZ8wmMxX/vp5q+XVR/5qWb73bOVnv1u+mDPG8sDUf1kO1bnOj5a1z420TNx4vPqzb9+2jH7q75bss1exHe3g1ThSn2gZf1i2zxtreXDaJzXaAaI57Fn6uGXk4nra1TacJxuufxst218faxm78Jvq46Owsl5YVvOLOnp9xMr+bC3OVn8HW+PcwTW6Xmctbs2dXsPhDywTHnrRsv7w1YWgpTR83Fersx1zWWbfWCLHPm5Z9J/q497436WWsaPmWbb8VpF/S8X5xL9nWEY+/bblndfH1l8utiBb41yRbC2OFovljz9q1VX+2DzD8uBD8yzbaxW59ZfHxi/nNdxOsin/tqdl43z158UTn0yzPDh2oWX7b5Zm9fvvv1t+//13S1ZWltVXk9/xZzabMZubZjiEk5MTzs7O7Nixgz/++MOmdXQ6HXv37sXLywuFwvaRAGX5uVe6mZQdtL5uly5dADiQn1/r85rbOGP2HBYuWlL1PjllI6/OnM3mrV/y5NPP4D3Cn9lzIzEYDKyM/TsPj32ERx5/ks1bv6xap+jQIbxH+LNteybBL4Ti6z+aZ/76HHn5BxvcvtOnT7Nk6TJGjhrD2IDHWbp8BWfOnAHAwcGBv/TpU7Vsp06d6NatG7///nutPMxmM8vfjsFvtIann/kr27Zn1kr/+ZfjvLPuXea9/garVq/hvzk5VXc7Xqk9Of/jo+QUDuTn8+ayFUx7ZQZvxazCYCivtcyEkBd50NePoPHBZHy1HYAp4a/wTfa3pH26iSHDvcnduxeA4uISpr0yHe8R/ozWBBCzajUmU/V42uSUjUyf9RpfbN6CJkBL+KszgIrf74MNHxK3dh2jxj6K36ixvJfwAb8cP86M2XPw9R/Ns8HP15reFWDX7m8Y/1wI3iP8CX4hlP/u2WP1u1S9VKh6VY/AdnFxxoKFkpKfriqeVyrE7ZJjzc5AXmoUwVo/vH21BEdn1D8CVJ/PptWRBI+rnFJyQiRJ+TXGL5tK2Lp6JoFaDd41b1v3jSSzMs+yXfEVU056++EXGN4BpppR4uSsrPVe5UiN/dARVS8o3p9fNTLFcPwkRqUjSju4OOVqaGIumetmEqjxw9svkMkJueh12aydMwGNnx9+gTNJqm96Tgc1Ln1qfuCI0gGMpopjq6xUj0KtxqkqXY2nlwu6/blU3KdtICcxkuAAP4b4atBOiZapQK9hCqWy1nujCXBwrByxbG1/F7YoTgglaF0Rxh8TCfb2JjC+BMx6clJjiJigxbty2rPF6SUN3CHS8HFrKskg9tXKqUQDQpmdIFMvXqY0lzy9C54Dq8/RqnvdUOvyydODrfu7af8GVu1UExLqhbK9jDJuSlbjKJpKWVYcEc9W1MH8np3J8vSiijLCnM1ijR8R6ZVlgDmbxQGBxGZVT5HjrQ0jdpce/f5k5odq8fP1Qzsljt01fqOy5DC8p8SzKSGK0EAtfn4atOFxZFqZkrNW3e7ZmcRm2Xq3Z2UdKCmXzNXhaP38CE4ssV7fLIhvRNSuzsGPN7D33gn8dUBl/dmUy9c5JoY87MdNda7hSK9eUHLgIBfnsyn/VY+pqt4nKkh9okWYcsncY2TYaL8a7QDR9EwYyg2oVL3qTrbhPNlg/dtcRF6+EVePAVV3EHKHB/cqi8irUTZ2+PqItf3ZWpytpdsY5w6vsfU6a3Fr7vR2rOF2dyOd0qE/o8Kpb/XaioFe3E8uOd/TcnE+lcWq+EN4hgbRv43UexoVZ2txBFAqa61rMpmgiyP2Nf7fhspjQ7kBejRQL7Qh/7aoReN81edFA7uy8lEO92d4jyv8h5tBk3f8ZWRkkJGR0WT5DRw4kE6dOrF9+3aOHDlS73Jms5nvvvuO3bt3061bN2688cZGfY/h+JVNKQpg+MV6Q7TfnXfi6+PNnHnziVr8Jod++MGmvLO//S8HDuTx4Qfree8f7/D1f3bxzITnufPOO9jy7008E/g0S5etwGCovjBnNBpJ/2Izby9bypdffM7IEQ8xZVoEv/9ef+fp628s4syZM3ywPp5318bx668nWLV6TZ3L6nQ/U1p6DLcB99T6fE3cWvrefDPr1qxm8CBP/h63ltOnTwNw7tw5Vq3+O6797mTm9Fd48onH2bt3H0WXdIJdiR9+OExR0Q+ET/kbUQvmo9P9TPrmLRXfazQy/40oAh7RkP7ZJ0ybMrmq8ffmoig87/dAM2Y0W7/4nHvd3Tl9+jSTp4bTpUsX1v9jLW+8HknW118TveLtWt+Zn3+QrRnbSEx4jxXRb1Z9vv79RJzUajb/+zPCXp7IP+LfY8as13huwni+/OJzbunbl+VvVef1XcH3LIhazP+9+AKf/etjQp4LZvbc+RyrMcVtXd/11JNaHnv0kVoZcgWvAAAgAElEQVTb1L+fK98VFFx1PK+E8pL3hu1xrDrsxtTo91gT4YEhfSVr/1P3id3w/U52GwcQEhlH8kcrCXHJJ3ZJIoWVBWhh4jwWf6Ni0uo0dmamsFTrjHJgMDFxYXjagWFPDJPnpWP0DefdDe8RE+xM4ZoIZqd2oOnAzEXs3m/E5S7XyhOQitETQ3HZE0Xoq3FsykpjeUIRw57V4FJjtbzEGDL7hBK/aRvJEa4UJkQSPCcd5bNxpG9NYcbdh1i3Mo1iW7ZBl0uOTsW9d1XMoaXs4Yjp1MlaF/0Vdgo4fhK9GUzZcbyeqMd3SQrbNsWz0F8N2DdJOEQ7ZjZQuCWGtdm9ePQpr8vKjoplLt3fhS1cxsewJsQVe+cg1mxKJz7YGfS5ZO63Z/ikKBI/2sDbWkcyV0SzsZ7iscHjVp/FovB4jgycTHxqOmkrAlFmRLJoUwcqa5uCXo8eNb1rzo6oUtMbHWV1TfFR1/5uLiJpdTr2z0Ywrm/zb3Kb1Ng4iitzKoPYNzPgqRjSt6aRGDGU3nb29Ze9Zh2pqz9BoY0mfWsKSwfpSXo9mLDEckYvSWFb2jKG6ZNZ/lHtgY7G/WlsOuvNwg1pbPs8jpAuGbwemUhxPReRDXtiiIjOx+X5aNI2pRA/0YWcZZGNer5OcUo0qXZBrPn8C+LHO1utb7aYszv57EsTIx/zxvHiZz+X8JPJmX53OdazkoqRoS/inLOIKbPeYfPXnxL7wSGGBI3l1hba7HZJ6hPNQ1dCsdGF/nfXWYsTTcaI4Qzos5ZVDOIMCCR0QTI5pyqTG3OerKv+bdZTprent6rG72inwkkFJ45XtvCuhfqItf3ZWpytpdsS52tBo+vHVuLW3OkdgS3tbmscHFHY6TlxqsZnZntQGDlx3NBCcTaRkxDH7ttDmeqvhrY2AMGWOFuLYy0m9PvTWJ50CJdxWjwvdsxZKY8NhnKMpenMHqfBe7SWwCnRpBbUNfC+nvzbupaI89WeF80lFJaCi1u/NlXvbPKOv841nj3WFA4fPsyFCxe47777KCgoYPPmzRw8eJDS0lKOHz/O0aNH2bt3L+np6Zw4cYKRI0dy7tw5ChrZAXLhwoUr3sYLF2wreZYsWsiMVyPYd+AAf33uBaaEv8LRow0Pd+3ZsyezZryKg4MD/e68k7v692fokME8MnYMnTp14pGxYzh37hw/Himutd7MV1+hd+/e3HDDDbzw/HN07tyZjK++qvM79u7bT15+Pq9HzqXvzTdzs5MTfwubxJfb6l5+7bvx3O9xHwPuqd3xF/JcMM8EPs0dd9xO8ITxGI3nKD12DICtGdu49ZZbGDnCl25KJX1vvhnP++/n+6Iim2LXkK5duzLuicdxcOhMt25K3N3dOFJcEQ+j0ci5c+e49ZZb6NatG0O9hqAZMxoAR8euXH/99SjsFfTo3p3rr7+ej5I/5rpOnXjj9UjuvPMOhgwexLzXZvPv9C84/GN1x/PpM2d4fd5cVKpete7afMj7QR5/LKDit9GMBeCZoEDcBgzghhtuYPTD/nxfWFR1p+Pf497hheeD8fF+kJ49e+Lr443XkMHsyPq6we9SKpU4OnatFQdVbxUn28hzF+0HhxEzS4unqzPumkBGORsp/rHui8PK4WEsnR6E70BnnPq6MS7QHyddbuUcyAYKi3Sohmrw7asAhRpfzVCURSUY71CjRM/WxHQMPuEsDPahv7Mz7poIFo5Xk5OYSE77HqxVpSw9nk2/D2W8xrnqM4XKFffb1SiOZ7BiTgw7emgY/0Dt5/AoHwhlntYVpR04jfDG086Ia2Ak492UYKfC19cDDh+i0GqcDGQmpFB0t5ZxbhWf3OY1FNX+T1ifpcNkMlC8JYb5qUUYK2eKN54qx2CvwqmPCqVSjXtAMOMuvTNUXFPK0mbi56sheEk2ThMXMW1Q3c2PuvZ3YQOFEqUCsFOg7FH5dx9/ZkSFMW64Gy5qZzyDghhmX0TO/rrvvm3ouC1O28But1AWBnvholKicvVnalA//rc9W+76q8lU8byMWpNOKECBqWKE4iXq2t/L0uJIMmqZEXQNHwONjKO4Qmf1nDAq6K1Wo3RQ4jRIS8johvY7BfcHRzLeQwV2Kob5e9Db5EzA9FCG9VFADw9GD1KjLzpUq1ywV2uYNtEHJwXg4ExAqBaX4gy21NkM0LM1cRu9gyOZ5OOKqocKF59QQgbpyMyq75kelzP00TBjohdODgoUdtbqmy3HlLOTb+28ePDeGju3oRwDShQlKSyZ9CRjRj7MUy/MJymnOooKVT/uuV2N/a/biI1cydc9xvLUsLb1/MW2RuoTzcRQjgFHFMUVd/pevIM2YY/UBpqWEk9NKCETI4nftI1tq8PoXxLP7Kj0itlVbDxP1lv/Npowoqi9fuX7iwOlr4n6iLX92VqcraXbEOdrQmPrddbi1tzp7Zyt7W6rlB74DjSxNTGZvFMmTLpckqLWsNsAJlomzqaCRGK3qgiZqmlzd5nbHGdrcbyoKJFgPz/Ghq2h0C2cpcHVA5eslcdOXlomBU9m6YZ0diZHM16VS+yslWTW7ARrIP+2rMXifNXnxXIM5fYozfkkzKmcJUkbyvzk/Fadke76ps7woYceatL8fvvtN4YNG0bPnj259dZb2bdvHyUlJTg4OHD+/HkcHBz45ZdfeOCBB7jpporJUYYOHUphYWGjvqdrzytvOHXt2dum5Tp16sQTjz/G4wGPsvPr/7B6zTu89PJkNiYnoXSse4SnnV3t7ndHx67YXW9X6z1AeXk59enUqRNOTk6Ulh6rM/1AXj5/GAyMfiSg1ud/nj/P6dOn6dq1uoPpq+2ZbM/cQcK76y7Lx1FZ/T/06N4dgDNnzgIVd+UVFxfz2txI7O3t6dzZHoXCHie1ut7ttlWnS2LUpYsDZyu/t3u3bkx86UVmvjYXryGDeTzgUYZ6Dak3r/yDBxno7obihhuqPrvv/w3Ezs6O/IMHueP22wDo1asXvXr1vGz9mr+X4oYbUNxwA9fX+Kxr166cP3+ec0YjDp07k5eXz3ffFRC/PqFqGfOfZtQ3VU/0U993XapLF4eqOyxbm8JBWT0Cw06JsgsYz9b/6GBDQQapW3aSc6CYMsNJ9OZeGAyAWonnUDdiN6aTGeiGr0rP7i3foHfx5zYFYM7nQAG4T/WoNeLDZZAHTom55OnAs723U0rSWLzuIP1D4hl1sZgyF5EwK4pCzUpStM7o89NYFb2GV+apeHdZjQpRzVH7CiUKO6DG4aLookCJAaMRGjrj6zOWsSJLxbOrn6jKWzEwlKURehavnMCDC5T0HxHMpEAPZqcrUdqBckQoU3fNZHHwBLb4aQgI0ODr2rZGBpuyotC+mV3ZValg2GspLPRpD1Wf9slJE0mih47i/G0kxYcRdiqauBC32rteXfu7uCqm4/lsSU9n955DHNHr0Z8x0f9s3Q2t+o9bAwcKijDsiUbjF117pTu8MADyc1VSKLAHTCbg4sO+TSZMKLC/9OHfde3vx9OJTdAxasEy3BW0vVGtLaUxcRRXTq1hWkg2s+cFETxYQ4BGQ8Bw54YvAtSsdjsoUdgpak0TpOiigLMXhwHVo68aJ3QU60xw9yXfZj7EgQIDOfmBeK+t+TmonrC92ax0dsXlklHM9dc3bc72qh3M2QsDXmZAzW2zA8z5fLr5PqbO+YDpNxk5mDSfBQtWcVPSQkY6HiJpzmJ+GPM26x9z5uTBT1n7VhxzFqhYvWRsPdODXuOkPtF8KvfX1HQPZkQmMk9t4kBiJLMjV+KUHMWoNjS9VXvnNCKI8RffOPswbWI2O17LYPdxDeNsPE/WW/+2V2CPidrX3iveKxwU1059xNr+bDXOVtKtxfla0dh6nbW4NXd6O2dTu9smKgJmRVIWvYYIbTz0cSNgYjCjDkej///s3X9clfX9//GHUoc0T6lHWYdZ0A8hC/ITYZI2DIU0KRqNBsvJZrHpBz85qJSyMBdqojZMJ9PFoiAaFN9oNEpHy7DpbDrKwAxcCUkcZx6yDmkcQ79/8ENA4ByUH4rP++12bsp5X9f7ep/X9et9Xe/39b6GGHs+zvWVvLo2DyJSmHEW3tdzPs4O4tjk6ihSMidR/VkxGzNS+dWCWtatCMfT6vh4bPAKZZZX4x+DvAiLi2FzRDKbdiQQFGLoPP+z/Km/XotzN5wXXV1svPvaFvzjV/BmkolD29fx6BOJrPHM4fGAvjm2dHvDX3fz9/dv/v+AAQOwWq34+vpy+eWXN3+/detWampqmhv+hg0bRkBAQJeW8wNvXz7b/u5plfEH3j5dmn7gwIHcNimQMWOu5e577mX7+/8iZMrk01p2E0fvyuusYfDEieO4jRzJX157tdM8ysr3krT0aR6O/w3XXHN1l8p14sQJ/P1v4hczf+7UfGeqZTyifnovQbdN4o2/FvDkU0ncPG4cS377ZIfzDRgwoNV3AwYMYMCAAWf8PsI2C2r4B3jkobjmpwPPxNEjRxk5wrlG6L7QUX+eitw45mbYmTo7hsdj/HD/MoPoB04OF+weHEVQdgrrH45k1VED5jGTWJ4U3WpIS87yE9VpsxWz5ol1VPglkBHR4q5UcR651kB+G9ZQ+zH5hPPU03XEzMwgd08o88Z0XxHsezJIeKYYzzmpzGl1c86Ad1giGWGJzd+UpWZj8GxseDR4EJGUQ9CeIvLzclg1O5Pc2Smsi/Jqu4g+Y7g1joysk1umYei5X8k/qxka3r3j7uGF/xAr05My2Bi+grCmG0Qdbe9y2uylacQmFGIMj+H+xHh83UpZGbGgw+H1Ot5vzYABY+gS3k7oWv3qvONmxsTuhmGLmi5QrVYOYSao5XtTO9jeqwvz2HzYgutj03mj8bu6o3ZInk7wO/G8vSK0l35IH3M2jnKGjPhGryZvWimb8/PIWvYA6X7xPJfUw72qj9gbOgy4tHfebeiNFJTwJsundd952VF9s3dU8nGZDfOENkMAmcyYDaMZP28m4y8DGMKNMyK58ZUVfPAJTHHJ43VrIAvvbNgZhl//YxYm1TFvViavfzKdOdf28s8426k+0bNMZtwNXkyIj2ZCY3j9oyPxz17Bzk9gqqoJPcbgZsZEKbZanD9PdlT/Nppwd6tj30EbzS/uqLdSbYURbqbzpz7iaHse5SDORx2ku3Qe5/NGV+t1juLW0+nnOkfX3V3hFsCcZwKY0/S3rYD4ZZ7c4Gno+TjvKSC31Ia1PJbA7Mbv7HbqeILArZPYUpDY6ew9ritx7iyOTVwMmMwemMwe+F5roCIilaztocyq6Px4nLMi9NROuEYT7kaoqG3RRbeD/B+feJbfB+ulOD8ecGbnxZynR2MabuCmGXFE+DXM7z4xmrCxBby6qwIC+uZeaLcP9dmTjh8/3vz0Wkve3t4cPXr0jPK+wm8CrsZLujzfoEuHM2psx0+QNTlw4ACVn3/e6rtLLrmEAQMG4Gro2Z3swIED7K+qwsfn+nbTx1x7LQf++1/K93b8vj2L5QAPPbKA0Ol38OOwu7pcBk9PD/Z8UtZnj83/wM2NmPtnsfqZVRS+/Xf+e7D9cX2uGzOGj0pKOXbsWPN3H+7axffff891Y7r/qvpaby+K3vtHt+RV89VXDBt6jnWxrC8n/9VSPKOTmBfqh3s7D4RV5KWxc1wCOZk5FORmkpYU01wpx8ULby8oLy5u9eh0RXEx1cbReJ/L1/r2crIWJpLrGsXyhSGtT+R2qKu3t+5hcqmJEdipO7NDYWtVBSxamIEtKJHlEQ66WNlL2VRUi3+gX6ubWaYxk5i1MJXnZntQkltAydnUW9TFiMlkav50+CJkOUM2bG22S8NgA671LXpTdba9y2nbmZ9HuV8MyTEh+Jqdr2ucut8a8fYyY9vxz34zhHKPMfvgb6rgo10nhzyrLi7GMsoH36ZTdCfbu3tYMnm5OWRnZjZ8XkgkzM2A/+w/kZEQ0qs/pU85E0fpNgY3H6bGJJKxKooh/8hjk/Mjap4WW/F2ymiow52isW63c+s/u29YHCfqm72i3kJlFQy/rM1ZbuSN3Diqkt27W3TUtNsB14bhhuxgr687pd43HDv27qz39QeqT/Q8Nz/8L6+kpLTFHlpnh1OGx5IzUm/F2uYgaN1TSoWLJ+4jceI86aD+7eKFr48rJS2vo8uL+eiID/4+hvOnPuJoe3YUZ0fpDuJ83uhqvc5R3Ho6/ZzlxHX3GbJuLqLEHMAED3o+zmNmkpbT4jiUmckjtxowToonOy2ue37QaTnzOLeKI2CztZlxsAGjS8O7Xh0dj03YsVrbzG8pp+ywEXez0WH+Z6/ejfOZnhdNLp74jzVRUbq3xYMvDcMZuxpcTycA3eKcavgbOHAgkydPPmX4yxEjRnDTTTedUd4XuF7EzffNcTxhSwMGMP7nsbi0GBayIy/nvMIvZsXwYmYWuz/+mJ3/LuaxxxNxN5sZ539mZW/P71P/wO6P97D74495fNFvueqqK5l82yQADBc2HGCbGr/G3zyO8TePY2Hik/xrx06+/PJLira8x9vvbAYahlt9MP4hrrjiCmbcF0XVF19QVfVF8/v7nDF92lSOH68n/YVMLBYLhw9/zb+LP+DQoUMAvP+vHUyacjv/Lv6gYZmHDzN1+l08/0JGu387a88nZSQtfZry8r0cOXKED3ftYvDgwVx6SfuNvDN+Fon92DF+u2QZn362jx07/83Sp1cw7fYQvEaP7tKynfHg3Fj+sXUbqes3UPXFF+yrqGD9H9NOq4G0rHwv13p7d3sZe5SLK8bBULFrO2UHrVQUF7AmJY+yFjc26ux12D4rZWeVFavVivWwDXtzupmwGSEY3lnN0qztlFVVUrIxhUUvW/C9L4oJ52qdrd5CflICa6r8mPdwKCarheqqho/NDviFMHXQFtasK6LCBtgtbE7LYZtbIEHd9bSfdTtLF6Swc1Q082d4YGtcfrXF2nASqypgZXI2m/dUUl2+naykJPKHRjJrckPFoiI7iUc3FFJmtWO3VVLyibWhp58a1847to0riPplIunvlFJhsVBRXMDS1ELqxk5ighuOt3c5bcZBQ6j7tJitlVaqy7eTm5zKppYXBa4GXLFhbbyz1Nl+6x0eTZC9gKWLs9lZacVmrWRbbgb5n/XRj+tDdlvD+ch2xA71dmxWK1arreHY6OJDWIQX/05fTe4eK9Y9eazJqeSme0PxdsHx9m404W42t/g0dEowDDXjbjpXT2qnwVEcpVvYd6Xx6OIMtlXasNttlO0ux+pqxv3S7l1OnaWQ59MLKamspGxrBovWFuEaEk1YYwctV1dDw/sGbQBmwqJDMBat5tG0IioO2rBWlZKfnkfJ6TZyOVHf7BVf11BjNzD80ravePDgjig/dj+/mrc+sVL7dSV/X5/J+27BTLkeuDGYKYO2sGH9Fj6vBewW3ns+h/dHBvIjPe13kuoTvcSDsPv8KElbTf4eK7bDlWxKzWCbWwjTujYQknTCujGZ6NmJpBeVU22xUPZOGos2FGO+O5IgIw7Pkw7r3xiYGB6O6R9prCmsxGopJj01j9pJ4Ux14zyqjzjYnh3VRxzWVxzE+XzhME6V5D4YSnBcXsM7LB3GrafTAbut8f5TLXXUYTvcWN8/mzoyt+F4v3dwHXPKerBRkp7MmoJiKqoqKdmYSsKGvUyYFenk9n2GcXYxYmp1HDJjNLjCIFNzg1ZfcBznLsbRkk38jFgWZW+nrMpC9WfbyV2WxrbBtzB1nMHx8bjyNRKiY1ma2zB/xa5CVi7OpNwrnBnjDI7zP0v1epzP+LxowD8qCvctqTxbWInNZqUsP5Xcci+CJvXdWLVn/VCfvemKG2/BN/SnlBS84tT0Y8Pu44e+zjXaxT34f1zp4cnr+W/wp/QXuPjiwfj9z/+wbk0KgwZ1/8tKfH18WLo8maqqL7jZ35+Fjy5g4MCGdt5Ro37I9deNYcnTy8nNfhmA5KeXsn7Dcyxa/BT2Y8fwGn0N0TMbRpR/7k/p7N9fxf79VYRHRDYvY+DAgfzzvXedKs/gwYNJeOQRXs/P5w8bnsN4iZFrrrqaKz09ARgw4NT3Gbpc4ELLUTfb/u0Mjysux23kSB5OeIzDhw9z5ZWerHh6KRdddFG70xuNRtI2pPLM755lVsyvGTxoEGF3hRLzwP1dW7CTxt7gy7o1q1m7LpU/Z7/CyJEjmTjhFursdgxd6DJZ9cUXHLPbuarxHYTnDg8i4mIoW5XKryJTMfkEM2t2LFMTTjbweoeE4pmdxkP3pVHXVMkymPAOjSPlkUmYJiawbnEaa15I5lcbbBjcvAiKSWaeoyfUzmL2d1JZ9Y4VKGJlTFGLFBMRqXnMH+vHb55ZwPqUNOaGJ2HDiHlsCL99Jgb/bjqc7ExPJr/SDpVpzI1MO5kwKITlmxIJunQ0V9avZk18GtXfG/G+NZKU5KjmG7LuNwfgmZ7DQzOSOFRnxH1sMI8n3HPWvZBZep5x2gKWH01jfXoi6ZVWGGzGK3AuKbHhuOPM9t5HBe8HfKPjmVW1jqW/jASzH2Gz4phzOI7NTROM8mPC1Tm8nJzD1MwYruxsv3UL4am1sH5dBk8+kMohTHjfHMIMv4Ze0ecPKxsTw1m6o+nvUh4NLwBDAE/lr2CqETzvTWJ5TRIr4yNZg5mbIpJ4orGFQ9u78zqLo3QPg0cgEy5N49kHM6g4DCM8/IhIjGt4R1c33thydfPjBrawMi6ZiqMmfIMTWTcvoPn9zCa/Sfimr2NpaghBCwMwBiSw7uk01qSvJjrDCkYPfIPuYd73p1sCx/XNXlFrpRYj7b3affiUBSyrTWXt4p+x5msj5huCeWTZA1xvAPBjzooFPL/mTzx87xJqaUhfuOIBbtQ7L5vp+Np7TCEL+F1tKisTI1l1uOE65PEVMQ3vvJFuYQpNZPnRVNZviCe9qg6Dmyf+YUt4vMV7jTo7TzqqfwMYfKJJecTGog0PEG414jlpJsvjJtF3t9P7hqPt2VF9xFG64tyg8zgZwIXW7wx2ELeeTq/Iiicyrby5PBUPhpOFkbBnCnj8LB3S2PF+7+g6pu16MGLyMrFvfSLRyXUYRvkRFreaOZNPbr2Kc3tx7mIczVEkJ8L69BTmbrBgw4jn2GAeWTWXIGdGOvGIInkxPJuewtx1FuyDzXgFzGTdg1EN7+870/z7SF/E+YyP16PCWb7YxtK1cYQus2P09OOuJUnMuqobA9NFA05064vL+ofKnf/g/aw/cOxo+8+8GgYPIWDmXC6/8ew7CpXv3cvMXz7AX/7fK83vPJT+78XMl7DW1PDQb+b1+rJLSkrw9fXtodwrSZ+9gH2RqTw1uWmwHjvVhcn8apmVWTmriTifesqJiIiInEOqs2OJyvNh3cux+PajpzVrjx3v6yKcF4ZceE4NUCQiIiIi0qO++eYbAD788EOH0+qJv3Z4+N+K+br/4bPt73Jgz4fUWhuGxBxi+gHm6/6HqwKCuHDQ4D4uZefUnHv+sB87xut/eYNnU1b1dVG6X72F6qpabBYrtvrGd7AdrqBkRzl1V4fgqxd3iIiIiIiIiIiIiIg0U8NfBwyDh3Dt5Du5dvKdfV0UkU5Vf1HNnaHTueLyy/u6KN3PJYA5iyN5Ni2R8MxaMBhwHWTCKyCKlFV6z4+IiIiIiIiIiIiISEsa6lNEzkjPDvUpIiIiInJ20VCfvUNDfYqIiIiInNSVoT5VkxYRERERERERERERERHpB9TwJyIiIiIiIiIiIiIiItIPqOFPREREREREREREREREpB9Qw5+IiIiIiIiIiIiIiIhIP6CGPxEREREREREREREREZF+QA1/IiIiIiIiIiIiIiIiIv2AGv5ERERERERERERERERE+gE1/ImIiIiIiIiIiIiIiIj0A2r4ExEREREREREREREREekH1PAnIiIiIiIiIiIiIiIi0g+o4U9ERERERERERERERESkH1DDn4iIiIiIiIiIiIiIiEg/oIY/ERERERERERERERERkX5ADX8iIiIiIiIiIiIiIiIi/YAa/kRERERERERERERERET6ATX8iYiIiIiIiIiIiIiIiPQDavgTERERERERERERERER6QfU8CciIiIiIiIiIiIiIiLSD6jhT0RERERERERERERERKQfUMOfiIiIiIiIiIiIiIiISD+ghj8RERERERERERERERGRfkANfyIiIiIiIiIiIiIiIiL9gBr+RERERERERERERERERPoBNfyJiIiIiIiIiIiIiIiI9ANq+BMRERERERERERERERHpB9TwJyIiIiIiIiIiIiIiItIPXNDXBTibVX/7DRW2r/ja/h0AQ10HcaVxGJcNNvZxyURERERERERERERERERaU8NfOw599y0FlWVYjnxzSloR8MOLLyXUw5vhroN7v3AiIiIiIiIiIiIiIiIi7dBQn22UHz7Ei2XF7Tb6Nfni26954ZN/8+k31l4smZyv9u+vIueV3L4uhoiIiIiIiIiIiIiInOXU8NfCwaO15Fd8zLHj9Q6ntR+v5/V9H3Pou297tEx1dXUkLHyC26bczt0/+Snff/89U6bewV/ffKtbl/NMyrOMnxjIv3bs7NZ8O1K6ezcZL2Xx+9T1pL+Y0SvLBMh6OZup0+8643wqP/+cWTG/5kdBwTyxaDHle/cyfmIgFsuBbiglPDQ/gceeWATAyJEjyPpzNgcOtM7722+/Jf+vBdz/q9nccdfd3bLc02XrNNVKbmww4SmlvVSa/sbG5sXhjA+MJcvSJqU0j6WxkQQHBRMcEcvSvHIH66Jr7JZSNmWlMDcimMDEolMnOFxM+sKZDcuPjGPNO5ZTpxEBrMXZLIqNJDgokMCwmcRv2E51fZv02Y3p4TEsyi7t1m35vHG0lKwFMxviGJnCNruD6fekERkUS1ZVr5Su/zhqYWdBBktnhxMYnsLOttXGeivbUuMInxZMYOhM4tOKaRqF1+AAACAASURBVNlVrCvbe3VeHIETg4nf6Ghl9kMO4ih9q/qdFGLCgxkfFMrcXAvV2bEERqZS4vgy6vyzbQl333YbU0753MmqFpdetbtfZ9W8n3H37bdzd9RcVv1lL7VNifZK/p7yEDPuvJ077vgJc5Jy2F3b3sKkp+vH0kBx7nkVGTGMnxjY6hOYWEhzjcCZ+kYn9W+wU7ExhZiIYAKDw4lOzKakg5XYr+sjB7ezfsHJa9qVBZXYu5LusL7ifJz7tS7X6xzFrafTbVRsLWD94hiCg2JI/+wMf38vcbTfOzyutFWaSnib6ceHtbj+cZQOcLCY9MQYQoMb1v3clEIqmhZYb2FTSovtIrWo9XHqaCWbUhcQGdZQ5wyPTSZ3T9/vQI6Pr23ZKMtNIiY8lMDG35H/WYuoO4qDo+3RUsT6hY0xnhZJTHLByRg7lf/ZqVfjbCsgPjDwlP1j/MRAIjeUn8yjs+3Z0fL7wFnd8Ldnzx6qq6tPa94PP/yQmpoap6c/wQnyKz7m+xPHnZ7n2PF63qjYczrFc1rh39/h/ff/xYY/rOMPv3+WCy7o/tFZT5w4QdGW9xhz7bVsfvfUm/vr/rCBpU8nO/zOWRs3/Y3X8//KVVdeScQ9P2ba7befVj6n46JBF3HRRRedcT4vZrzEiRPwyssvETfv/7qhZK0NuuhkOS+66CLuujOU/5f3l1bTVFss/K3wbb79tmcbn52xsqjvT7z9lW17Kmu22znlzaKfZRP/SBr7RseQ8sKfSIkZzb53HJ0Eu+ZQcQH52ys5dLS9E5WV/GWJ5NaFsvyFTNJmmdm6LJH08nYmlfObrYhnk/KwBcST9nIeGQmTsOU9waNZlQ3p5WnELyyAaYlk5OTw3OzRlG1I1HHlNFgLM9jwkYlZqTlkr47G39DXJeqn9m8nt7CYfTY7de0kV2Qn8miRiVmrMslOvgfXgkQW5TV2jOjK9m4tZE36XgyDevTXnLU6jaP0rfpSslLzsE1MJOflNJ6aau7rEp3d/OPYkPVnspo/L7IsajRDro3k7hsbp9mXw8KEND6/5gGWpT3Hsvu9+Pzd7Vga63W7n1/Emv9czyMZf+OtV5dzx9FXWLxue8c36s5XvVA/FhTnXlJXa8N1XCw5OTnkNX6y4ybRVL3r9DzpqP4N2HelkbCqGO/ZqWS/kEiQLZNHVxSd2oDbn+sj9ZWkJyaxaXAU63JySIv1omxtHEua6mWO0nFcX3E6zv1cV+t1juLW0+nUV7L5tUJKLDbs58qxzYn93tFx5RS1NmxDJ/H4yyenz9sQww0uTqbbS1nz8AJybQE8sjaTjGVRmD4pZOdBADslqQtY+ZEH81JzyEuJZsQ/k4jfUNpcv6l4NYmVxWZmJGXy5svJzHIrZmXCarb15Q7kRJxPmWVrKg9l2Ahals2Wt7N5alwla55Io6wenIlDp9vj0WJWPriCbSMjWZ6ZQ3ZyFCN2pBC/rml+J/I/G/V2nI0hPNFyO87JIWdxKO5D/bg31KthAZ1uz46W3zfO6oa/Sy+9lJ07d3a5YaOyspL9+/czZMgQp+f59OsaDn13pKtF5L9Ha6mwfdXl+Y4fd66BsaamBjc3N7y9RuNu7pmL6o/37OEbm40ZP4uk6L33TinbV4cPnzJPe985Y9++Ct4t2sI9d4dx68QJXHbZZZjNl51WXqdj2NChDBs2rMN0p9fLV19x7bVemM2XMWLEiO4qXrOhw4YybNjQ5r9Dpkzmzbc2cuLEiebvRl9zDb9/NoXIn97b7cvvqm2bi8/uE8a5yl5K+rotuIcF4+nSMsHG5vRMyq+N4an4EHw9PPCdFk/a2mi8XTrKrOvcQxNYt3Y1c/zaqQZ+VkDuDg/ui4/C38OM57RY7r/FQm6ebkJJG8ZJPJGRRkp0AJ5mE54To7k/0JWKHY29PL0iWb4hlafCfXB3M+M9LZb7b7XruHIa6g7XwEgf/L3MuJtNHV/AyZnxCmf56tUk3zsa17Zp9aW8mlfOTdGxhPmYcfcJZ/6M0ZS8WtBQ4Xd6e7exbUMaJWPDCTL11g87iziKo/SxWmy1Bq4cewueZjOmU3onSSuGIVz2Q/PJz6C9vPU3O3c8GMloF4Ba3nsxk73XxrBwXjDXX+HB9bf/hjUpMxvTK/nw3xZG3/5jbhwODBnN3fcGwkcfsFf7Qwu9Uz8Wxbm32Gx2DG6eeI4y4970MTXW7hydJx3Vv7GzNa+A2klz+U2IF+6j/JgVG86Qf+SQ36otpp/XR/YUkP/p9cyKC8XbzYTnpFjmRwzh3ZwCqp1Jd1hfcTbO/VyX63WO4tbT6YCLD7OeWc26hNA292LOYg73ewfHlXbYbbXYL/XAu+X0ZmPzdaajdGtBGrk1k3hkcQxBY8x4jg3lqQ0riBgF2It5dWMtQTFzCbrKhMkrhPnzQqjNz2Fz4y1nzxkp5KXGEzbWjMnsQ1hMOL62Yrbu7aEYOsOJOLdV8a9ibGNDuXeMEVyM+M6IxL+mmG1VOBWHTrfHQX7c/0wa6+JD8DWbcB8bzvwZPli3FjbsX87kfzbq9TgbMLXcjkfBzvwtmGbGN2yvONieHS2/j3R7w199fT319d1zBeLu7o6Hhwfvvvsu33zT8Tv3WrJYLHzwwQcEBARgMDh/y+uzb5x/OvB05i3fu5fJt9/BJ2VlzPjFLCaHTKO+vp4TJ06QmfUyd4X/hMm330Hcw/OpqvoCgOdfyOAPG56j8vPPGT8xkOf+lN5u3p3lkbr+j/wk8mfYjx1rKOu+fUycNJmyspOP5Wx+dws3jh3LuHH+1NR8RUnpySEZf/LTn/HGXwvI/2sB4ycG8tD8hHa/A8jOeZWHFzzK3ze/yy8f+BXh90bx1NJl2Gwnx6HZtn07ZrOZ664b08Uot5aZ9TKp6/9Izqu51NQ0NLzur6rif/9vHrNifs3Pfv4LwiMiueOuuwkKmcaixU8BYDINx2Qa3pxPds6rPJLwGG++tZHQsHDiHp4PNAyjuWz5CqZMvYPpYT9m+cpVHDnS0DD8YNxD/HP7++S9ns/4iYEUf/BBu2XsLA9H68U03IRp+MmataenByc4QWXl52cUt55it7e8XWllZ3oi0WENQybEJBewr7NDgrWU/LWJREeEEhgYTOjMRLJKW+Znpyw/mbn3hRPY6rHrcFYWN05RWciahxuHtwudSfzawtaPtZ+jKrJTyb8gnHnTTK1vCNuL2byjjgnTgnFvd047mxYGE5NRzOYNC4gMDSYwOJK56cVYLdtZv3AmocHBBEcuIKv09LpK2UpLqXDz4Ybm/ghG/Md6YCstbVzfNnZmNGwH44NCCX8wmdzTXJac+wzG1neF6+zAoCGNFwRG3D1aphsxDWl7XBFHKtJjiNpQTt1nGUQHBhKZVgn1VnbmphA/M5zAxmEll7YdnqiVzvfbVsfasBgeTdfQi6eoKqbE6on/2JPncNMNPpgtpZRYwdnt3b4rk2e3mJkVE4DxfLyx7zCO0j0a6wtZxWxeG0d4cDDRGQ29aDvc3+1FLApdwCabnc2LgxkflMjmDrbRDvOot5AVG0xo0sme9dV5cQRHJPdtD+5esvuVTD64YSY/v77xWtVezHs77Yy/PZj2u0MOYfhwqPxoN01XnbVfWrEbh2A8V25G9gaH9WPpFopzL7Fjq7W1unfRihPnyU7r3/XllJTW4eV3/cmOYlf7cYOxnJIW1+L9vT5it1iwDvXkyhah8h7nh6m8mBKb43SH68HJOPd7Xa3XOYpbT6efwzq/7nZwXGmHrdYGQzuub3SebmNrUSnGiSFMHNpO8mEL1iMm3EedvHdvGBvATRSz85PGL1yMGFve2q+3Y2cIrn38BHLncT6V0WSE8mI+aqrnWi0cYjimITgXBwdMHuZWo4QZhxjB3jg6TTfk31f6Ms72rRmkW4KZc49H4zcOtmdHy+8j3d7wV1hYSGFhYbflN3bsWAYOHMg777zDvn37Opyuvr6ejz/+mG3btnHJJZcwcuTILi2npq7rT/s1+crJeb/99luWr3iGRY8vZNObb+Di4sKfc15h09/eZvnSJbzy55e4bswYHk54lPr6eu6L+imzfjGTUaN+yKY332DmjJ+1m29neTww6xccrz/On7NzAEj9wx+5687peHt7Nc+/uaiIceNuYuill+LlNZp3Np8c7jM7K4Obx/lz5/Q7eG/z26x4emm73zXZ/v6/eP9fO1i35lme/+MfqNr/BQVvbWxO//zz/XhccXmX4tueqSEh/GLmDNzN7rycncPx48e5fNQo/vD7NaSn/ZE/v/Qiebk5vPXGX9hcuJGnFje8L2/MmDE8ljC/VV6lpbvZVPg2Gel/YlXy0wA8+dslHDlyhBefT+O59al8+eUhnl27DoCnlyThf5MfoXdMY9Obb3CDr2+7ZewsD0fr5d6fhHP3XXe2ys97tBcf7+nZoWVPl+e1Xs0VibK0BB7KsuA7ewXP/TGZGaMq2flpJ7eZP9nCtrrrmZWYSvbLq5nlWcqaZRnNPb/sO9bx0Jq9eM9L4+3NBeQsnIS7KYD5a5O5fwxgLWTRg8lsNd7DUxsyeW7xPRj/mczcpMJz+4a0JY+VOTbC4trpPWuppKLOE+8xnXexL8lIYbNbDGn5b5Md70VZeiLRCwsw3pdKwaYc5o/Zy4bVeVScRvGqD1rANBxTi7IZR5rhSytWwL49lSczrAQty+Ht/DSeCjHDqc/GyPmm3kbZxhTWbx/OXfcGnDqELUB9Odt21bU4rogzPGeksG6WF64eUazLLyAt2gOsxWze5crEOUlkvJzJ78KHsHlVMq920MO40/3WWsSSuDT2jZ1LWm4BeasiMRYmsuS86q7sBKsVK2ZGtOwVbzIzAgvVB9uZvr3tvb6crLUFuN53snfheaercZQzUpGTTK5LFOveeJO0GR6d7++GSTyRlUjQIAMTEvJ4Oy+RoPZu9nSWh4uZGXHRmIrSSN9lh6PbSc+qxD8mhgn9/enBo1v4y9/sTLk7kOZ7AAcq+dzuwehrO7orYGJKzAN47FzCgwl/4K33XmfNi3sZHzWdK3qp2OcEJ+vHcoYU515Sh+0IWItWNHTiDIskZnE2O5uezujKebK9+ne9lWqrKyNaPrLtYsLdBIcONl5Fnwf1EYNxCIZaK4da3K6wu7hiqK+husZxusP14Eyczwddrh87iFtPp/cH7V53OziutMNmq6WuqoBHI0IJnBZO5IOt37HXaXp9JWVV4Okzuv3r+kFDMLhYOdRy+fWuYKjj0MFTe4LZLcWkr8mjeuw9RHidktw3nLm/AXiGxxIxuICHZieyfmMRWSk52KZGMc1El+PgmJ2dO0phtA9XuvRE/n2g1+NsZeNrWzBNu+fk61Mcbc+Olt9Hur3h76KLuucdak0+/fRTjh8/zo033siePXt466232L17N1VVVRw8eJD9+/fzwQcfUFBQwKFDh5gyZQrfffcde7rYQNJyCMWuOt6FeR+4/xd4e43G1dWV7777jj+mPU/i449x/XVjGGEy8euY+/n662/Y+5//cNFFF+HqehEuA10Yeuml7cbVUR6urq488lAcL2S8RNGW99hVUsL//vrXzfP/5z+fUlX1BTf7+wMQcPPNvLvlveb0Cy+8kIEDBzJw4EAMBgMXXHBBu981GTZsGAsT5nPxxYMZNmwYN910I/sqKprTjxw9wiWXXNL898d7PiHt+fafZOyMm9tILr74Yn506wSOHD1KtcW5G4+GCy9khKn1HvftkSM8+cTjmEzDMRgMfPDhLkpKS3ky8XFG/fCH/NDdnf+LncPf3v47AEOGXMwFF1yAwdXA0Esvbfe9i47ycLRejEYjQ4Zc3CpP0wgTNV91fVjZ3jAjtLEHhO2fpOdW4BWTxPxQP7yv8iJoRiz3enV8+944MZblj0QRNNYD91E+RESG4G4pbh4j+VD5XmyegYQFmDC4GPGcFop//V724YVpEJTlZbDZEMoTC8OZ4OWB97hwnkgIxVCUwavn7PvmrGxKzaB64lxmjW0ndrZabAzBUJHNopjw5icl03e0rqQab43hiXAvjC7gPjkQf5c6vCITmeFjBBcTQUF+8Oleyk6nU5vdDgbXVk15hgvA8H0tdUDd4Vpsribc3UwYjWZ8w6KJ8FEzzvmsOm8BwUGhRC/bjvvsJfxmXPvVteqCNPK/vuXkcUWcY2jsDeliwDi08f9uIcxPiiViog+eZg/8o6KY4FrOzl3tV/A7228r8jLZ5hPDU9EBeJqMmLxCmBc1mn+/s/3c7mTR3Rp7VrYadMIABuwNPRTbaG97r85LJasunPlR5/E+0MU4ypmxuYUyf3YA7oMMGFwc7++GoQ1DOLkOMmIc2v653eExwyuK+fdAbmo2m7PT2GyOZl5fXhX3EvvOLbzvEsCPbmgRN1stNowYKnNYNucn3DHldu69fxFZO08eXQ2m0Vx3lRnXL99mTeJq3hs6nXsn9P94dYmT9WM5Q4pzLzHiHxrDrNmJpOW/zdtrY/GuTOPRpMYhJp08T3ZY/66zU4eh9fyNfzeNQnBe1EeuD2CCoYisjGKsdjvW0jyWPFPQ/H5Vh+mO1oMTcT4vdLVe5yhuPZ1+juv4utvBcaUd7gHhzImey/LMArZkJzPDVMyahNXNQ0R2nl6LrdYVY30p6QsbR4AIj2FRdmnDiA9GP4LG2tmUkU3JYTt2SzFZSevYZgN7yzFq6otZGRHMjyLiSK8J4anF4bifBSMeOHt/A4DBZnyv98TkUkHuskTWfDSaGff6NTQgORsHZ32WzfPvwNR7AzH1RP69rE/ibN3Opl3DmTC55fnPwfbsaPl95NQWizN02223dWt+X331FRMmTGDYsGFcccUVfPjhh1RWVjJo0CCOHTvGoEGD+O9//8utt97KZZc1DI5yyy23UFZW1qXlGA2n31jZlXmvuerq5v//59NPOXr0KLEPzms1zbFj33PokBW8HefnTB4TJ9zCTTfeyKOPJ/Jw/G+49NKTDW/vvFvECJOJa65pKFfA+HG8mPkSez75hDHXXuv072ri4tL66Dt48MV88cXJRrmLXC/CZjt5w/Gaq6/ih+5dHyTk+Rde5LvvvqOurg673Y7NZmN/VRXLlq/gu+++47vv6hr+rWv4/48mTmh+6q+t4cOHM3z4yff+fVRSyjc2G9PuDGs13ffHjvHtt99y8cUXt83iFM7k0dl6ac/gwYO6/L7L3jK16b5D5W7K6jwJu7lr76O07Skkd+MWdn5UQbWtBmv9cGw2wAzu4wLwfHkL+cXhzPNzpbqwkJ31o5k1CsBG2R4LxrF+eLc4khrG3ILv4AJKPrGB17nXG9W2NY31u3yYk95BTxYXoL6U3AI/5idm8ITZzkcZiTyauBr37CSmNj127uJ68gRjMGJwaZy36avBBozYqKuDrp6JXA0GsNc1DB3QyF5nx+46BFfAODmGeVsXsDR6JhuDQwkLCyXoLFsX9qIkwp/e3vgbDEx4LIenJqlxsqe4hyaS4WehovRtstJiiT2cTOosn9abXmUeSzfsxntW2snjipwR+8FSNhYUsG3HXvZZrViP2PE+2n4Fv+P91sZHe8qx7UgmNDi59UxXB2ADtLoaGQy40tA3gqYhaOx27BhOHZKmve39YAFr0i1MXbwCXwPQD4fVckpX4ihnzOjh1eJ9Id2xvzuThwHfmXOZWriARzO8mLUh/LwYNnD3zg/g+v/l+paXTI31utffupF5C1/kkcvq2J21iMWLn+WyrKeYMmQvWQuX8p87fsfzd3tQs/t11j+TysLFJtYum97B8KDnIWfrx3JmFOde4z45ihlNf3hM4jezt/PuY4VsOxhKhJPnyQ7r364GXLHTuo2j4W/DIMP5Ux8xTmJ+koWlqxIJz6rD5BPKrF+GU51UinGIE+mO1oOjOJ8vulqvcxS3nk4/x3V23d3pccXt1LwMXqHManq6bpAXYXExbI5IZtOOBIJCDJ2nTwZXFxvvvrYF//gVvJlk4tD2dTz6RCJrPHN4PMBEWEIi1cnriA9PAzcfwmZHM/XTZKxDWj6N6cec1EzutZSzNT+NRTFW5q9NZGrXbjt2O6fubwBgZ+eaBaypjSEtcxJGSzGvrk1mVVwyxg0JBJmcjIMz7OWkL8vAcksCyQFN83Zj/n2gL+JsK95OmdGPWW36vXS+PeNg+T0ZpY51e8Nfd/NvfBINYMCAAVitVnx9fbn88pPDRW7dupWamprmhr9hw4YREBDQpeV4GIdSWnPgtMroMeT0arfHjzc8KZj5wvO4m0/viOVMHna7nU8/+4yLL76Yb79tPSzpu0VF1Hz1FVOm3gFA08OLm9/dcloNf+1p+TTlqFGjqPz85HvqDAZDl97F2OT+X/6i3e//8Ps1XS9gGydOHMdt5Ej+8tqrPZpHZ+ulPUePHGXkiBGnXaZe1YXeNxW5cczNsDN1dgyPx/jh/mUG0Q+0GC7YK5SIsQWkp8Sy+agdg9mHiGUJhLWqlLi2PuifBb1/zsTm1wqpPgxLI4NpGki3zm6nYmYom6NTSZtmxt3gxYT4aCY07vb+0ZH4Z69g5ycwtWuHv9NiMpvhYA3Wepp7Wx2qsYLb6Ia/XTyISMohaE8R+Xk5rJqdSe7sFNZFnS1jMoDh1jgysk7W9g0dPLUg3cTQ8G4zdw8v/IdYmZ6UwcbwFYQ1nUJtxax5Yh0VfglkRPRxLb6fsJemEZtQiDE8hvsT4/F1K2VlxAIqOrp5Y+hovzUDBoyhS3g7oRcOMOcyNzMmdjcMW9R0oWC1cggzQS3PWx1s79WFeWw+bMH1sem80fhd3VE7JE8n+J143l4R2ks/pI85G0fpId2xvzuRx5fllB01YnSpw37UTpd7IZ1zKvm4zIZ5QpshgkxmzIbRjJ83k/GXAQzhxhmR3PjKCj74BKa45PG6NZCFdzbsDMOv/zELk+qYNyuT1z+ZzpzuuWQ795n6vn58XlCc+4zBzYyJUmy1OH+e7Kj+bTTh7lbHvoM2aOpqWm+l2goj3EznVX3EODaK5ZlRzX/btyazweiB51An0o86WA8uncf5vNHVep2juPV0+rnO0XV3y0lbHVecyNtowt0IFbUddAVrlT4c03ADN82II8KvIc7uE6MJG1vAq7sqIMAL3AKY80wAc5rmtxUQv8yTGzxb1wmNbmaMbmY8x3pij3mA9XnhTI31cTokPcLZONuKyN0EU1MnNUTM7MeMpESsM+PIKowmKMrsdBw6Z2VzciLptlCWrwppvXa6Jf8+0gdxrvikHLtHVMNQqc0cbM/XVzpefh/o9qE+e9Lx48cZOHAg7m2eEPP29ubo0aNnlPe1Q0cy+IILuzzfkAsNjL709Bpjrr7qSgwGA1u2/OO05nc2j/QXMzCbL2NZ0mL+lP5C87CY+/dX8eln+3gy8XFeeiGdl15IJ+vFdIInB7H53ZPv+bvggguor299l7C975wRcLM/1dUW9uw5e98gOubaaznw3/9Svndvj+bR0XrpSM1XXzFs6FnehdLsgTsVlLUY8xvsrZ4Ka6W+nPxXS/GMTmJeqB/u7XU22ZVDuiWE5S9kkpebQ87aRGb4NZ3CjHh7mbGVFlPSctz90mJKjpjxHX32915pz7SFmeTlZJKd2fh5+h48DV7c90wayeEe4OaH/+WVlJS2iHNdww2z02hHPy1GHx88rcXsrGr6xkZJcSXG633wbDGdacwkZi1M5bnZHpTkFlByNvUWdTFiMpmaPx29sFrOlA1bm1O0YbAB1/oWvSzt5WQtTCTXNYrlC0P09Fg32ZmfR7lfDMkxIfianT84nLrfNh5rd/yTnWf/aCB9y+yDv6mCj3adHPKsurgYyygffJtO4Z1s7+5hyeTl5pw8/r+QSJibAf/ZfyIjIaRXf0qfciaO0kO6Y393Jg8LuSk5EJFMyr2Quzrj9IYeP5fUW6isguGXtTnLjbyRG0dVsnt37cnv7HbAtaFeZwd7fV3rJ24uNTEcO/YzuwTuX86C+vF5QXHuHfVWrG1GZrfuKaXCxRP3kThxnnRQ/3bxwtfHlZLi4pNDlJUX89ERH/x9DOdxfcTG1sLtEDCJG9q9NmyT7mg9OIjzeaOr9TpHcevp9HOWg/3e0XHlFHas1jaVM0s5ZYeNuJuNjtNdPPEfa6KidG+LgRQbhnd1NbjSHuvmIkrMAUxobCC229oUmCEYB4Hd1peVRifub7Rip+574PsWX7mYMA0FW237r+BoGwdnylSyIYEnt5uZ9/RcJji4Xup6/n2hr+JsY1+FtbFRvGVejrbnri+/N5xTDX8DBw5k8uTJpwwnOWLECG666aYzyvvCgS5MvbzrT6LccYU3Fww8vTBefPHFxNz/S9Y/l8ZfC97k4MEv+aiklMyslzsv64UGDh78khMnTjjMY//+Kl56OZv/i/1fbh43jgm3BLBi1e+AhmE+hw0dSsiUyZjNlzV/Qqffwef79/PZvn0AXHH55Xzw4S4qP/+cL6qrO/zOGd7e3tx88zhe/X+v8Y+t2zhw4ABfVFfz/r92MGnK7fy7+AMAvjp8mKnT7+L5FzLa/fuTT8qw2Wqp+uILtm77Zxei7tj4m8cx/uZxLEx8kn/t2MmXX35J0Zb3ePudzR3OY7iwoXLw34MHncqjs/XSkbLyvVzrfXL8V5vNhtVaw7fffsuJ4yewWmuwWmvO9OefluoqCzY7YAog7FZXNm9IImt7JdWfFZOfmszLpR2clF1cMQ6Gil3bKTtopaK4gDUpeZS1vLFRX4fdWk5JuQWr1Yr1sI2W53jv8GiCjr7GkuQ8dn5moaI4jyXJr2G/NZp7x/Tkr+45BpMZd3OLj9sQXDFgdDPT8P5pD8Lu86MkbTX5e6zYDleyKTWDbW4hTOvGTk+2w1asVgu2eqC+lmqrFWtT8D1CmXGLhayUDHZarFRsTGX9v8xE3OMHQEV2Eo9uKKTMasduq6TkE2tDTz81rp13bBtXEPXLRNLfog4tqwAAIABJREFUKaXCYqGiuIClqYXUjZ3EBDeg3kJ+UgJrqvyY93AoJquF6irLyeOKnDbjoCHUfVrM1kor1eXbyU1OZVPLizNXA67YsDZeAXa233qHRxNkL2Dp4mx2VlqxWSvZlptB/md99OP6kN1mxWq1Yjtih3o7NqsVq9XWcAHg4kNYhBf/Tl9N7h4r1j15rMmp5KZ7Q/F2wfH2bjS1Pv6bGzolGIaacTedyzciushRHKVHnc7+7upqgKNWDtmcy8NakML6qkDmRPrgGx3PXbZsVmZX9vyP60tf11BjNzD80iFtEjy4I8qP3c+v5q1PrNR+Xcnf12fyvlswU64HbgxmyqAtbFi/hc9rAbuF957P4f2RgfxIT/u10Dv1Y1Gce4N1YzLRsxNJLyqn2mKh7J00Fm0oxnx3JEFGHJ4nHda/MTAxPBzTP9JYU1iJ1VJMemoetZPCmerG+VMfsZeTuyqF3O3lVFeWsjk9iVU7RjM7uvGdTI7SHdZXHMT5fOEwTpXkPhhKcFxe47vmHMWtp9MBu63x/lMtddQ13huxYT+bOjK34Wi/d3hcabseKl8jITqWpbnbKauyULGrkJWLMyn3CmfGOIPjdAz4R0XhviWVZwsrsdmslOWnklvuRdAkD8BGSXoyawqKqaiqpGRjKgkb9jJhVmTjdVM5z8dGE7Mqj53lFqqrytmcnkx6qYnbQvruhOP4+NomjsYApt1SS+7aNLZV2QEbFRvTyC33ZFqgl+M4gIPt0U5ZdiLx2TamxsUzYZC1+fqy4TLfifzPQn0SZwBqsNaAyTS8TYkcbM8Ol983zvqhPnuT99CRTLzMk60HKpyafpL7lVx9yZk9lxD98xkYjUaefyGD5FW/44orLif0jmmdznN78BTSX3iR6XdM5bIf/KDTPFY88zuCp0zmujENV4Xxv3mQyJ/9nMK3/87md4uYMjnolIbU8TePY+ill7L53SKuuvJK7ov6KR/v2cPMXz7A2Bt8Wbv6d+1+56zwu8O44vLL2fVRCR988CEXD7kYn+uvO6UcLhe4MGBA+38Xf/AhhX9/h8EXD2bMtU68DLGLkp9eyvoNz7Fo8VPYjx3Da/Q1RM+c0eH0o0b9kOuvG8OSp5eTm/2ywzw6Wy8hwVNOyb/qiy84Zrdz1VVXNn/32BOL2LHz381/Tw/7MQDvbX77tIZPPRPhkbFEpOYxf6yJqQtSsK1eTfoTM9ng6sGE8BjmhVlY3+6cHkTExVC2KpVfRaZi8glm1uxYpiZknJzEJ4SpQ+JY82AkK5t7exgY4RfFb5Ni8HcL4anV8Oy6DB59IAX7YA98gxNYN6d/PzVkClnA72pTWZkYyarDRsxjQ3h8RUzDOxi6Q30RKyMS2dQc82TCiwCfWPI2ROGOiakLkqlekcyj92WAyYewhUnMuqphavebA/BMz+GhGUkcqjPiPjaYxxPuOS/e3yOtGactYPnRNNanJ5JeaYXBZrwC55IS2/A+J/s7qax6xwoUsTKmqMWcpsbjSh8VvB/wjY5nVtU6lv4yEsx+hM2KY87hOJq7sYzyY8LVObycnMPUzBiu7Gy/dQvhqbWwfl0GTz6QyiFMeN8cwgy/82F4vpasbEwMZ+mOpr9LeTS8AAwBPJW/gqlG8Lw3ieU1SayMj2QNZm6KSOKJsIahPbS9O6+zOEoPO4393eQ3Cd/0dSxNDSFoYUDneRz+J8+m7cY3JoMJRgA/5sROImpFCrmTVxMxqjd/bC+qtVKLkSFt2/2A4VMWsKw2lbWLf8aar42YbwjmkWUPcL0BwI85Kxbw/Jo/8fC9S6ilIX3hige4Ue+8bKXH68cCKM69wRSayPKjqazfEE96VR0GN0/8w5bweIv3GnV2nnRU/wYw+EST8oiNRRseINxqxHPSTJbHTWr/HfP9lcGEp1sNa5JjWXnYFfcxgcxOiaW5uuEoHcf1FcW5QedxMoALuLa4Jegobj2dXpEVT2RaeXN5Kh4MJwsjYc8U8PhZOqSxw/3e4XGlzXrwiCJ5MTybnsLcdRbsg814Bcxk3YNRDe+GdpQOMCqc5YttLF0bR+gyO0ZPP+5a0nTfyIjJy8S+9YlEJ9dhGOVHWNxq5kxuXAsuXtz/dBxsyGHpvHVUH4ERV/sxNXE1c/z67oTj+Pjadns2MXVhMnXrUlk5e3rD7/C8hYglyY3vR3QQBxxsj1cVsCa1GFs95C+eSX6LsnrHZJIxy8Nh/mejvogzAPU2bDZwbe/9h51uz46W3zcGnGj5AjYBYM9XB9m4v5y6+u/bTb/I5QLuuMIb76HtPgst0u1ezHwJa00ND/1mXl8X5RQlJSX4+vr2UO52StY+wJLv40mL92uuhNkr80h4YB2uCW+yPERXmCIiIiLSe2qPHe/rIpwXhlx4Tg1QJCIiIiLSo7755hsAPvzwQ4fT6om/dowZ5saVlwyntOYA+775iq/t3wEw1PUirjQOx2f4D3B1Ueikd9iPHeP1v7zBsymr+roofaCO6v0Wal0bHlE3GoF6K2XFxVS4+jHrnB5/XURERERERERERESke6n1qgMXuVyA/8hR+I/sr2PNyLmi+otq7gydzhWXX97XRekDRqbGJ7JvbSZzo1KpczHg6jIEs88k5qyNZqpG/BIRERERERERERERaaahPkXkjPTsUJ8iIiIiImcXDfXZOzTUp4iIiIjISV0Z6lM1aREREREREREREREREZF+QA1/IiIiIiIiIiIiIiIiIv2AGv5ERERERERERERERERE+gE1/ImIiIiIiIiIiIiIiIj0A2r4ExEREREREREREREREekH1PAnIiIiIiIiIiIiIiIi0g+o4U9ERERERERERERERESkH1DDn4iIiIiIiIiIiIiIiEg/oIY/ERERERERERERERERkX5ADX8iIiIiIiIiIiIiIiIi/YAa/kRERERERERERERERET6ATX8iYiIiIiIiIiIiIiIiPQDavgTERERERERERERERER6QfU8CciIiIiIiIiIiIiIiLSD6jhT0RERERERERERERERKQfUMOfiIiIiIiIiIiIiIiISD+ghj8RERERERERERERERGRfkANfyIiIiIiIiIiIiIiIiL9gBr+RERERERERERERERERPoBNfyJiIiIiIiIiIiIiIiI9ANq+BMRERERERERERERERHpB9TwJyIiIiIiIiIiIiIiItIPqOFPREREREREREREREREpB9Qw5+IiIiIiIiIiIiIiIhIP6CGPxEREREREREREREREZF+QA1/IiIiIiIiIiIiIiIiIv3ABX1dgLPZl7WfYvm6lNq6QwxgAENcR+I+1AfTxVf2ddFEREREREREREREREREWlHDXzsOH/2CrZ/+kUO1n52SVrz/FdyMo5l49a+55KLL+qB0IiIiIiIiIiIiIiIiIqfSUJ9tfF6zk4KSJ9tt9Gty0LaXv5YkUnX4w14smUj79u+vIueV3L4uhoiIiIiIiIiIiIiI9DE1/LVQc+Rztvwnle+P1zmc9lj9dxSV/57DR79wOv8DBw6wMPFJpk6/i6CQaTy84FHKyvc2pz80P4FHEh47rbJ3RfnevYyfGIjFcuCM87Lb7byQkUnkfT/nR7dN4e6f/JTU9X/ku+++a57mxcyXmBXza/73/+bx0PwElq9YxV/y/8o333wDQO5rr59xOU5XXV0dCQuf4LYpt3P3T37K999/32dl6YqH5ifw2BOLABg5cgRZf87mwIEzX5+nq8TW9D87mxYGE5y0vc/K0l9Yi7NZFBtJcFAggWEzid+wner6xsStSQRPDGT8KZ9Qlu7orhLYyH/41GXMzbUCULI2sp3lBzI+KJFNR7urDNJfdLo9N6XPbkwPj2FRdim2jrOTjhwtJWvBzIY4Rqawze5g+j1pRAbFklXVK6XrP45a2FmQwdLZ4QSGp7Czvk16vZVtqXGETwsmMHQm8WnFWFskd2V7r86LI3BiMPEbHa3MfshBHKWHWQt5NCyU6IzyHl9UdXYsgZGplLTdl9rRYf1jYiDjA8/S49m2Jdx9221MOeVzJ6t2npysdvfrrJr3M+6+/XbujprLqr/spbYp0V7J31MeYsadt3PHHT9hTlIOu2vbW5jYSvNYGhtJcFAwwRGxLM0rV52iByjOPa8iI+aU41xgYiHNNQJn6hud1L/BTsXGFGIiggkMDic6MbvFdX1r/bo+cnA76xfMbNiWI+NYWVCJvSvpDusrzse5X+tyvc5R3Ho63UbF1gLWL44hOCiG9I6fDTlL2di8OLyhbmRp8XWX14ODOFiKWL8whtDgYAKnRRKTXEBFqx3EQZxLUwlvW58La+f6CqD8/7N372FRVesDx79KDqKMt1FOkB3oImZCHskLYoIoKElhlAblkcLoaPhTwbwdEzPxhloYJmlSFBwKkqIoUsNELI1zJLyAIZgJZWDmeGnwMmM4vz9A7jCDggK+n+eZR9lrX9a8e8/aa++19tox+Lk647bi1t9vNFy+1qQhLzGUAG9PnF098Q4MI/nn8kBpUgh2rrtu67Opdj287vLYQJxLi9keXmW/R6YbyG/L0KRxNjK94d9964tzi274y83Npaio6LqWPXDgAGfOnDF6fj16vj0aSenVK0Yv89dVLd/+9DagNzivTqdjRtDLXL58mbCVy1n3+hp69epJ6PKV6PVlyw8ZNIjBDz9s9PZvtStXrjAz+GU+TfqcZ5/xZfOmSObODuKHrCx2fLOz2rz33nsvr68J47XFi3h6wlP8cfoPPkr4mKtXrzLOY8wt+gaQ+s1O/vvf/7Hp7Q28/dab3HFH6xj91qxjRzp27AhAx44defwxTz5J+vyW5WfB+gza4CXAraNJ583QJDSOwUR9mETMfBc0SYtYEFdYlj44iJiEBJIqPrGET7JF2c+HCQ5NlYkSNCUK+vpHVtlOAks9VQD0nVx9elJCJNMclNh4eeNq1lR5EG2CoeM5P4rghSngEUJMQgKbp/Yhb1MIa9JvxyviG6NOjWHTIRX+kQnEr/NjkOJW56iN+jWDxNQsjmt01NVVrCA+hAXpKvzXxhIf9iSmKSEsTiq/4m7M8a5OJSL6KIrbtExtMI6i+SmtGTTcEaf7VLc6J9XYPx9JUmICSYkJxATagdKFVxLK/k5KCGWi5a3OYR0GBbEp7iPiKj4fsMK3D+YP+DB+YPk8xxNYOD+KX+5/gRVRm1kxxZZfdmVQXH6z4PB7i4n4qT9zYr5m65ZVPHrpY5ZskPp3LT/HEzwniuN9Agh//13CA/pwfKehm0Si0STON4W2RIPp4EASqlxzxQe5cK161+B50lD9G9AdjGL+2iz6To0k/v0QXDWxLFidXrsBty3XR0oLiQ4JZXsnXzYkJBAVaEve+iCWXauXGUrHcH3F6Di3cY2t1xmKW3OnU1pI2qepZBdr0LXCsk2TEUlEhg5ljemNrl83FIdLWayZsZq9vXxYFZtAfJgvPfeFE7whp6J+YjDOJRo03Vx45cMq95Y2BfCQSc2MFJO4PoEi0xZwgWtE+VprkT2RzI7R4Loint074lk6uJCIRVHklQJKdxZV/f4JCSQs8cSqmwMTPW2rr6ie8rjhOOvIjpzHmkPWzIxMICncj57fhxK8Kadl1yObOs5GpBv63bfGOLfohr+uXbuSmZnJhQsXGrVcYWEhv/76K+bm5kYv89vZA416eu+aMxcKKT7/o8H5co8c4Zdff+WVBfP5x4CHGPCQPQvmzuG9zRtp164dAL4+E/F5ekKj83CrxH0Uz4+5R3hn4wbGP/4YD/TtyyPDndi8MZLHPMdVm7e9SXs6mZmhVCq59957eNL7Cc6cPUdRUTGdOnW6Rd8Azpw5g4WFBX1t+2BleX13Cq5evdrEuTK8rW7du9G9e7eK6e6jR/HV1m0Vjcg33cGsyoJS3DilC4tiogj3c8TGUoXNcD+mOJtSsK+8N5ZCiVVvy8qPWT7J27R4BfnSt1YF6TqVatBcBFVv22rbUpVXMBTdVNWm9zyRQnKxM7MCHGgBVTHRkhg6nm19WLUpkqXedlhZWNLXI5Apj+jYm5bVsiuiLZD23BnoZccgW0usLFXyW2wutt6sWreOsIl9MK2ZVprDlqR8HvYLxMvOEis7b+ZO6kP2lpSy86TRx7uGvZuiyB7gjWvLane5OQzFUTQ/hS0T5ocwbXgLOwCVKqwsLcs+SnNMMafntb8tVSiaqh7UlBTm3HmXZeXH7Chbv9bx6Awf+pgAlPDtB7EcfSCAhTPd6P93a/qPmUVE+OTy9EIO/FBMnzFPMLAHYN6H8ROd4dB+jsrvoQoNadGx5D8QwNJgd+ytrbH3CCZqvV/T1Y8FEuebR6PRobCwwabqdZ+qvHZn6DxpqP6Njj1JKZS4TGeWuy1WvR3wD/TG/LsEkqu1AbTx+khuCsnH+uMf5ElfCxU2LoHMnWDOroQUioxJN1hfMTbObVyj63WG4tbc6YCJHf6vr2PDfE9sWlvZpsshesNurLzcquf9eurXDcXBzIEpr0exIdgde0sVVgO8mTvJDvWeVKOPf52mBF1Xa/pWLecslbWuY9Up4Ww85cYEx1pXXzefwfK1toL/ZaEZ4MnEfkowUWI/yYdBZ7LYewJAgarq9+8Nmcm7UU0OZkLvqmuprzw2EGddFlu2leAaMB3Xe1WobN2ZO9OdkuQE0s41T4iaRJPH2XB6w7/71hnnJm/4Ky0tpbS0aa5ArKyssLa2ZteuXRXDQhpSXFzM/v37cXR0RKEw/pbXiXOHrjeb/HbuoMF5rjVuHcrJqTa9ah7nLljI0mUrKv6OT9jCy/MWsHX71zz19DM4j3JnwSshaDQa1kW8xZhxj/HYE0+xdfvXFcvkHz2K8yh3duxMw29KAK7uHjzzz+fIzjncYP4uXLjAilWrGT32UcZ5PcGqNWu5ePFig8skff4FT4x/nDv/9rdq09u3N3xYde/Wjfbt2jW6UbemfZk/8GF8Aodycli5ei2zZs/l9fA30WhKqs0z2f8FRri64TvJj9TypxHfez+GtzdtpvCXXxg63JnN70YDUFBQyKzZc3Ae5Y6Hpxfhb65Hp6u8HRefsIU58//NV1u34enlTdDLc4Gy/fdB7H+I3LiJseMex23sON6N/oDfT51i7oKFuLp78Kzf89WGdwXYs/d7Jj3nj/Mod/ymBPC/ffsMbkvVQ4WqR2VJb2NjjR49hYW/3FA8r5dWaY6yaqFooiE7MRQ/bzecXb3xC0utvweoOofk9SH4TfDE2dkNz8khxOVUuf2pK2T7+nn4eHviXGNIybTydRbtiWKBvzfOzmXDb7SFoWYUyup9s7Q6wMy8zhv52QkxZA7ww9/uWmrZkKsBMVmkbZqHj6cbzm4+TI/OQl2cwcaFk/F0c8PNZx5xOfVFqgRNiSk9uxlTjhazJXo3Vr5+OFVkW0NmTAh+Xm4MdfXEe0YYifVuS7R1DR/PSqysq6YrUZlTrdwVhhVEB+C7KR/tzzH4OTvjE1UIpWoyE8MJnuyNc/mwkstrDk9UTcO/W11hKhEvlw8l6hXAgmgZerGWE1lkq20YNKDyHK16yA7L4hyy1WDs8a47GMubuy3xD3BEeTve2DcYR9FUitIjCX62rA7m9uw81qTkl5URpRks93QjOOXaUw8ZLPfyISI9g+iQ8iGdvAOJ2KNGfTCexQHeuLm64T0jkr1V9lFRfCDOM6JIjg4lwMcbNzdPvIMiSTMwJGe1ut2z84hIN/YOaXkdKC6LtPVBeLu54RdTaLi+mRvViKjdmMMfx7L/ocn8s395HUuXxbeZOoaOcePOOpcwp0cPKDx0mGvj2ZT8oUZXs/59u9NlkbZPi5OHG1a3Oi9tmcT5JtGhKdGgUvWoO9mI82SD9e/SfLJztNg69K+8vrzPgYeU+WRXKRvben1EV1yMupsN91QJVd/BDqjys8jWGE43uB+MjHOb19h6naG4NXd6K1cQH0nyHd7M9FBVv+5rhvq1ytqy2lOFSnMl6MpHRTEizpoSDXQzUJ85l86bUUcZFNCEHd1vUGPu1wEoVUrIz+LQtUtrdTGn6YGqjueVdHtiiC52Y9qT1tWn11ceG4rzuWLUF1VY9a7MnWKAIw+TReaRRnzpW6Cp49yY/VBLK41zkzf8paamkpqa2mTrGzBgAO3bt2fnzp0cP3683vlKS0v58ccf2bt3L126dKFXr16N2o7m8vW/H+3Py78bnKfP/ffj6uLMwkWLCV2+kqM//WTUujP++z8OHcrmPx+8x7vvvM233+3hmcnPc//997Hty2Se8XmaVavXotFU3pjTarWkfLWVN1av4uuvvmD0qJHMmBXM+fP1N56++toyLl68yAfvRbF5YyR//HGaN9dvqHf+M2fOcvLkSR6ytzfqe9R09tw5rur1dDbvfF3LV/XTT8fIz/+JoBn/R+iSxRQXnyRl6zYALmu1LH4tFK/HPEn5/FNmzZhecXPtWd+n8X9uMr1738X2r75g8qRnuHDhAtNnBtGpUyfee2cjr70aQvq33xK29o1q28zJOcz21B3ERL/L2rCVFdPfez8GK0tLtn75OYEvTeWdqHeZO//fPDd5El9/9QV39+7Nmtcr1/Vj7hGWhC7nXy9M4fNPPsb/OT8WvLKY36oMcVvXtiY+5c34xx+rlqe+fWz5MTf3huN5PZye9cSmyt+anZG8ecyOmWHvsiHYAU3KOjZ+V3cFSnNkN3u1/fEPiST+w3X42+QQsSKmosdRXswiln+vYtr6JHanJbDK2xrlAD/CIwMZZAKafeFMX5SC1jWIzbHvEu5nTd6GYBYktpEudKUa8raFszGjB49PdKw1VAOX0tmyTcdYb5daadkx4aRZBBCVvIP4YFvyokPwW5iC8tlIUrYnMLffUTatS6Kgzg1r0FzSkrmpvJFwQgALNmWgruuCLyeJxBMOTBhb+dSsLiOSV2PUuK5IYEdyFEvdLaH2szHidmPoeAYozWfvQS02D9jKE2uNYDMpnA3+tpha+7IhOYUoP2tQZ5F20JTh00KJ+TCWN7zNSVsbxpZ6iscGf7fqdJYFRXF8wHSiElNIWuuDMjWEZbdVd2UjqNWosaRn1V6YKkt6UkzRqTrmr+t4L80nbn0Kps/W7OV5G2lsHMX1OZdKxMpUmBhOyvYkYoKH0dPEtP6yt7SYxPWfovAOI2V7AqsGq4l71Y/AmBI8ViSwI2k1Tup41nxYvaOj9mASyZecWRqbxI4vIvHvlMqrITEU1HMTWbMvnOCwHGyeDyMpOYGoqTZkrg5p1Pt1ChLCSDTxZcMXXxE1ydpgffOmubSbz7/WMXq8MxX3GE4W8ovOmj4P1HfXQcXogBewzlzGjPlvs/Xbz4j44ChDfcfx95uU7VahuJACrQ19+9VZuxBNReJ8k2jRXAR1+uqyTpxePgQsiSfz2lMDjTlP1lX/LlVTpDalp6rKfjRRYaWC06fKWwBug/qIQmmOokTN6ar9jk1MUZSeoeiM4XSD+8GYON8OGl0/NhC35k5vzYqTWJOgwSuojqewm71+rSNzXw70seMeE4yKs0ZTgvZECgsmeOLs4Y3PjDASczXV1xkdyd57A5jpbgktrQOCMfc3ABvvQCZ0SmH21BA2bksnLjwBzVhfPGo9Sa1m26e7UXk8Wf21HQ2Vx4bibGaOwkTN6apPnZWagkLL6VOtpHN+E8XZ+P1QVx5aZ5ybvOGvY5V3jzWFY8eOcfXqVQYOHEhubi5bt27l8OHDnDhxglOnTvHrr7+yf/9+UlJSOH36NKNHj+by5cvkNrIB5Kr++odr1OuNK3lWLFvK3JeDOXDoEP98bgozgmbz668Nd3ft3r078+e+jJmZGX3uv58H+vZl2NAhPDbuUdq3b89j4x7l8uXL/Hy8oNpy816eTc+ePenQoQNTnn+Ojh07kvrNN3VuY/+Bg2Tn5PBqyCv0vusu7rKy4v8Cp/H1jrrnh/JeGUCvKmeM5SvDGDl6DKPcPZg1e06dy+n1etTqM3z55VdY3vk37rK68X6CnTt3ZsKTT2Bm1pEuXZTY29txvKAAKGsEvXz5Mn+/+266dOnCMMeheD7qAZQdq6amHTFpb0K3rl3p2LEjH8Z/TLv27Xnt1RDuv/8+hg4ZzKJ/L+DLlK849nNlw/OFixd5ddErqFQ9qj21OdJ5BE+M9yrbN+XDnT7j64Nd//506NABjzHuHMnLrxiS863It5nyvB8uziPo3r07ri7OOA4dwq70bxvcllKpxLxGo6mqp4ozZ8/ecDyvx6RHqpeSpkMCCZ/vzSBba+w9fRhrraXg57pvDiuHB7Jqji+uA6yx6m3HBB93rIqzyDwFoCEvvxjVME9ceytAYYmr5zCU+YVo77NEiZrtMSloXIJY6udCX2tr7D2DWTrJksyYGDJbeWetoqR5uLl64rciA6upy5g1uPbpTfe/3ew1ccR1QO3bdMpHAljkbYvSBKxGOTPIRIutTwiT7JRgosLV1QGOHSWvrjiZ2OHxvB/T5kWSsuMrYuY4cjppEcF1jKednZqOxsGNQVWypz1XgsZUhZWFCqXSEnsvPybYSTPO7cyY4xmgKCWK5PPDmORpXWe6qIdCiVIBmChQdiv/v4U7c0MDmTDcDhtLawb5+uJkmk/mwborng39bguSYtlrF8BSP0dsVEpUtu7M9O3DDzsz5Km/qsp7uFYbdEIBCnRlPRRrqOt4L0qKJE7rzVzf2/g30Mg4iut0Sc1prYKelpYozZRYDfbG36Oh407Bw34hTHJQgYkKJ3cHeuqs8ZoTgJOFAro54DHYEnX+0WrlgqmlJ7OmumClAMys8QrwxqYglW35dW1DzfaYHfT0C2Gaiy2qbipsXALwH1xMWnr97/SoSWPhydypjliZKVCYGKpv3jy6zN3818SREQ9VObg1JWhQoihMYMW0p3h09BgmTllMXGZlFBWqPjx4ryWmf+wgImQd33Ybx0Sntjju3g3QlKDBHEVYM53bAAAgAElEQVRB2ROo157sjN4nZ6kmJXG+SZQM8gzAf2oIUck72LE+kL6FUSwILR9i0sjzZL31b60OLYrqy5f/fa2j9G1RH+nviJMinbiYLNQ6HeqcJJa9nlLxflWD6Yb2gxFxvi00tl5nKG7Nnd5qqdkeGUPR8On413F/qNnr1z/H895OGDvRGRUYFWcrR2+m+U1nVWwKu+PDmKTKImL+uoqhEXW5MURsV+E/07PFPWVu7P0NADpZYt/fBpVJAYkrQog41IdJE+t4TY46g+0He+A0qnq522B5bCjOSgdcB+jYHhNP9jkduuIs4kI3sFcDulbwcpUmjbOx+6EurTTOdzT1CkeOHNmk6zt79ixOTk50796dv//97xw4cIDCwkLMzMy4cuUKZmZm/P777zzyyCPceWfZ4CjDhg0jLy+vUdvpbFrPEApG6GRq3EVX+/btefKJ8Tzh9Ti7v/2O9Rve5sWXprMlPg5lPe8jNDGp3kXD3LwzJneYVPsboKSkhPq0b98eKysrTpyo+x2Gh7Jz+FOjweMxr2rT/7pyhQsXLvBu9Pt8+dXWiukJcbF07ly23dOnKyv4L039F89N/ifbU3dUG64S4OdjPzMreA53dLgDU1NTHrK3Y5zH2Ir3G96I9jVi1KmTGZcuXgKga5cuTH3xBeb9+xUchw7hCa/HGeY4tN515Rw+zAB7OxQdOlRMG/iPAZiYmJBz+DD33XsPAD169KBHj+61lq+6vxQdOqDo0IE7qkzr3LkzV65c4bJWi1nHjmRn5/Djj7lEvRddMU/pX6VY3lk50E9926qpUyezGx469XrNXpTC5tWVJ2KFmbKyB4aJEmUn0F7S1ru8JjeVxG27yTxUQJHmDOrSHmg0gKWSQcPsiNiSQpqPHa4qNXu3fY/axp17FEBpDodywX6mQ7UeHzaDHbCKySK7GAa14usUK88QYhyKKcjZQVxUIIHnwoj0t6t2Ujq0LwvsAut4+TFQtde+Qln23psq8yk6KVCiQauF2mc6FU6+fpV5cQxgrnc6L6amkjcpoLL3WGk+e7LU9H3SrvoQD6MCmLlnHsv9JrPNzRMvL09cbVtWz2BdeijeKzPKhqFAgdO/E1jqIo2TzcWY45nCJJZvOkxf/yjGyv3MJqE7lcO2lBT27jvKcbUa9UUdfS/VXfGs/3er4VBuPpp9YXi6hVVf6D5HNIDsrnIKBaaATgdce+m6TocOBaY1XsJe5/F+KoWI6GLGLlmNvYKW16v1ZmlMHMX1s/Rkln8GCxb54jfEEy9PT7yGWzd88Vu1vmGmRGGiwLRG3YJLWuqv9QG9LbGimIJiHfSrsbXSoxzK1ZCZ44PzxqrTQfWk8b1llda2td7NUX990+jV3rDDmfuh/0v0rzZEPlCaw2dbBzJz4QfMuVPL4bjFLFnyJnfGLWW0+VHiFi7np0ff4L3x1pw5/BkbX49k4RIV61eMq2d40NtQeRwTUxyYGxLDIksdh2JCWBCyDqv4UMZ2M7gGYQyJ801jNcqXSdf+sHZh1tQMdv07lb2nPJlg5Hmy3vq3qQJTdFRv4yj7W2GmuH3qI0oX5oYWs3xtCN5xWlR2nvg/701RaA5KcyPSDe0HQ3G+XTS2Xmcobs2d3kpp9kSx8aAd06LreSKqOevXunyiV8RQPGw+YY7lWzcizgpbT/xty5PMbPEKCiBtQhjb983HdVQxW9YnwYRwJrXA+3pG3d8AQEdmxDwiSgKIinVBWZzFlvVhrA0KQ7lpfrX39WmyMshTOuBf9fsaKo8NxlmF1/wQisI2EOwdBRZ2eE31Y+yxMNTmLeseXV2aLs7G74c6tdI4N3nDX1MbNGhQxf/btWuHWq3G3t6eu+++u2L6nj17OHPmTEXDX/fu3XF0dGzUdu7s8iDH/vjuuvJo2aVfo+Zv3749I12c6dfvAcY/OZGM//4P99Gjrmvb11x7gqw+DTUM6vVXsejVi88/3VJn+gv+zzPpmWcq/u7atSvt27enp0pFds5hRo9yBaBHj+706NGdrl271FrH3b3vZvpLU7njDpOK5ZtT1Xj4Pj0R15EufPFlCq8uDWXI4MEse+3Veper2RjZrl072rVrZzDGjcxg2T/AnNlBFU8H3ohLFy/Rq2fPG17P9bDcF0NiriczG/gp1Ne/oSAxiOkxOsZODeCVAAes/ojB74XK4YKt3HxxjQ9n48s+rL2kwLKfC6tC/aoNLUoLGee7ySnK3gVlZW3LIHM140Jj2Oa9Gq+KC+pCso9osBp+c4ZE7GmhghJN9fcnao6Sd0KJjXWNs6TCmgmhCbjmppOclMDaqbEkTg1ng68tLYXikSBi4iqPTIVR7zMU183Q8azJImLRBgoc5hMz4SbehW3DdDlRBM5PRekdwJSQYOwtclgzYV69w+vV/7u1BBQoPZexY37j6le3HQtLVBwuGy7n2gWbWs1pLHG1qDJfPcd7UWoSaeeKMf33OL4on6a9pIOwcbjtDGbHas+b9EVuMWPjKG6QEnu/dSR55JCWnETciheIdghmc2gz96q+qCvrMGBS13m3rDeS6/yvWOXRdOdlQ/XNm6OQH/M0WDr1qV5vU1liqejD0JmTGXongDkDJ/kw8OPV7D8Co02S+EztzMLHyn4MPfo/wcJQLTP9Y/nsyDimPXCTv0ZLpbLESmGLU7AfTuXF6iA/HwbFrybzCIyV01fTkDjfMgoLS1TkoCnB+PNkffVvpQorCy3HT2ngWhNBqZoiddk13+1UH1EO8GVVrG/F37o9YWxSWmPTzYj0Swb2g0nDcb5tNLZeZyhuzZ3eSqV9mkrROVju48by8mlanY6CyZ6k+UUSNaq56tdq0sJCiNZ4smqte2WH0OuJs1KFlRIKSjSQm0JijgZ1fiDO8eXpOh1aFuG8x4XdKSE3kukbZ/B+XTlNOonbYWykS1lsLB2YFBqCenIQcal+uPpWXgsWHMlHZ+1bNlRqOUPlccJKa8NxtnBk2uuOTKvIUwrBK2x4yKYV3ANrqjh75hi9H+pkzPHcAuPcvK0vTezq1asVT69V1bdvXy5dunRD67ZRDaFjh8a3wHZSdOfu7g8bnO/kyZMU/vJLtWldunShXbt2mCqa9wA4efIkv544gZ1d/zrT+z3wACd//538o0frTO/cuTMqVY+Kz7VGu8c8x/HZ58mcPGn4/YgdTDvQq1dPunfv3uyNfnX5m4UFAVP8Wff6WlJ3fMPvp+oe1+fBfv04lJ3DlStXKqYdOHiQv/76iwf7Nf1V9QN9bUn/9voanGs6c/Ys3bvdmi6WPdGhvZ6fYGk+yVtysPELZaanA1Z1/AQLkqLIHDyfhNgEUhJjiQoNqLjIxMSWvraQn5VVrTGqICuLImUf+rbatgMNmhrxVHRSYFpao3dJaTHHT4DK8vqfWK7XOTWaao0DOvJyC8HSBquqDa0nCinAEqt6Yq3q54L/wkg2T7UmOzGF7JbUW9REiUqlqvg0+EJpcQOMOJ51+cQtDCHR1JdVC93l6bEmkpmcRL5DAGEB7thbGl/XqP27VdLX1hLNvu9b/RDKzc7SjkGqAg4drBwRoSgri+LedthfO0U3cLxbeYWRlJhAfGxs2ef9ELwsFAya+i4x891v6le5pYyJo2gyCgs7xgaEELPWF/Pvkthu/Iia10WTlUEeZXW4Wsrrdpl7vq/e0ehGGFHfvClKiyk8AT3urHGW6zWQgb0LOXy4SkdNnQ4wLRtOSAe6Um31Ht5dVfRAh+7GLoHbFgsHBt1dSHZOlSNHq4NawzKJGyJxvjlK1ahrFILq3BwKTGyw6oUR50kD9W8TW+ztTMmueh2dn8Whi3YMslPcxvURDXtSM8DRpe4RdWqmG9oPBuJ822hsvc5Q3Jo7vZXyWBhLUkJs5e925ZPYKGx59vUowrytm6l+rSF703xezbBk5srpOFVdj8E461Cra1xcFueTd06JlaUS+k0mKqFKORQby5xHFChdgomPCrreDDcBI+/XVdCh/Qv4q8okExWqbpWv0bq23uMF6vJOHpUMlceq6zie1WnpZFs64tQCn6Ss1NRxNnY/1KOVxrlVNfy1b9+eUaNG1Rr+smfPnjz8sOHGt4bc0d4Ux3v8G7lUO4bdOwWT9h0Mzvlhwsc85x/AB7FxHP7xRzJ/yOLfr4RgZWnJ4EE3lve6vBX5Nod/zOXwjz/yyuLXuPfeexg10gUARYeyA/Ja49fQIYMZOmQwC0Ne5X/7Mvnjjz9I3/0tO3amNbgN/+cmc/fdvXk+YCpfpnzFkbw80nd/S/IXKShMTRuV3//+bx8uo8fwQ9Z+AM6eO8fYcY/z3vsxdf5trNwjeYQuX0l+/lEuXrzIgYMH6dSpE1271H4qEWDSMz7orlzhtWUrOPbzcfZl/sDylavxGOOObZ8+jdq2MWZMD+S7PXuJ3LiJE7/9xvGCAja+E3Vd44rn5R/lgb59mzyPxthr4Yxr4x58LWNiirITFBzMIO+UmoKsFCLCk8ircmNDq9Oi+TmHzBNq1Go16nMadBXplnhNckexcx3L4zLIO1FI9rZwFn9YjP2zvji10jqbZttqfJ8PIXpnDgXFxRRkpbA8MhXtABecqvbEOqdGrTWlZ7emvoOlY+/6APzmRbH9YCFFJ/LZGxfKmlQY6eNW7SkA3ali1JjTs0ZFsSA+lAWbUslT69BpCsk+oi7r6SeNa7cdg8dzaTHJofOJOOHAzJc9UamLKTpR9tFII9MNUZqZoz2WxZ5CNUX5GSSGRbK96sWVqQJTNKjL7yw19Lvt6+2Hqy6F5UviySxUo1EXsjcxhuSfb9GXu4V0mrLzkeaiDkp1aNRq1GpN2ZPtJnZ4TbDlh+h1JOaqUecmEZFQyMMTPcuGSDZ0vCtVWFlaVvmUdUpQdLPEStVKT2rXw1AcRZPQHYxiwZIY9hZq0Ok05B3OR21qiVXXpt2OtjiV96JTyS4sJG9PDIvXp2Pq7odXeachU1NF2fsGNQCWePm5o0xfx4KodApOaVCfyCE5Oons623kMqK+eVOcP8MZnYIeXWu+4sGaR30dOPzeOrYeUVNyvpBvNsbyXws3RvcHBrox2mw3mzbu5pcSQFfMt+8l8N9ezoyQp/2qsMbrWQeyo9aRnKtGc66Q7ZEx7LVwx8PuVuetLZE43wzqbWH4TQ0hOj2fouJi8nZGsXhTFpbjfXBVYvA8afh6UsFwb29U30URkVqIujiL6MgkSly8GWvB7VMf0eWTuDacxIx8igpzSIsOZe2+Pkz1K3/nk6F0g/UVA3G+XRiMUyGJMzxxC0oqe4elwbg1dzqg05TffypBixbNufL6fkvqyFyDQmVZ/XdrYY4pCpQWlqiMKDdq7wcMxEFHXnwIwfEaxgYF42SmrriuKbu8NBDnwk+Z7xfI8sQM8k4UU3AwlTVLYsm39WbSYEVZR23L6t9JqTAFM1VZw+AtYrh8rRFHpSMew0pIXB/F3hM6QEPBtigS823wcK7aC+4M6jOgUtXo2G+wPDZ0PGvIjg4jIiWLghOFZG+LZP6mozj5+7To66omj7Mx+6HB4711xrnFD/V5M1n3GMyA3t4cPJFk1PwOd0+gd7d/GDVv0Iz/4x5rGz5L/oJ3o9+nc+dOOPzjH2yICMfMrOlfVmJvZ8fyVWGcOPEbQwYNYuGCeRVP2vXufRf9H+zHspWrSIz/EICwlcvZuGkzi5csRXflCrZ97sdv8qSGNkHHjh3ZuGE90R/E8m70B5z64w+6de3KI8OdePGFxjWitmtX+32GJneYUHXUzZp/G8P673dj0asXL8//N+fOneOee2xYvXI5HTt2rHN+pVJJ1KZIXn/jTfwD/kUnMzO8Hvck4IUpjduwkQY8ZM+GiHWs3xDJR/Ef06tXL4Y7DUOr06FoRJfJE7/9xhWdjnvL30F4s732+nQGXddhbM2EoADy1kbyok8kKjs3/KcGMnZ+ZQNvX3dPbOKjmP1sFNprlSyFir6eQYTPcUE1fD4blkQR8X4YL27SoLCwxTUgjJkTWnTXlQYpPeax6lIUG6NDiC5UQydLbJ2nEx7oXX3orRI1GszL3i/QpBQ4BYXh/34U74XEU3DOlJ739WfswnVMc6lewdKWaMFUibLG4Wo1xBGb6ARmTwrltFaJ1QA3Xpn/ZIt7IbNofoaOZ93OSNbuVAPprAlIr7KkigmRScwdcIsy3gbY+wXjf2IDy5/3AUsHvPyDmHYuiIpuPb0dcLovgQ/DEhgbG8A9Df1uLdxZuh42bojh1RciOY2KvkPcmeRQ1sv/9qFmW4g3yyteZZzDAu8UUDiyNHk1Y5VgMzGUVWdCWRPsQwSWPDwhlEXlLRxyvBuvoTiKpqGwdsapaxRvzoih4Bz0tHZgQkhQ2Tu6mvDGlqmFAw+xmzVBYRRcUmHvFsKGmZXvn1E5uGAfvYHlke64LnRE6TifDSujiIheh1+MGpTW2Ls+ycy/GtxMAwzXN2+KEjUlKKnr1e49Rs9jRUkk65c8Q8R5JZYPuTFnxQv0VwA4MG31PN6LeJeXJy6jhLL0hatfYKC887Ialfs83iiJZE2ID2vPKbEc4M4rqwPK3okjmozEufmpPENYdSmSjZuCiT6hRWFhwyCvZbxS5b1GDZ0njbmeVNj5ET5Hw+JNL+CtVmLjMplVQS51vxusrVKosLE4Q0RYIGvOmWLVz5mp4YEVHVMMpmO4viJxLtNwnBRgQvV3BhuIW3OnF8QF4xOVX5GfghnexKHE6/UUXmnFQxo3dj80GId7U4iIzEJTCslLJpNcZTt9A2KJ8bduOM7WvoQtgTejw5m+oRhdJ0tsHSezYYZvrfc0tySGy9eacVQxdmEY2g2RrJk6jqKL0NNmGBOWhVW+3xCgVINGA6bX8T64ho9nJSpbFcc3huAXpkXR2wGvoHVMG9WyS6Gmj7Ph/WDod98a49xO36QvLmsbjqszyPg5Gl3pxTrTFXd0xuneAKx7DKoz/VbKP3qUyc+/wOeffFzxzkPR9n0Q+x/UZ84we9bMm77t7Oxs7O3tm2nthURPncdxn0iWjrr2sLuOotQwXlyhxj9hHRNup55yQgghhBCtSFF8IL5Jdmz4MBD7FnwTp7FKrly91Vm4LZh3aFUDFAkhhBBCCNGs/vzzTwAOHDhgcF554q8O96gcuaurPcdOf0fRuRxKtH8AYN7RAquudtzX6xEUJp1ucS4bJs25tw/dlSt89vkXvBm+9lZnpemVFlN0ogRNsRpNafk72M4VkL0vH+197tjLi8CEEEIIIYQQQgghhBBCiArS8FcPxR2d6XfnWPrdOfZWZ0WIBhX9VsRjnuP4+9133+qsND0TR6Yt8eHNqBC8Y0tAocDUTIWtoy/ha+U9P0IIIYQQQgghhBBCCCFEVTLUpxDihjTvUJ9CCCGEEEK0LDLU580hQ30KIYQQQghRqTFDfUpNWgghhBBCCCGEEEIIIYQQQog2QBr+hBBCCCGEEEIIIYQQQgghhGgDpOFPCCGEEEIIIYQQQgghhBBCiDZAGv6EEEIIIYQQQgghhBBCCCGEaAOk4U8IIYQQQgghhBBCCCGEEEKINkAa/oQQQgghhBBCCCGEEEIIIYRoA6ThTwghhBBCCCGEEEIIIYQQQog2QBr+hBBCCCGEEEIIIYQQQgghhGgDpOFPCCGEEEIIIYQQQgghhBBCiDZAGv6EEEIIIYQQQgghhBBCCCGEaAOk4U8IIYQQQgghhBBCCCGEEEKINkAa/oQQQgghhBBCCCGEEEIIIYRoA6ThTwghhBBCCCGEEEIIIYQQQog2QBr+hBBCCCGEEEIIIYQQQgghhGgDpOFPCCGEEEIIIYQQQgghhBBCiDZAGv6EEEIIIYQQQgghhBBCCCGEaAOk4U8IIYQQQgghhBBCCCGEEEKINkAa/oQQQgghhBBCCCGEEEIIIYRoA6ThTwghhBBCCCGEEEIIIYQQQog2QBr+hBBCCCGEEEIIIYQQQgghhGgDpOFPCCGEEEIIIYQQQgghhBBCiDZAGv6EEEIIIYQQQgghhBBCCCGEaAOk4U8IIYQQQgghhBBCCCGEEEKINkAa/oQQQgghhBBCCCGEEEIIIYRoA6ThTwghhBBCCCGEEEIIIYQQQog2QBr+hBBCCCGEEEIIIYQQQgghhGgD7rjVGWjJjpw6xv7ffuT3ktO0A+5U9mLgXf2x7XXPrc6aEEIIIYQQQgghhBBCCCGEENVIw18dCs/+xhvp75L3x8+10qL3JfLg3+7nZZcA7up65y3InRBCCCGEEEIIIYQQQgghhBC1yVCfNewt+IFZny2ts9Hvmh9//4n/S1rC/345eBNzJoQQQgghhBBCCCGEEEIIIUT9pOGvip/Vv7Jq50Yu/6U1OO+lK5dZ8U0kv5wtMnr9J0+eZGHIq4wd9ziu7h68PG8BeflHa833evibDB3uzP/2ZRq13kuXL7PxnSi8J/gwYuRofCZN5vMvvqw1X1r6boYOd2bjO1FGrffChQskf5nClBen8ujj42ulxydsYfxTTxu1rqp0V66wM20Xb2/azPq3Ivno4y2NXkdj/Ziby7qIt3AZPYZPP/u8VvqRvDyem/IiI1zdmPLiNI4XFFRL/yTpM57yeQaX0WOY8uJUDh7KrnM7Wq2WJyY8zVNPP9NgfiLe2sDQ4c61Po6PuHDlypWK+XIOH+bFaYGMGDmaJyY8zVdbt1WkXblyhXURb+E5/knGjnuc0OUruXDhQiOi0nSKShtKVZMY6IZ3eM7Nyk6boM6KZ3GgD26uzjh7TSZ4U0b1OJ/KYOO8ybi5uuHmE8SalEJ0TZwHXXEO2+PCmT7BDeeQ9Gpp2et96jyGh7qGsP1SE2dEtHqGjmd1VjyLp5anewewOD4Hza3Lbut1KYe4eZPL4ugTzl5DhUJuFD6ugcSduCm5azsuFZOZEsPyqd44e4eTWfMcWKpmb2QQ3h5uOHtOJjgqC3WV5MYc70VJQTgPdyN4W1OX8K2AgTiKZqZOZYGXJ34x+c2+qaL4QJx9IslusD5Zpt76x3Bnhjq30PJs7zLGjxzJ6Fqfx1hb5XKv5PBnrJ35DOPHjGG873TWfn6UkmuJukK+CZ/NpMfG8OijTzEtNIHDJXVtTGhyklge6FNWR54QyPKkfKlTNAOJc/MriAmoVc45h6RWXvMZU99o6HoSHQXbwgmY4Iazmzd+IfFk17MT23R9xNB1taF0g/UV4+PcpjW6Xmcobs2drqFgTwoblwTg5hpAdP3PhrRQGtKWeJfVjYqrTG7y/VCpznKiOJ2NCwPwdHPD2cOHgLAUCqr9gIyIs66YtMh5+Hi54ezqjc/CKPYW1zHfTWS4fK2htJjt4VXiHpneyPK4CY7XU1lEh5TvC8/JTA9PrbEvWp5GxxkNeYmhBHh74uzqiXdgGMk/6xqRbsTxbmQcW8p5s0U3/OXm5lJUZHzDWlUHDhzgzJkzRs+v1+tZlfY2utIrhmcud/kvLWFpm9CjNzivTqdjRtDLXL58mbCVy1n3+hp69epJ6PKV6PWVy+v1etJ3f0u/Bx4gbVd6A2ssX++VK/zfzGBSv/mGaf96kc2bInnG52k2vrOZX3+tfuW7a1d6+Xp3GfX9ioqL+Tp1R5M2Jv31119sfjea7MOHcR7xCE8/PYEB9vZNtv76fLNzFz8dO0Zpae0S4sKFCwS9PA/HoUP46D8f8JC9HUEvz0NX3gCX+EkSH8T8h/lzZvNh7PvY9e/P7LnzOHv2bK11RX8Qy4ULFw3m57nJk/kk4aNqn4cdBvL0hKfo0KEDAAUFhUyfGUz/Bx8kLvZ9pr4YwKeffV7xHaI/iOFQdg6x0e+yJT6OP06fJnLTOzcSpuu2IK7wlmy3zdKk82ZoEhrHYKI+TCJmvguapEWVcS4tJDoklO2dfNmQkEBUoC1564NYlt60VxCns1JIzijk9KXaJ6q+kyNJSkio8olkmoMSGy9vXM2aNBuitTN0POdHEbwwBTxCiElIYPPUPuRtCmFNEx/PtwN1agybDqnwj0wgfp0fgxS3Okdt1K8ZJKZmcVyjo66uYgXxISxIV+G/Npb4sCcxTQlhcVL51Wljjnd1KhHRR1HcpmVqg3EUzU9pzaDhjjjdp7rVOanG/vlIkhITSEpMICbQDpQuvJJQ9ndSQigTLW91DuswKIhNcR8RV/H5gBW+fTB/wIfxA8vnOZ7AwvlR/HL/C6yI2syKKbb8siuD4vJLl8PvLSbip/7MifmarVtW8eilj1myIaPJO321ej/HEzwniuN9Agh//13CA/pwfKehm0Si0STON4W2RIPp4EASqlxzxQe5cK161+B50lD9G9AdjGL+2iz6To0k/v0QXDWxLFidXrsBty3XRwxdVxtx3W2ovmJ0nNu4xtbrDMWtudMpLSTt01SyizXoWmHZpsmIJCJDh7LG9KbeDxXqKicuZbFmxmr29vJhVWwC8WG+9NwXTvCGnCodGAzFWcPetUG8utMUr4XvEv/ufLzIYlvuLewOaET5Wp2O7Mh5rDlkzczIBJLC/ej5fSjBmyrj0OzHqy6HiJfnkahxZM76WGJW+KI6kkrmqaYMTBNrdJxBsyeS2TEaXFfEs3tHPEsHFxKxKIq8UuPSDcbZ2Di2pPOmvgX77bff9J9//rm+pKSkUcsVFBTok5OT9Vqt1uhlMgr368e+89x1ffafOGxw/QcOHtQPcRqhP31aXW16zTzmHD6sdxk9Rv916g79o4+P15eWlja43s3vvqd3dffQq9VnGlyvTqfTjxrjof/2uz36ocOd9T8fP24wz9d8kvSZ3uMxr1rTP4r/WD/+qYlGr0ev1+u3bv9a/+proXqNpnH71BhXr141OI/HY176T5I+qzbtk6TP9OOfmlix/F9//aX3eMxLv/3rVL1er9dfvHhRf/L33yvmLy0t1bs/6qnfuv3rauv55Zdf9c6j3PWRG9/RPznRt1F533/goH6cl7f+4qVLFdMWvBKinz4zqGGvUWMAACAASURBVNp8VY8H/4B/6T9O/KTi732ZP+iffLpx220Khw4d0o/4v0/1p+ud47R+y0uj9U+8kX0Tc9X6af/8s+pf+j3Lx1XGOXuD/onRc/Wfn62c48jmf+pHvPSR/rdmyMvORaP1Ixbtaji/36/SP/HUKv2ePxucTdymGjye9X/qfyuomv6nftuicfrRr+7SG38WF3q9Xv/bBy/oR/xzs/7IX0Yu8ONm/dMjX9L/59dmzVabdfrTWfoRT7yh31c13n9l61c/NVof9GXlWfH0lln6Ec9c2y/GHu9/6vcsf1o/btFm/bKnR+uDtt5mvwaDcRRtyW8fvaQf8fQG/aFG7ts/P5+rHzF2lX7PLTgmNLrS6/+c3K5/+bF/6tfsv1Q+7bz+q4WP6h+Z+Yn+aJ3L/Kzf+Nxo/dQtpyqn/fcNvcfEt/QZl24gH63g0zh/6ncuGqcfMevTZqkPi2skzjfLvlVP6Ecv/77uRCPOkw3Xv7X6na+O049b+n1l/SOvvF5YVHVDbbw+Yui62lC6wf1gbJzbuEbX6wzFrbnTqzj2gX7yyBf07x27sRDcVNps/Zv/HKcP3PCG/oWq36nJ98M19ZcTpwuK9FVLotOfztKPeOqN2nW++uJ8bLP+6ZFP61f/0LLKnobL15ozf68PGfeEftl3ld9B+79V+nFjF+m3nS1bvrmP19OfztKPGLdUv/OsvlVpVJz1ev2hN57Wj1hU5bpau0s/f+wL+vcKjEk3HGfj4tj8583z58/rz58/r09PTzf4afIn/kpLS+t8qup6WFlZYW1tza5du/jzzz+NWqa4uJj9+/fj6OiIQmF8V/d9v9Y9dKNRy544ZHCeTp06AXAop/pwhzXzmLZrNwMHDGDw4EGcOXOW7BrzB86YxazZcyr+/vyLL3li/OP06NG9wfXu++EHrl7VM8xxKPfeew9pu3Yb/mJGuOOOO8jav59Jz/kzcvQYZs2ew/nzlftq8ZKlrFy9tuLv7zP+y5AhgzA373zD287af4DET5M4ePAQy1eGGT2EaU0HDx5i4D/+Qbt27QAwMTHhIXt7Dhws269mZmb8zcKiYv727dvTpUsXzp8/X209a8PX8di4R7nrLqtG52H9hrfxm/QsZh07AmW/o+/27OVxT89q87VvX/mT7d69O/syf6j4W60+Q9cuXRu97SZhZk7lEacmMzoEP6+yR6MDwlI43lCRoM4heX0IfhM8cXZ2w3NyCHE51R+1zksOY/qz3jg7Vx3qxJs1WeVzFKYS8XL58Haekwle3/IfWTdEoazeN0uroyLOuuJi1N1suKfKLH0HO6DKzyp/DF3H9oVuBMRkkbZpHj6ebji7+TA9Ogt1cQYbF07G080NN595xOU0RV/DYrZE78bK1w+nijxpyIwpOw6GunriPSOMxCbZlmiNGjqeQYmVddV0JSrzsiflhfEKogPw3ZSP9ucY/Jyd8YkqhFI1mYnhBE/2xrl8WMnlDQ4L3PDvtlpZ6xXAgmgZerGWE1lkq20YNKDyKSnVQ3ZYFueQrQZjj3fdwVje3G2Jf4AjylbYy/iGGYyjaCpF6ZEEP1tWB3N7dh5rUvLLyojSDJZ7uhGccu2phwyWe/kQkZ5RObyNdyARe9SoD8azOMAbN1c3vGdEsrfKPiqKD8R5RhTJ0aEE+Hjj5uaJd1AkaQaG5CzaE8UCf++KfEWkG/u0Z3kdKC6LtPVBeLu54RdTaLi+mXt91xHX4/DHsex/aDL/7F9ee9Zl8W2mjqFj3LizziXM6dEDCg8d5tp4NiV/qNEpzVGa3Jw8twq6LNL2aXHycKPxV2PCaBLnm0SHpkSDStWj7mQjzpMN1r9L88nO0WLr0L/yOv4+Bx5S5pNdpWxs6/URQ9fVBq+7De0HI+Pc5jW2Xmcobs2d3soVxEeSfIc3Mz1U1a/7mno/lGuonFBZW1Z76lBprgRd3aOm1Pldvsug2NIdrwEtayibhu9v1HCuGPVFFVa9K1MVAxx5mCwyj3ATjlcNe9JzUA53Z3i3Rn7RW6xRcQaUKiXkZ3Ho2i0MdTGn6YHK3Ih0g3E2Lo4t7bzZ5A1/qamppKamNtn6BgwYQPv27dm5cyfHjx+vd77S0lJ+/PFH9u7dS5cuXejVq1ejtlN0/uR15/G3878bnKfP/ffj6uLMwkWLCV2+kqM//VTnfGnp6Qwe/DDdunbF1rYPO9OqD/dZtWH1zJmznDr1B3YPPmhw+2lp6Tw8cCAmJiYMHTzY6OE+DTl16g82v/s+c2cHs3rVcn7MPcJHCR9Xy+/Vq2X51WhKOH/+PHf3vrtJtg1QVFTMoZwcZv7fdAKmPH9d6/j91Cksahwvf7Ow4OTvde/X4uKTnDjxG3b9K+OetiudvPyjTJv6YrWhW43x00/HyM/P51GPMRXTfisqQqfT8WC/B+pdbvpLUzl48BAzgmaz+9vviNgQid8/n23UtpvK4xMdy0/mOvKi5jM7rhj7qavZ/E4Yk3oXknmsgdvMR3azV9sf/5BI4j9ch79NDhErYioftd63gdkRR+k7M4odaSkkLHTBSuXI3PVhTOkHqFNZPCOMPconWbopls1LnkT5fRjTQ1Pbxg3pUg1528LZmNGjIs4KpTmKEjWnq4RVZ2KKovQMRVVGOM6OCSfNIoCo5B3EB9uSFx2C38IUlM9GkrI9gbn9jrJpXRIFN5rHnCQSTzgwYWzl+Fq6jEhejVHjuiKBHclRLHW3BExvdEuitavjeK49Tz57D2qxecC23gqdqM1mUjgb/G0xtfZlQ3IKUX7WoM4i7aApw6eFEvNhLG94m5O2Nowt9dw/b/B3q05nWVAUxwdMJyoxhaS1PihTQ1iWLEMvVqNWo8aSnlVHR1RZ0pNiiuoaSqWu4700n7j1KZg+G8yE3s2f5RapsXEU1+dcKhErU2FiOCnbk4gJHkZPE9P6y97SYhLXf4rCO4yU7QmsGqwm7lU/AmNK8FiRwI6k1Tip41nzYfWOi9qDSSRfcmZpbBI7vojEv1Mqr4bEUFDPxbBmXzjBYTnYPB9GUnICUVNtyFwd0qj36xQkhJFo4suGL74iapK1wfrmTXNpN59/rWP0eGfMr007WcgvOmv6PGBez0IqRge8gHXmMmbMf5ut335GxAdHGeo7jr/fpGy3CsWFFGht6NuvztqFaCoS55tEi+YiqNNXl3Xi9PIhYEk8mefKkxtznqyr/l2qpkhtSk9Vlf1oosJKBadPlV9F3wb1EUPX1Qavuw3tB2PifDtodP3YQNyaO701K05iTYIGryA/+tbsHNTU+wEaWU7oyNyXA33suMeojks6jv9cALZ9jJz/FjDm/oaZOQoTNafPVV3OFBRaTp/SNP/xWlpI3gmwsevTeu+vGBNnwMY7kAmdUpg9NYSN29KJC09AM9YXD5UR6Qb3gxFxbIHnzSZv+OvYsSMdy59aagrHjh3j6tWrDBw4kNzcXLZu3crhw4c5ceIEp06d4tdff2X//v2kpKRw+vRpRo8ezeXLl8nNzW3Udkr1V687j6VXjbtiXLFsKXNfDubAoUP887kpzAiaXe09fD/9dIwTJ35jyKBBADgOGcKu3d9WW8emyLd4681wADQlZU3U9fYCK3f16lW+/W4Pgwc/XLbeoUPIP/oTv13n+xOr6tSpE2+Gr+UfAx5iyODBuDiPIC8vryJ9eehrvLJgPgAXL5W9+07ZBE/7XaPVafEePx6l0rzi3XiNpdPpaj0h2aFDB7TauvugbNwcxcMOA+lf3uB66fJlwt9cz6wZ01Ga13fBXr8vv9rK8OFOdOnSpWKaRlMClL1n0T/gX7iMHsPzL/yr4ilEgG5du/F3679z9uw55i5YyN8sLHByGtbo7TeFWYPLC0bN90QnFmAbEMpcTwf63muL66RAJtrWf3pRDg9k1RxfXAdYY9Xbjgk+7lgVZ1WMkXw6/ygaG2e8HFUoTJTYeHgyqPQox7FFZQZ5STGkKTxZtNAbJ1tr+g72ZtF8TxTpMWzJvwlfvhkVJc3DzdUTvxUZWE1dVhnn/o44KdKJi8lCrdOhzkli2espFe+BuUb5SACLvG1RmoDVKGcGmWix9Qlhkp0STFS4ujrAsaPk3WCntuzUdDQObgyqcn7UnitBY6rCykKFUmmJvZcfE+xabTVDNIF6j+ea86VEkXx+GJM8rW9yDls5hRKlAjBRoOxW/n8Ld+aGBjJhuB02ltYM8vXFyTSfzIN1P33b0O+2ICmWvXYBLPVzxEalRGXrzkzfPvywM6NtdLJoKuU9WKtVKxSgQFfWQ7GGuo73oqRI4rTezPW9jX8DjYyjuE6X1JzWKuhpaYnSTInVYG/8PRo67hQ87BfCJAcVmKhwcnegp84arzkBOFkooJsDHoMtUecfrVYumFp6MmuqC1YKwMwarwBvbApS2VZnPU3N9pgd9PQLYZqLLapuKmxcAvAfXExauvHvlNZYeDJ3qiNWZgoUJobrmzeLLnM3/zVxZMRDVQ5uTQkalCgKE1gx7SkeHT2GiVMWE5dZGUWFqg8P3muJ6R87iAhZx7fdxjHRqWW9f/GW05SgwRxFQdkTqNee7IzeJ2epJiVxvkmUDPIMwH9qCFHJO9ixPpC+hVEsCE2hCIw+T9Zb/9bq0KKovnz539dGIbgt6iOGrqsNpRvaD0bE+bbQ2Hqdobg1d3qrpWZ7ZAxFw6fjX9cTck29H2hkOfFzPO/thLETnTGuBqNFowGVeQlpkfPw8XDD2cOHgLCUFjHCl7H3N1A64DpAx/aYeLLP6dAVZxEXuoG9GtBxM47XEjQlpihLc4heWD56j3cAi+NzWsW7Ro2OM0AnS+z726AyKSBxRQgRh/owaaJDZUNdQ+kG42w4ji3xvNnkDX8jR45k5MiRTba+s2fP4uTkhLW1NY8++ih33nknhYWFHD16lAMHDlBQUMCxY8cYMmQIzs7OdOvWjWHDhtUahtGQXp0bbjxrcFlz44qs9u3b8+QT49nyURxhK5ZRVFTMiy9NR1NS1sizc1c6PVUq7r//PgAchw7m5MmT5B45Uuf6Oncua0A7fbrhSvb+Awc5e+4cjkOHAPCPAQ+hUCjYVT7c56703YwZ91jFZ1e68cOAmpqaoqjS4Na9Wzc0JRfqnPdag/Cf5Y1aTaFz587Vhg1dvfYNZgQF86+XprNgYYhR6zA1Na09xJZOV2cD9jc709iZtovgmTMqpr0X/QF33XUXj44dU2t+gIi3NlSL79mzZ6ulZ/6Qhb2dXbVp7duXDTu65ZNPee3VEL5KTmLI4IeZ9++FXLhwgatXrxI0Zy6uLs7854P32BT5Fn9q/mThosVGfeemFhhd/lLawsPkaW0YPsTS0CLVaHJTiQ4PYbr/ZLyXpKAu1aApLzmtBjtiU7yb5CwNoKMoNZXM0j707Q2gIS+3GOUAB/pWKZwV/YZh36mY7COt4TRWPyvPEGJiowif744uJrAyzv/P3v2HRVXlDxx/K+2Q5qQ5xjpsBf0Qa4UswiRNTIFAKIrCoFwpktJwNeiHuiZmoa6oiWGwWiQFa0FSFPulcjEBW802ohRMwVRYCcy8ag5K3MT5/jEIw88ZFBTx83oenoeZc+feO59759xz7jn3HO1YXowJZ9B/ogn08iM8sQy3JwJx0vRDa972bN5rX6NFYwO2Zj2lNH01aKmljTZu69SVsqVQYairc9MhHMaHM9O1lEWhk4lankZuafc7Fmp+DP6+/nj5+uPlG8j8/G5QeuzB2jyfzZVnsmjNToaGReAj9zM7hXqomKzkWOZEhBM8OZatJ1XUmtbP9bZ/twZ27CrFkB9rGt6v/s8/vhhqai+KCsN5o9FgCzQpVqgqKhpsm0/u3dr5fiib+OQqfGaG4nIp95XoSBzF2dP782yYnq3zQgidm0jGlvaGAq5n3uO6jxaNjaZF2YKa2vaHcLpGjz1VlFW1srW6PezYZaAgIbghr/Hw8uPl/NqGTo/W0Do44disd3h75c3zZWfBdzBsGMPM980GqCvm489UPOe+yyefvc/cMQofLHidL34F6vawbu4ifrznVdau/ZB1CZGMqUll7oJPOfsxa3qg+jhmZKv4RqewcWM6r4xVWBe9kg3HLH5aWEvifN7Yjw9h0lgHtDagcRjLs1O9sS3MYeshrL5Otln+ttVgi9r08/WvNX00l055xFK92lK6peNgKc6Xio6W6yzFravTL1KGLUms3u7MtKltPBHV2cehI/mEWkry4hSq7opgmrv1T4zb2oCSm0nBNaEkZn1KdmI49oVxzE6+8L38rbq/AYCOgNnRBJFNVKAffpEpKF6h+NjVD316Hs5XWxsDWz/ajDZoKZ9u3EjaC8MoWxNN/Lbufw/M+jirFMTPIr56MkmpqXyavpKZruUsj4wlV7Ei3Yo4txvHbnrdvOxC74AlbvVPvwH06tULRVFwcXHh2msbh4vcsmULR44cYfBg06wIV111Fe7u7h3aznD7W9i4Z8tZ7eNw+7aHY2xN7969uWesB7fccjMPPDSRbV//F2/P8eTl53Pk6FE8fSYAcGbEyNy8zdxyc8ttDNLpGKTTUbzzBzzHj2tze7l5puFCn3xqasN7p06dIjc/n0mPhTDqLnfeT01pSNNqO/7Umrm2hrrsf+WVXKnV8r8DB7jVxbnVZc5VxDNTqTtVh9FopLeNde3agwcP5udDTbv7/nzoEIMH/7HJeyWle4hZ9Heej3q2oXEWIO2D9fTq1avhuP3++yl+//13PH0msGJZLFPCnmDSo482LN+/f+M8fDW//cbeffsY6jSkybb+aGfa9vORz6LXm87rp8On8M/30tjz416MRiM/H/yZR4MfAUyNucuWLCZkUih79+7jxhtvsOq7d5aydSl8HriUgDNvdOAx/LKMSKanqPhMDeelcFfsf0khdIrZcMFO/gQNzyY5LoLcGhWN3pmgxbMJsDNfS7NhqbrrMAAdpTHNBWXv4IRbPwW/mPo4DwDt8BCWpIY0LKpuiWWN1gHH8z1mt2EPJRVaHB2atdJoHAiKSWfcrnyyMtNZPjWVjKlxJIQ4necdbJvm7khS1jVe1TUDutHVuSdq53wGwFBI/LwEylxnkxLUsc4DonVqcRIRs3PQBobzZHQULnbFLAua1ebwem3/bvWABq3/QjbO7lj56pJjp0fHTtNwOWc6+ikKh9Ezzvy61cb5XpmTSe6xKmz/5se/6t+rrVEh1g+vTVFsXNp07t8ey9o4inOkxSV0JZm+xeRmZbJu8RSSXaN4K8a/a+fuOqliAHQ2rV13awEN42Z/yhLfzrsuWyxvnhfl/FBiQD+q2RBBOj16zRBGzpzMyMEA/bh9UjC3f7CU73aDp00mHysezL3P9GMYOOxB5sbUMjMslY93+zGtY1XRnkunx17jxKioUEbVZ6tuocG4pS2lYDf4yOWrc0icLxiNnR4dxRiqsf462Vb5W6vD3q6W/YcMcKaJoE6hUoFBdrpLqjxiqV7dbnqNheNg036cLxkdLddZiltXp1+kcj/KofIYLAr2YlH9e7WqStlkf3JDE0ka37nHwVI+kb7Uv/7JPoXc2GiSDf4sWe5t5dN+AFp0dlo0A8J5KaD+HvIN3jzpm07oN4XABb63ZOn+hjk7d6a95s60M68N2UQtduRWR815OF8Hohuo4Y5JkQS5mj5vPzqUgOHZrN9eBu7d5x5dq6yNsyGfjA3gkzjWdI7pXZkUE40yOZJ1OaGM8y9uP32ipTgb2o1j5d7ued3s9g1/5k6fPk3v3r2xt29aFR06dCjl5dYP/dKaMTeMIOnrdH79rWPdPnV9B3CXg6vF5Q4ePEitquJwXeNMDFdeeSW9evXCVqPhwIEK9u7bzysvRzPcxaVhmTcS/0FuXj4R055udb1+E3z5MPNjJj0azKBBgxq39/PP6AYO5LLLLiNv85dMfPghJj3aWFjJ27yZ11clcPjwYQYNGmRxuNDOMsLNjYKCbxk75m60Wut7eVir3xUdH0b0tuG3sjb53Ybz69SpU+zYUcQLz0c1LFNVdZDnXpiFv98EHgy4v8nn17//T8zbOnO++IIPMz9h9Rvx6HQD0Wg0DU9nNrdv335Onz7Nddc2nfdw4MCruN7RkR1FxQ0Nf6qqYjQaufxyW06ePEldnamBs1cv09OBgwcPplevXvxW+1uHY3CubOvqe0XoHbDnI0p2GcDhzPFtZ+LeulKy1hfjGJrOTP/6C9YvzZbZnk5ylTdL3glvOUY5WoY66TFsKqRIdW/oVaEWF1J0Uo/PkIt17gkDhhotWrNeV5q+msY4t7L8lpxt4B7Nree70bOinDL0uLXRTqO7ZSxht4zF54YIQjKyKZrohEt3aZi10aK7eMv1FxErzme1lHVzo8mwDSFhbkcqA6I9BVmZlLpGsjHc23ST2cq5rFr+bqNMeW3OVxSo7rhJG3nb9M646ZLYsV2B+g4RlYWFVF3jisuZykk757t9QCyZ480y+rpSkmfEUBn8Ni95X0IN4tbEUXQajZ0zPuHO+IzWExyRyYZyf8K6cF4KQ+E2SnAirLX7DDZODHWC5C1fYfAd2+YcHh1iTXnzfKirorwCBg5udpW7+nZuvyaJnTurmTi4vgOmqgK2puGGakCtqzXl4WfKUP11DERFrTl/u9/t2bnidm0SRcUG0DcOZ0iLYZvEOZE4nx91CspJHeZTDSm7iimzccT+aqCvpeukhfK3jRMuzrZkFRZiCKjPa0sL2XHSmYnOGuxHXKrlEUv16mbplsorFuJ8yehouc5S3Lo6/SLlOzcVN/P7RRWZRM0qZNxrMUy8QQ99DZ16HCzlE6YtGChaM5uXt+mZuWo6ozpYjh/q6gypxZTVuTeM5FCr1oLGtmMr6lQdvV/XkpKbT5HenWkOdP35auOI23AdG4r3oPqeGfbSNLyr7QWNoyUdjbNK7SnglNlbNjp0A85MlWYh3eJxaD+O3bUe3+lDfXal3r17M378eGxsml6BBw0axB133HFO6778Mltm3P14hz7Ti148OyYMjY3lueXeS/+Ax8PCeTd1HTt/+IGCbwv520vR2Ov1jHC7g015+Vw1YADenuPR6wc3/Pn7TeB/Bw6wb/9+AKZNn8Ffn21skAp7fDJ/tLNjytPPsHFTLiUlpfzr/7IJDZvChn/nsPOHH/jll194IOC+Juu9z9/P1CiY/2Vbu4zBYEBRjnDixAmMp40oyhEU5UiHYjRv/gIWLYlteH2vtyf9+/cnae077NpdwqFDv7B7dwlf//cbxnrey7eF3wFw9NgxfPzuZ+07Ka2+7ohTp0417LvxtJETJ06gKEeoqTHVlO/18uTUqVMk/GMNBw8eJD4hkb59+zJ2zN2mbR89yoyo57juuuuY9FgIFT/9REXFT1T89BNganAzj23//v25zMYGvX5wi7kDmzv0i+muw1VXtbz6TQl7nMQ1b7J9RxFHjhxlxeuruN7RgSE33cSwP/+Z/gP6s/S1FRw/fpyTJ0+yMv4Nrrv2WpyGDGmxrq5WO3wso+wAnTsBd9uSuyaGddvKqdxXSFZiLO8Vt3H1s7FF2xfKtm+j5JBCWWE28XGZlJjfnK6rRVVKKSqtQlEUlGMGDGarGxoYyriaj1gYm0nBvirKCjNZGPsR6t2hTLylK7911zF8vpSQJ6JJ3lRMWVUVZYXZLErMaYyzWkrG8jgytpVSWV5MbnIMy78ZwtRQVzqzmGo4pqAoVRjqgLpqKhUFxdD0WKqHqlDox6Bmp3BZWgxz1uRQoqiohnKKdiumnn7dpdFPnDcWz+e6KrJiZhNf4crM5/3RKVVUVpj+DN1/5IluTdunH7V7C9lSrlBZuo2M2EQ2KGZBtdVgiwFFMXV6au93OzQwlHFqNosWpFFQrmBQytmakULWvgv05S4g1aCgKAqGkyrUqRgUBUUxmIYcsXEmIMiJb5NXkrFLQdmVSXx6OXdM9Dd1XrF0vmt12Ov1Zn860xBfA/TY6y7eGxEdZimOolOo25OYsyCFreUGVNVAyc5SFFs99v0tf7YjaqtyWJucQ1F5OSVbUpi/Kh9b71AC6uvAtrYa03yDBgA9AaHeaPNXMicpn7JDBpSKYrKSMyk620Yua8qb58OvRziiahjYv/noKg5MCHFl59qVfLZbofrXcr5YncrXdl54DgNu98Kzz2bWrN7M/6oBtYov16bz9dUejJGn/cw4EPCYK0VJK8napWA4Vs6GxBS22nnj2zWDzVyiJM7ng/J5LKFTo0nOL6WyqoqSTUnMX1OI/oFgxmmxeJ20WP5Gw+jAQHT/SSI+pxylqpDkxEyqxwbiY8elUx6xVK+2lG6xvGIhzpcKi3EqJ2OGP16RmaY5LC3GravTAdVQf/+pmlpq6++NGFDPd9mhAzQ6fdPfrV0/bNGgtdObOhF09nGwmE+olKRFE5VmwCcyilF9lIZ6j3LmmRsLcdbePZkgslmYtI3KYwYqC9NYnX2EO8Z7nPf4nmE5f20eRwNFybHEZxdSVlFO0eeJzF6zh1FhwVbmE+d6vmpwCwnBfnMir+eUYzAolGQlklHqxLix3WcuuuY6HGetO753VZOxKomtFSpgoOzzJDJKHfH1cLKcbsVxaDeO3fS6eVE98dfV7r7ejUmuD7Cu8BOrln98xEPced1wq5aNnPFXrndw5OOsf/F28jtccUVfXG+7jYT4OPr06UNuXj6e48e1aNQceecIBvTvT25ePjdcfz29e/emd+/G9tq+ffvy5j/e4M2ktayMf4Njx47xpz/Z88zTT3Ofvx/xCYlc7+jIkJtuarJebb9+uI+8k015eQQ9HNjqPv9t3ny+Kfi24bVfwIMAfJm70WKD1hm9e/emV6/G/bW1tWXa0+Hk5W/mi025nD59mqvtrubG6x1bfHeby2yof5it1dfW2rV7N08/81dOnz4NwBuJq3kjcTWhf5nE9Gem0qdPH1YsX8riJUv5YH0GTk5OrFgWyx/q5y586+1kDhyoDFsfgQAAIABJREFU4MCBCgKDgpt8t6++zOv4Dpk5fvw4Go2mYVvmvL08MVRXM/+VGI4fP87ttw3ntaWx2NjYYGNjw6qVK3h9VQJBIY8B4Hrbbby+Ynmr6+pqcQsC64eE0uEzKw7DypUkz5vMGlsHRgWGMzOgitWtftKBoMhwSpYn8lRwIjpnL8KmRuAz26yB19kbn36RxM8IZlnDDR8Ng1xDeCUmHDc7b15dCa8npDBnShxqXwdcvGaTMO3ifWpI6zuLJTVJrE6OJrlcgb56nDymExdRH2eNDke7I8THRrDsmC32t3gwNS6i4QZap6jLZ1lQNBsaYh5LYD7gHEHmmpCGIcBqq2vBVou2WZZgf6c7jsnpPDcphsO1WuyHe/HS7Ie6dugw0S1ZOp/VTYks36QA+SwLzzf7pI6gxExetO4yK1rhEhpFWEUCi54IBr0rAWGRTDsWSe6ZBa5xZdSN6bwXm45PajjXt/e7tfPm1VWwOiGFl6ckchgdQ+/0ZpKrqZf/pUPh8+hAFn1z5nUxcwKzQePOq1lL8dGC48QYlhyJYVlUMPHouSMohnn1GbSc79ZrL46ic2gcPBjVP4nXZ6RQdgwGObgSFB2JzwCsfkLYGrZ2rtzKZpZFxlJWo8PFK5qEmY3zz+hcx+KSnMCiRG/GzXVH6z6bhL8nEZ+8ktAUBbQOuIx7iJmn2t1MO6wob54P1QrVaOnXyqwKAz1nsbg6kVULHiX+Vy36W714YfEUhmkAXJm2dBZr49/m+YkLqcaUPnfpFG6XOS+b0HnPYkV1Isuig1l+TIt+uDcvLQ3vVnOt9AQS566n849mSU0iq9dEkVxRi8bOEbeAhbwU5txQ6mrvOmmxPglonEOJe8HA/DVTCFS0OI6dzJLITnrS+mJhqV5tRb3bUnlF4mzSfpw0YEPTOYMtxK2r08vWRRGc1DiPXNmMQNahJeC1bF66iIc07uzj0K5D2cQnFmKog6wFk8kySxoankpKmIPlOGucmBY7m9XLEwgNrAKdE6Mei+XFgAt3t89y/to8jlp0Tjr2r44mNLYWzTWuBESuZNr4xih2+fl6TSBLFhhYtCoS/8UqWkdX7l8YQ9j5nSWqQzoeZx0+c2OpTUhk2VQ/Kk/CIMe7CFoYWz/KiKV0K873izCOvYxtTch2Ccvb+zWr/vMOJ9oYO6Wf7RVEjXmS0def21OGQvQERUVFuJgNT9u5VIpWTWHhqSiSolwbMlu1PJPZUxKwnf0pS7ylhimEEEII0R1VpkUQkulMwnsR3Weo705Q/fvpC70Ll4R+f7ioBigSQgghhBCiSx0/fhyA77//3uKy8sRfK+65cSRu1zizcc8WCn/ayc+GwwAM1l6N6zXOeA0ZzRUa6V4pRNerpfJAFdW2pqEAtFqgTqGksJAyW1fCLuLx14UQQgghhBBCCCGEEEKIziYNf23oZ3sFDzrfy4PO917oXRHiEqbFJyqa/atSmR6SSK2NBlubfuidxzJtVSg+MuKXEEIIIYQQQgghhBBCCNFAhvoUQpyTrh3qUwghhBBCiO5Fhvo8P2SoTyGEEEIIIRqdGerTGlKSFkIIIYQQQgghhBBCCCGEEKIHkIY/IYQQQgghhBBCCCGEEEIIIXoAafgTQgghhBBCCCGEEEIIIYQQogeQhj8hhBBCCCGEEEIIIYQQQgghegBp+BNCCCGEEEIIIYQQQgghhBCiB5CGPyGEEEIIIYQQQgghhBBCCCF6AGn4E0IIIYQQQgghhBBCCCGEEKIHkIY/IYQQQgghhBBCCCGEEEIIIXoAafgTQgghhBBCCCGEEEIIIYQQogeQhj8hhBBCCCGEEEIIIYQQQgghegBp+BNCCCGEEEIIIYQQQgghhBCiB5CGPyGEEEIIIYQQQgghhBBCCCF6AGn4E0IIIYQQQgghhBBCCCGEEKIHkIY/IYQQQgghhBBCCCGEEEIIIXoAafgTQgghhBBCCCGEEEIIIYQQogeQhj8hhBBCCCGEEEIIIYQQQgghegBp+BNCCCGEEEIIIYQQQgghhBCiB5CGPyGEEEIIIYQQQgghhBBCCCF6AGn4E0IIIYQQQgghhBBCCCGEEKIHkIY/IYQQQgghhBBCCCGEEEIIIXoAafgTQgghhBBCCCGEEEIIIYQQogeQhj8hhBBCCCGEEEIIIYQQQgghegBp+BNCCCGEEEIIIYQQQgghhBCiB5CGPyGEEEIIIYQQQgghhBBCCCF6AGn4E0IIIYQQQgghhBBCCCGEEKIHkIY/IYQQQgghhBBCCCGEEEIIIXqAyy70DnRnFXuOsnfHYY79chJ6wVV2fbnx1qv5040DLvSuCSGEEEIIIYQQQgghhBBCCNGENPy14tABA5mJ3/PTj8dapG18bzfXDb2KwOm3o9NfcQH2TgghhBBCCCGEEEIIIYQQQoiWZKjPZnb99yBv/u3LVhv9zvhfyVFWz95MaeHP53HPhBBCCCGEEEIIIYQQQgghhGibNPyZOVh2nPUrv0WtrbO4bG3NKT5Y8S2/VBisX//Bg8yNfhkfv/sZ5+3L87PmUFK6p8Vyr8W9zsjRHvz3mwKr1lvz22+sfjOJwKBgxtzjSfCkyXzyr/9rsVxu/mZGjvZg9ZtJVq33xIkTZP1fNk8+NZUJ9z/QIj0tfT0PPPyIVesyp/7+O5ty8/jHmrdY9UYi73+wvsPr6Kgfdu1iZfwbjPW8l48+/qRF+u6SEh5/8inGjPPiyaemsb+srEn6h5kf83Dwo4z1vJcnn5rK9h1FrW6ntraWB4Me4eFHHm13f+LfSGDkaI8Wf+53j+X3339vWK54506emhbBmHs8eTDoET797POGtN9//52V8W/g/8BD+PjdT8yiv3PixIkORKXzVLb7k1HIiPAiMK74fO1Oj1CWEt7i/PCIzkE1W8ZQnMmiiGC8xnnhFRTBosxSrM+ROqCmkGXBHox8LIkS82N9rJDkuZNN2w+OJH5TVVdsXfQASmEa8yOC8RrngUfAZKLWbGuSbyiFacyfWp8eGM78tOKuOZd7uppi1s2abIpjcBxbVQvL70oieFwE6yrOy971HDVVFGSnsGhqIB6BcRQ0vwbWKWxNjCTQ1wsP/8lEJRWimCV35HyvzIzEY7QXUZ9bOpg9kIU4ii6m5DAnwJ/QlNIu31RlWgQewYkUWa6CUbQquNUy9MjRHoz06Kb52daFPHDPPXi2+LuP5WbVveqdH7N85qM8cO+9PBAyneWf7KH6TKJazhdxzzHpvnuZMOFhpsWks7O6tY2J81Y+vpSp5WxYXp8/ewUSuiCNIglyp7NYH7SmvNFO+RtUyj6PIzyo/jhGt30ce3R55NA2Vs9qrNMuyy5vUue2mG6xvGJ9nHu0DpfrLMWtq9MNlG3JZvWCcLzGhZO87xy//3lnIHdBoKlsZH6bxlI9poVzjFNVPqvnhuPv5YWHbzDhsdmUNc9GDhWSHF2/jP9kpsflNC5TU86GVbMI9vdi5Dh/gp9PJLcb3HaynL82Z6AkI4bwQH88xvkTGBFL1j7V6nS1altDjEZ6BRIanULBmR+QIZsoj9bLxsFr6svxxYkENk8PsOb4X1idGmdr4mSm1eteXRUb4szyscT8ZvvT/fKNbt3wt2vXLiorK8/qs99//z1Hjhyxenmj0cj61ws59ftpqz+j1taREf8dRqMVy6oqMyKf57fffiP274tY+doyrr56EDGL/o7RbAVGo5H8zV9yy803k5uXb3m9v//OX2dGkfPFF0x7+ineWpPIo8GPsPrNtzhwoGnNNy8vv369eVZ9v8qqKv6ds7FTG5NOnTrFW28nU7RzJx5j7uaRR4IY7uLSaetvyxeb8vhx717q6lrmECdOnCDy+Vm4j7yT9//5Lre6OBP5/CzU+ga4jA8zeTfln8x+4TneS30H52HDeO7FWRw9erTFupLfTeXEiZMW9+fxyZP5MP39Jn93uN7OI0EP84c//AGAsrJyps+MYtif/8y61HeY+lQ4H338ScN3SH43hR1FxaQmv836tHX8cvgwiWvePJcwnbU568ovyHZ7stpqA7YjIkhPTyez/i8tciyaMwvsSyPqhST2Dwkn7p23iQsfwv5Nli6CZ0OlZF0iG37VNntfIWtxNBm1/ix5J5WkMD1bFkeT3PX3B8XFxpDP6zGZGNyjSHovk5TZYzFkzmvMN0qTiJqbDb7RpKSn89bUIZSsiWZZ/qVYIz43Sk4Ka3boCEtMJ21lKG4ay58RZ+HANjJyCtlvUKltJbksLZo5+TrClqeSFvsQttnRzM+sr6F25HxXcohP3oOmT5d+m26r3TiKrqd1wG20O6Nu1F3oPWnC5YlEMjPSycxIJyXCGbRjeSnd9DozPYaJ+gu9h61wi2TNuvdZ1/D3LotDhtDv5mAeuL1+mf3pzJ2dxP9umsLipLdY/KQT/8vbRlV9uW7n2vnE/ziMF1L+zWfrlzCh5gMWJGyjB96CPzfnrXx8aStKmseyPc68tG4jmzNjCTiZzpxVcj52Nkv1wXavk5bK34C6PYnZywsZOjWRtHeiGWdIZc7S/JYN5T25PFJXTnJ0DBv6hpCQnk5ShBMlqyJZeKZcZikdy+UVq+Pcw3W0XGcpbl2dTl05uR/lUFRlQL0IryGGbYnEb1NpfhfHUj2muXOKU00hy2YsZevVwSxJTSctNoRB38QRlVDceL1Qi4l/fhYZBndeWJVKyuIQdLtzKDgEYGDD4kjiS52YFpfKp+/E4FuXw8vRKZRdyGNiRf7a4iNbEnkuxcC4xWls3pjGqyPKiZ/X2LG+/XSFz5fH8rmNP6+8nU5mYgQuZSnMWZxtajzXejPvvcbrRGZ6OukL/LEf4MpEfyfTBqoNGAaM5SXz5daEc6tNF8fqXHR2nK2J0xmtXvdUihJnsWyHAzMT08mMC2XQVzFErTE7n7tjvmHsxn766SfjJ598Yqyuru7Q58rKyoxZWVnG2tpaqz+zu+CgMToo66z+9u74xeL6v9++3XjnqDHGw4eVJu8338finTuNYz3vNf47Z6Nxwv0PGOvq6tpd71tvrzWO8/Y1KsqRdterqqpx/L2+xi//s8U4crSHcd/+/Rb3+YwPMz82+t4X0OL999M+MD7w8ESr12M0Go2fbfi38eVXYowGQ8eOqTVOnz5tcRnf+wKMH2Z+3OS9DzM/Nj7w8MSGz586dcroe1+AccO/c4xGo9F48uRJ48Gff25Yvq6uzug9wd/42YZ/N1nP//53wOgx3tuYuPpN40MTQzq07999v93oFxBoPFlT0/DenJeijdNnRjZZzvx8CAt/2vhBxocNr78p+Nb40CMd225n2LFjh3HMXz8yHm5zicPG9c94Gh9cUXQe9+ri982SB42ei75qI/W4cdM8P+OYZz8y/tTVO1L2vnGKzxTj6wkvGsc8+pZx96n69/e+a5x8zzPGfx5o3KfP5/kZ/ZZ8ZbQ+5xWXitrjx81fGbcs8jPLN44bfyozTzedS54v58m51EE/vTvFOOYvZr9TS354y/hIk9+x6IjDHz1rHPPgCuM35vE+VWRc+rCnMfL/Gq+Kh9c/a5Z/Wnu+HzduWfSI0W/eW8aFj3gaIz+7xH4NFuMoepKf3n/GOOaRBOOODh7b45+8aBzjs8S45QKcEwa17uz/Dm4wPn/fX4zLvqupf+9X46dzJxjvnvmhcU+rn9lnXP24p3Hq+kON7329wug78Q3jtppz2I+L4K9jzmP5+JJWZlz7hKcx4iOz2t+3K4x+Z/EbFu1rtz5oxXWy/fJ3rXHTy35Gv1fN6m4l9eXCSvMN9fDySFGC8UHPF42fHG18a/dbfzGOeeZ9Uz5iKd3icbA2zj1ch8t1luLW1elm9r5rnHzPFOPavecWgvOqtsj4+l/8jBEJK4xT2jjXWq3HtFzROcfpcFml0TwnOvzRs8YxD69ouF4c/uhZ4xi/V42bjhpbVVtZZvzJPNvZ+5bxkXumGNeWtbffXa/9/LWlHSseMY6ZZ1bfq80zzvZp/B6W0o3HjzepKx7/7EXjmHvmGTe1miVXGtf/1c845f3GINX+e95FWY/q7Dg31TJOJm1c92q/Mkb7PWhc+J/GoNf+d4nRz2ee8fPm528X5xu//vqr1X+d/sRfXV1dq09VnQ17e3scHBzIy8vj+PHjVn2mqqqK7777Dnd3dzQa67u67/nu0NnuplWf7du3LwA7ipsOd9h8H3PzNnP78OGMGOHGkSNHKWq2fMSMZ3n2uRcaXn/yr//jwQfuZ+DAq9pd7zfffsvp00buch/JDTdcT27eZstfzAqXXXYZhd99x6THw7jH816efe4Ffv218VjNX/Aqf1+6vOH1V9u+5s473ejX74pz3nbhd9+T8VEm27fvYNHfY60ewrS57dt3cPttt9GrVy8AbGxsuNXFhe+37wCgT58+/NHOrmH53r17c+WVV/Lrr782Wc/yuJXc5zeBP/3JvsP7sCrhH4ROeow+l18OmH5H/9mylfv9/Zss17t340/2qquu4puCbxteK8oR+l/Zv8Pb7hR9+jU+iYZCQXI0oQGmoQDCY7PZ316WoBSTtSqa0CB/PDy88J8czbpi8/6iKiVZsUx/LBCPJo9lB7KssH6J8hzin68f3s5/MlGrcloOH3BRUTFUG9DpBraRXEjuN7WM8vWi9bNNZcNcL8JTCsldYxoWwcMrmOnJhShV21g9dzL+Xl54Bc9iXXF7fQ0VsuJTUbwiCLqmaYqhuJgyO2dubehZr8VtuAOG4uL6422gIMV0Howc50/gjFgy2t2W6Mk02qZ9DWtVzPINLfYO5uladP1MT8oL65UlhxOyppTafSmEengQnFQOdQoFGXFETQ7Eo35YyUXNhydqov3fbZO8NiCcOcky9GILFYUUKY64DW98Skp3qzP6qmKKTF0yrTrf1e2pvL5ZT1i4O9ru0lvwfLIYR9FZKvMTiXrMVAbzemwWy7JLTXlE3TYW+XsRlX3mqYdtLAoIJj6/caghj8AI4rcoKNvTmB8eiNc4LwJnJLLV7BhVpkXgMSOJrOQYwoMD8fLyJzAykVwLQ3JWbkliTlhgw37F51v7tGd9GWhdIbmrIgn08iI0pdxyeXPX2dUjzsbOD1L57tbJ/GVYfelZLeTLApWR93oxuNVP9GPgQCjfsZMz49lU/6Kgavuh7c49tc83i+Vj0Tn6oRsIZduLG8oAhkNHqJXzsZNZqA9acZ1st/xdV0pRcS1OrsMa6/E3unKrtpQis7yxp5dH1KoqlAGOXG8WqqEjXNGVFlJksJxu8ThYGecer6PlOktx6+r0i1xZWiJZlwUy01d3bk9id0KcdA76Jk8davtpQT3ztKGBLfnFaEd7M3pA65/X6B2wN7+93acfWmqpteZxxS7U/v2NlrQ6LZQWsuNM1Vqp4jAD0fWzLh2ttsm6VVWFvv2wbeW6q25JIbnKi2kPOTS8Z6g2wICL7zrd2XE211qcoJ3r3rEqlJM67K9p3LpmuDt3UEjB7g5/tfOm0xv+cnJyyMnJ6bT1DR8+nN69e7Np0yb279/f5nJ1dXX88MMPbN26lSuvvJKrr766Q9tRqs5+OEtrPjvkppsYN9aDufPmE7Po7+z58cdWl8vNz2fEiDsY0L8/Tk5D2JTbdLhP84bVI0eOcujQLzj/+c8Wt5+bm88dt9+OjY0NI0eMsHq4T0sOHfqFt95+hxefi2LpkkX8sGs376d/0GR/T5827a/BUM2vv/7Ktddc2ynbBqisrGJHcTEz/zqd8CefOKt1/HzoEHbNzpc/2tlx8OefW12+quogFRU/4TysMe65efmUlO5h2tSnmgzdao0ff9xLaWkpE3zvbXjvp8pKVFXlz7fc3Obnpj8zle3bdzAj8jk2f/kf4hMSCf3LYx3adme5f6J7/cVcpSRpNs+tq8Jl6lLeejOWSdeUU7C3ndvMuzeztXYYYdGJpL23kjDHYuIXpzQ88q5+k8Bz8XsYOjOJjbnZpM8di73OnRdXxfLkLYCSw/wZsWzRPsSra1J5a8FDaL+KZXpMzkV8Q7oWw0lQ8peaGu0CgglfkEbBsfrkqnLKah0ZekuLgRuaKEqJI9cunKSsjaRFOVGSHE3o3Gy0jyWSvSGdF2/Zw5qVmZS18XnDpkTW7HNnZrhriyEiKg9VgW4gOrOCg/ZqPfyioADqtkReTlEYtzidjVlJvOqtB2zPKhqiB6kzUPJ5HKu3DTTLN5ovU8rW7bU43uzUZoFOtOQ4KY6EMCdsHUJIyMomKdQBlEJyt9syeloMKe+lsiKwH7nLY1nfxv3zdn+3Sj4LI5PYP3w6SRnZZC4PRpsTzcIsGXqxCUVBQc8g89ERdXoGUUVla/3EWjvf60pZtyob28eiWnS6uGR0NI7i7BzLIf7vOTAxjuwNmaRE3cUgG9u28966KjJWfYQmMJbsDeksGaGw7uVQIlKq8V2czsbMpYxS0lj2XtOOi7XbM8mq8eDV1Ew2/iuRsL7tD9Nk+CaOqNhiHJ+IJTMrnaSpjhQsje7QPBll6bFk2ISQ8K9PSZrkYLG8ed7UbOaTf6t4PuBBwz2Ig+X8T3VgyM2t3JUAQIdn+BQcChYyY/Y/+OzLj4l/dw8jQ/y47jzt9kXByvKxOFc6fKeG4/hNDOHPJ5KVn8my5FJGPeaP44XetR7FQn2wI9fJ1srfdQqVii2DdGa/Fxsd9jo4fKi+Fn0JlEc02n5oqhUOm92uUG1s0dQdofKI5XSLx8GaOF8KOlw+thC3rk6/mFVlsizdQEBkKEPPtZGn0+OkUvBNMQxx5noboK6ckgpwdB5idb3fUFhIWV8nhnbebeVzY839DcAxMIKgvtk8NzWa1Z/nsy4uHYNPCL4669IbqSjbM1m2bg+OQYG4tTjGCp9/tBmd70NNpv0wGKqprchmTpA/Hr6BBM+IJWPXRdQxv5Pi3Kj1OLV73evTD42NwuFj5svbgqaWw4e6byw7veHv8ssv5/L6p5Y6w969ezl9+jS33347u3bt4rPPPmPnzp1UVFRw6NAhDhw4wHfffUd2djaHDx/G09OT3377jV27dnVoO6dPd6yxpulnrZsXcPHCV3nx+Si+37GDvzz+JDMin2syD9+PP+6louIn7nRzA8D9zjvJ2/xlk3WsSXyDN16PA+pb7KHtXmBm+/flf7YwYsQdpvWOvJPSPT/y01nOn2iub9++vB63nNuG38qdI0Yw1mMMJSUlDemLYl7hpTmzAThZY5r7TtsJT/udUavWEvjAA2i1/RrmxusoVVVbPCH5hz/8gdo2upCsfiuJO1xvZ1h9g2vNb78R9/oqnp0xHW2/tirsbfu/Tz9j9OhRXHnllQ3vGQzVgGmexbDwpxnreS9PTHm64SlEgAH9B3Cdw3UcPXqMF+fM5Y92dowadVeHt98Znh1Rn+0aviI5owyn8Bhe9Hdl6A1OjJsUwUSnti/j2tERLHkhhHHDHbC/xpmgYG/sqwrrx/SGw6V7MDh6EOCuQ2OjxdHXH7e6PezHCV0fKMlMIVfjz7y5gYxycmDoiEDmzfZHk5/C+ot2vjktbv7hhE2NJilrIxtXRTC0PIk5MdlUAhiqMdAPTZmph/2ZnuvJ3zQtfGnvDmdeoBNaG7Af74GbTS1OwdFMctaCjY5x41xh7x5KWmuXNRQSn7gNl/AIxrXW+0pVQWPbpClPcxloTlVTC9Qeq8Zgq8PeTodWq8clIJQgZ2nGuZRVZs7Ca5w/oYu3YT91YWO+0Xy57CSyfr2LSf4OraaLNmi0aDWAjQbtgPr/7bx5MSaCoNHOOOodcAsJYZRtKQXbWy+Ytve7LctMZatzOK+GuuOo06Jz8mZmyBC+3bTtIu5k0QXqe7A2KVZoQINq6qHYTGvne2VmIutqA3kx5BL+DXQwjuIs1SgcrtUwSK9H20eL/YhAwnzbO+803BEazSRXHdjoGOXtyiDVgYAXwhllp4EBrviO0KOU7mmSL9jq/Xl26lhTj+0+DgSEB+JYlsPnrZbTFDakbGRQaDTTxjqhG6DDcWw4YSOqyM23fk5pg50/L051x76PBo2N5fLm+aIWbOZrG3fG3Gp+N6YaA1o05eksnvYwEzzvZeKT81lX0BhFjW4If75Bj+0vG4mPXsmXA/yYOKp7zb94wVlZPhbnTqNzwuUGPZpDOSyfG0feAH8m3S3nY+eyUB+08jrZZvm7VqUWTdPP178+MwrBJVEeGebOKE0+61IKUVQVpTiTha9lN8yvajHd0nGwIs6XhI6W6yzFravTL1oKGxJTqBw9nbDhnXDvpbPjtC+NtZvAZ6IHpitGNYZqW7R1xSTPrR9VJjCc+WnFrc+BWVNM8rpCdP7BjOsGc45ae38DgL56XIY5orMpI2NxNPE7hjBpomtjg6eldIDSFEK9vPCLSKDEOZIloa10lFa2sWH7QEaNb5pv27sHMi10OktSs9mcFsskXSHxs1eSe6z5CrqfTo3zGW3Eqd3rntaVccNVNqSkUXRMRa0qZF1MAlsNoHbjWY47veHvnnvu4Z577um09R09epRRo0bh4ODAhAkTGDx4MOXl5ezZs4fvv/+esrIy9u7dy5133omHhwcDBgzgrrvuajEMoyX9dWffWHmlzrocp3fv3jz04AOsf38dsYsXUllZxVPPTMdQbWrk2ZSXzyCdjptuuhEA95EjOHjwILt2t/7M6BVXmBrQDh9uvzLz3ffbOXrsGO4j7wTgtuG3otFoyKsf7jMvfzP3+t3X8JeXb/0woLa2tmjMGtyuGjAAQ3XrT0CeaRA+Xt+o1RmuuOKKJsOGLl2+ghmRUTz9zHTmzI22ah22trYth9hS1VYbsL/YlMum3DyiZs5oeG9t8rv86U9/YoLPvS2WB4h/I6FJfI8ePdokveDbQlycnZu817u3adjR9R9+xCsvR/NpViZ3jriDWX+by4kTJzh9+jSRL7zIuLEe/PPdtaxJfIPjhuPMnTffqu/c2SKS6yczLd9JSa0SzCYUAAAX8UlEQVQjo+/UW/pIE4ZdOSTHRTM9bDKBC7JR6gwY6q/09iPccazaTFahAVCpzMmhoG4IQ68BMFCyqwrtcFeGmuXkmlvuwqVvFUW7u2+vC0vsx4cwaawDWhvQOIzl2ane2BbmsPUQYAPUFZORreIbncLGjem8MlZhXfRKNphfuM177Wu0aGxoMhSApq+mjWESVIpS48jVhzPTv/VKvK1GA2ptkwmh1VoV1bYftoB2fDgzXUtZFDqZqOVp5JZ2v2Oh5sfg7+uPl68/Xr6BzM/vvhfrnsDeP5qU1CTiZnujpkQ05hvmyjNZtGYnQ8Mi8JH7R51CPVRMVnIscyLCCZ4cy9aTKmpN6+d6279bAzt2lWLIjzUN71f/5x9fDDW1rVfMLlUaDbaY+kY0UFVUNNg2Ly62dr4fyiY+uQqfmaG4XMp9JToSR3H29P48G6Zn67wQQucmkrGlvaGA65n3LO6jRWOjaVG2oKZp+aCFa/TYU0VZVStbq9vDjl0GChKCG/IaDy8/Xs6vbej0aA2tgxOOzXpBt1fePF92FnwHw4YxzHzf6st1H3+m4jn3XT757H3mjlH4YMHrfPErULeHdXMX8eM9r7J27YesS4hkTE0qcxd8ysHzu/vdm7XlY3Fu6kpJnh1DyfiFpKdmkrkmintOpvDcvPoGKdFp2q0PWnmdbLP8bavBFrXp5+tfa/poLp3yiHYsL8aEM+g/0QR6+RGeWIbbE4E4afqh7WdFuqXjYCnOl4qOlussxa2r0y9Shi1JrN7uzLSpbT8R1SGdGSe1lOTFKVTdFcE098a9s7UxsPWjzWiDlvLpxo2kvTCMsjXRxG9rXkY0UJAQS0adPy9Odjrbb9SprLq/AYBKQfws4qsnk5SayqfpK5npWs7yyFhyFWvS690YQlxqKklLpzOqIpGnZmW2GD3DULiNEq0rbs3arTRO/oQFuWPfBxjgREBkOHeczGfDN93/HljnxblRq3GyeN3TETA7miCyiQr0wy8yBcUrFB+7+iFsu6nLLvQOWOJW//QbQK9evVAUBRcXF669tvG53i1btnDkyBEGDzbNinDVVVfh7u7eoe1c7zyI7/MtTDbRhhucB3Vo+d69e3PPWA9uueVmHnhoItu+/i/enuPJy8/nyNGjePpMAODMiJG5eZu55eaWQz4O0ukYpNNRvPMHPMePa3N7uXmm4UKffGpqw3unTp0iNz+fSY+FMOoud95PTWlI02o7/tSaubaGuux/5ZVcqdXyvwMHuNXFudVlzlXEM1OpO1WH0Wikt4117dqDBw/m50NNu/v+fOgQgwf/scl7JaV7iFn0d56PerahcRYg7YP19OrVq+G4/f77KX7//Xc8fSawYlksU8KeYNKjjzYs379/4zx8Nb/9xt59+xjqNKTJtv5oZ9r285HPotebzuunw6fwz/fS2PPjXoxGIz8f/JlHgx8BTI25y5YsJmRSKHv37uPGG2+w6rt3lrJ1KXweuJSAM290YEiBsoxIpqeo+EwN56VwV+x/SSF0itlwwU7+BA3PJjkugtwaFY3emaDFswmwM19Ls2GpLrJxq62hsdOjoxhDNaDTY69xYlRUKKPq21jdQoNxS1tKwW7w6Vj211JdIeuzyjHUJhDilVD/nkqtuo2n/PMJik0lTK+HQ0dQ6sC+Pt6HjyhgN8T02saBoJh0xu3KJyszneVTU8mYGkdCSPcopAFo7o4kZV1jkUEz4OIt5F8UNKa5zewdnHDrp+AXU59vnHmi1FBI/LwEylxnkxLUsc4DonVqcRIRs3PQBobzZHQULnbFLAua1ebwemja+t3qAQ1a/4VsnH2uGUwPZ6dHx07TsEVnKhKKwmH0jDO/brVxvlfmZJJ7rArbv/nxr/r3amtUiPXDa1MUG5c2nfu3x7I2juIcaXEJXUmmbzG5WZmsWzyFZNco3orx79o50k6qGACdTWvX3VpAw7jZn7LEt/OuyxbLm+dFOT+UGNCPajaklU6PXjOEkTMnM3IwQD9unxTM7R8s5bvd4GmTyceKB3PvM/0YBg57kLkxtcwMS+Xj3X5Ma3tmgEtLV5ePhUlhJhmKB68EmM5HnXMgr/69lvDJKWTs8mfmLRd4/3qwJvVBa6+TbZW/tTrs7WrZf8gAZ5oI6hQqFRhkp7ukyiPa4SEsSQ1peK1uiWWN1gHHAVak11g4Djbtx/mS0dFynaW4dXX6RSr3oxwqj8GiYC8W1b9Xq6qUTfYnNzTRNBVER3RanBRyY6NJNvizZLk3jZ8ciG6ghjsmRRLkalq//ehQAoZns357Gbg33juqzIrh5c9teWzldNy6SxuLpfsbZxjyydgAPoljTd9d78qkmGiUyZGsywllnH9x++kh9YUaGw06vQM6vQMuN2soC0pk3TZ/XhrdWKos212K6hBiGkq1PVod9looqzYA3fyc76w4hzTWuVuLk6XrXvpSf3R27kx7zZ1pDdvMJmqxI7c6dt97id2+4c/c6dOn6d27N/b2TauiQ4cOpbzc+qFfWjPsLnv+nfoDJ453rLVbe9Xl3Dyi9WnYzR08eJBaVcXhusaZGK688kp69eqFrUbDgQMV7N23n1dejma4i0vDMm8k/oPcvHwipj3d6nr9JvjyYebHTHo0mEGDGhsgD/78M7qBA7nsssvI2/wlEx9+iEmPNhZW8jZv5vVVCRw+fJhBgwZZHC60s4xwc6Og4FvGjrkbrbbzc+t+V3R8GNHbht/K2uR3G86vU6dOsWNHES88H9WwTFXVQZ57YRb+fhN4MOD+Jp9f//4/MW/rzPniCz7M/ITVb8Sj0w1Eo9E0PJ3Z3L59+zl9+jTXXdt0gOqBA6/iekdHdhQVNzT8qaqK0Wjk8sttOXnyJHV1pgbOXr1MTwcOHjyYXr168Vvtbx2OwbmyravvBaR3wJ6PKNllAIczx1dtu9d3XSlZ64txDE1vfLLsl2bLbE8nucqbJe+EtzJGuZahTnoMmwopUt0bemWoxYUUndTjM6S7lAg6qE5BOanDfCh1ZVcxZTaO2F8NaF1xuzaJomID6BuHa6HFMAxnycaVF1PSmWbWOGDYFMNTWUNY8looLnagHeCMo7KZggpwcQAwUFRYjnZYeJP5PXS3jCXslrH43BBBSEY2RROdcOkuDbM2WnTdvIzTMxgw1GjRmvXm1PTVNOYbAGop6+ZGk2EbQsJc7+5e9LxoFGRlUuoaycZwb9NNZivnsmr5u40y5bU5X1GgujcdB180pXfGTZfEju0KOJjO5MrCQqquccXlTOWknfPdPiCWzPFmZdG6UpJnxFAZ/DYveV9CDeLWxFF0Go2dMz7hzviM1hMckcmGcn/CunA+J0PhNkpwIqy1vkA2Tgx1guQtX2HwHds5PdatKW+eD3VVlFfAwMHNrnJX387t1ySxc2c1EwfXd8BUVcDWVK6rAbWu1pSHnylD9dcxEBW15vztfrdn18XlY2GiQm2d2uJ8HIRKrZyPncdSfbCvpeukhfK3jRMuzrZkFRZiCKjPa0sL2XHSmYnOGuxHXKrlEQNbcraBezS3tlpnbZZuqbxiIc6XjI6W6yzFravTL1K+c1NxM7+lXZFJ1KxCxr0Ww8QbzuJ32ylxMlC0ZjYvb9Mzc9V0RpkfbxtH3Ibr2FC8B9X3zHCMpuFfbTWNk8ooW2KZHrcHl9mJTOsWx8eK+xtNqNSeAk6ZvWWjQzfgzBReltL1GAwqWq3Zd++rQWtjmgvWfL/2lyn1nUSabl9RQKcz+3xVKSXHtFyv7873TDs7zmd+A63HyVI9vLV7VEpuPkV6d6Z14xGxO32oz67Uu3dvxo8fj41N0yvwoEGDuOOOO85p3RpbG+5/6taOfagXPDDtVi77g+Uwvpf+AY+HhfNu6jp2/vADBd8W8reXorHX6xnhdgeb8vK5asAAvD3Ho9cPbvjz95vA/w4cYN/+/QBMmz6Dvz7b2CAV9vhk/mhnx5Snn2HjplxKSkr51/9lExo2hQ3/zmHnDz/wyy+/8EDAfU3We5+/n6lRMP/LtnYZg8GAohzhxIkTGE8bUZQjKMqRDoVo3vwFLFoS2/D6Xm9P+vfvT9Lad9i1u4RDh35h9+4Svv7vN4z1vJdvC78D4OixY/j43c/ad1Jafd0Rp06dath342kjJ06cQFGOUFNjqpnc6+XJqVOnSPjHGg4ePEh8QiJ9+/Zl7Ji7Tds+epQZUc9x3XXXMemxECp++omKip+o+OknwNTgZh7b/v37c5mNDXr94BZzBzZ36BfTXYerrmpZ2pkS9jiJa95k+44ijhw5yorXV3G9owNDbrqJYX/+M/0H9Gfpays4fvw4J0+eZGX8G1x37bU4DRnSYl1drXb4WEbZATp3Au62JXdNDOu2lVO5r5CsxFjeK26jQd3GFm1fKNu+jZJDCmWF2cTHZVJifnO6rhZVKaWotApFUVCOGTCYrW5oYCjjaj5iYWwmBfuqKCvMZGHsR6h3hzLxIu1xqnweS+jUaJLzS6msqqJkUxLz1xSifyCYcVoABwIec6UoaSVZuxQMx8rZkJjCVjtvfDvlYVoNWjs99nqzP20/U0OZXmeaO8zBn0l3VbEuLoWCKoWyzxNZ/V89QQ+5AlCWFsOcNTmUKCqqoZyi3Yqpp193afQT543h86WEPBFN8qZiyqqqKCvMZlFiTmO+UVdFVsxs4itcmfm8PzqlisoK05+h+4880a1p+/Sjdm8hW8oVKku3kRGbyAbFLKi2GmwxoCimse7a+90ODQxlnJrNogVpFJQrGJRytmakkLXvAn25C0g1KCiKguGkCnUqBkVBUQymIUdsnAkIcuLb5JVk7FJQdmUSn17OHRP9TZ1XLJ3vWl3TvFevMw3xNUCPva47VHTPE0txFJ1C3Z7EnAUpbC03oKoGSnaWotjqse9v+bMdUVuVw9rkHIrKyynZksL8VfnYeocSUF//trXVmOYbNADoCQj1Rpu/kjlJ+ZQdMqBUFJOVnEnR2TYqWFPePB9+PcIRVcPA/s1HV3FgQogrO9eu5LPdCtW/lvPF6lS+tvPCcxhwuxeefTazZvVm/lcNqFV8uTadr6/2YIw87Wemq8vHAgBXb3z6bCY+IZ8yA6BWkZuUzlY7D8ZdpHWv7shifdDCddJi+RsNowMD0f0nificcpSqQpITM6keG4iPHZdOeUQtJWN5HBnbSqksLyY3OYbl3wxhamh9I4SldIvlFQtxvlRYjFM5GTP88YrMrB8y2FLcujodUA3195+qqaUWw7H68v75Ljt0gEbX7B6OXT9s6+/tnOlE0G49psPHAQtxUilJiyYqzYBPZBSj+igN9R5T9VODW0gI9psTeT2nHINBoSQrkYxSJ8aNNbWkGLYnEfVyDtqHZjNtOA2fr1Qu3I0Cy/lrszhq3fG9q5qMVUlsrVABA2WfJ5FR6oivh5Pl9Ko0oiZFMD9tGyUVVVTu20bG4iS29r0LnxHm+fERlCO0fLCn/CNmh0awKMP0+bLtOSxbkEqpUyCTRnTf/LzT49ygjThZvO4ZKEqOJT67kLKKcoo+T2T2mj2MCgturJ92w3zjonrir6v92V3PPUFO5GW0OtN8C14hN+Pk+kfLCwKRM/7K9Q6OfJz1L95OfocrruiL6223kRAfR58+fcjNy8dz/LgWjZoj7xzBgP79yc3L54brr6d379707t3Y0Ni3b1/e/McbvJm0lpXxb3Ds2DH+9Cd7nnn6ae7z9yM+IZHrHR0ZctNNTdar7dcP95F3sikvj6CHA1vd57/Nm883Bd82vPYLeBCAL3M3WmzQOqN379706tW4v7a2tkx7Opy8/M18sSmX06dPc7Xd1dx4vWOL725zmQ31D7O1+tpau3bv5uln/srp06cBeCNxNW8krib0L5OY/sxU+vTpw4rlS1m8ZCkfrM/AycmJFcti+UP93IVvvZ3MgQMVHDhQQWBQcJPv9tWXeR3fITPHjx9Ho9E0bMuct5cnhupq5r8Sw/Hjx7n9tuG8tjQWGxsbbGxsWLVyBa+vSiAo5DEAXG+7jddXLG91XV0tbkFg/ZBQOnxmxWFYuZLkeZNZY+vAqMBwZgZUsbrVTzoQFBlOyfJEngpOROfsRdjUCHxmmzXwOnvj0y+S+BnBLGu44aNhkGsIr8SE42bnzasr4fWEFOZMiUPt64CL12wSpl28Tw3p/KNZUpPI6jVRJFfUorFzxC1gIS+FOTcMDaXznsWK6kSWRQez/JgW/XBvXloafh7nYNDhMyuWyqWxzHksBXTOBMyNIax+lFn7O91xTE7nuUkxHK7VYj/ci5dmP9S1Q4eJbknrO4slNUmsTo4muVyBvnqcPKYTF2HKN9RNiSzfpAD5LAvPN/ukjqDETF4cfoF2vAdwCY0irCKBRU8Eg96VgLBIph2LJPfMAte4MurGdN6LTccnNZzr2/vd2nnz6ipYnZDCy1MSOYyOoXf+f3v3HxJ3Hcdx/HUnu0vn0Q+XYAmKxaLSua4iLZhYimC0MBwaoTAczBSXkmhYhuWPPJ3olBMXluBw+N/AMPq3oD8ikMr+WRAzRvpHOUxlS5v37Q+9ef66O72bnnfPxz93xwe+fvn44fx+vy/f70+u3rGvVlNEjll901ig1h/dn3/VBwXjkiVDn451KM8mJZ9pVvutZnXWFKlXCXq+sFkfrSUcrHf/eZtHBIcl6ZRefnBQl6qGNTUnHUuyq7CxWnkPye8KYX9Y4+06oe/UWe3Q1J04peU0ynlhff+ZOHuW0oacau3PVXZDhmwZ9XJ+NqjeoR6VDs9KtiSlZb+lC3e9/hgv/Lje3A+Ls1qUTbHb7KrwyGt1alvsV1/T2+r9x6aEEzmqbSvTsxZJsqu8o05f9n6h98+0aFGr4w0dZXqOPS83OPjr4wgQbdd7XXUa6B5UZUGzFrQ6z590ndMLrMeg8ed+0NvfSV/X35JkSS1Vd+2CPr5cpoJZm5KzStReHaRK68PCEqfk+FvqdVSoc86qx54+pfPdFff+McXnuHxfrzDPq7zPk0WK0sY9g33M2/0enxqpUdHg+rPhqaoCjcim013j+vDQto72dR+z+9+D13lKGVdv/4QWVqSxphKNeZzJU+euaPhskpRYoPamBbX2Vev1tmXZku16o2XtudLKbxq5OKzrdySN1qlo1OMAL9brh56DaTns+/t18zzGKa/BoSVnvzrP52v6tnQsOVOFLY617he+xovlaJQGhrpVeXlGC7IpOT1HtRcrle1ZQ7K2d7V1835zScVyNEmXhrpV6ZzRckyCjmeUyFlVvGU/7FAS/Hles9M8+T4jxR2P042BRpU6lmRJtOt0dY/KX10/Tih+b5iMnTZki2CT3/+prz7/Rf/e3v7uMvroEb35brqeeYmHD8Dk5KTSPNrTBteyJvvK1HK3RoM19nsXF8t/XFN9mVPW+q/VnsudPAAAQCiaHq1Q8bVUOa9WhE6r7yBY/M910KcQEWL96KwDAAAQCKIRhBqTl8qn+fl5v49Dxd820l55XE+ejNdP397U7z//rbm/VpvmPhwfoyfSH9XJrEQ9ELP/VVVA5FnS9M0ZLVpXWwHYbJJWZnV9YkJTVrvOhkR/bwAAAAAAAACHxXaBHyEgDopn2Odeh94CQH8Q/O0g+ugRZeanKDM/5aBPBYhgNuXVNOpG3xVVFvdrKcoia1SsElKzVN5XqjyKbgEAAAAAAAD4wTPc2+k9sN8Mw9gQ9JlMpoADQFp9AgjI/W31CQAAAIQWWn3uD1p9AgCAYHLHIL5egf3mDvd8vdLqEwAAAAAAAAAARDzPcM/zvclkUlRUVMBtFYFAGYYhl8u1JYB2V//tdo0S/AEAAAAAAAAAgLCzOfRzfybwQyhxh9CGYWhlZWXL2G4rUumdAQAAAAAAAAAAwpJn6GcYBqEfQpZnALg5rN4Ngj8AAAAAAAAAABBWNgcm7paJhH4IZe41ut369RfBHwAAAAAAAAAACDubK6fMZiIRhD6z2RxQ1R+rHAAAAAAAAAAAhI3tqqVcLhfBHw4Fs9ksl8u156o/VjkAAAAAAAAAAAhLgeyVBhwU9vgDAAAAAAAAAADw4A5NCP5w2HiuWVp9AgAAAAAAAACAiLbX0AQIJXtZxwR/AAAAAAAAAAAgbFHxh8MmkDVL8AcAAAAAAAAAAACEAYI/AAAAAAAAAAAAIAwQ/AEAAAAAAAAAAABhgOAPAAAAAAAAAAAACAP/A2HRbaJHUofjAAAAAElFTkSuQmCC)"
      ],
      "metadata": {
        "id": "eiofRMsQbnbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "Zan9Ug3Ab6UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "“By decreasing loss”\n",
        "\n",
        "3.Reflex Router Attention on s_d_p CA for all hidden states (first 2 layer SA)\n",
        "\n",
        "2.Redlex Router Attention on all hidden states (linear combinations for future s_d_p) (first 2 layer SA)\n",
        "\n",
        "1.Reflex Attention (const) 6head=cat(CA(prev1), CA(prev2), SA) (first 2 layer SA) and also simple Transformer"
      ],
      "metadata": {
        "id": "yzUUSI8JcImT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hlky6IbBcDvX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подводя итог: RA может как увеличить, так и у ухудшить скор модели. Так, закрепленный вариант с Reflex Attention (без роутера) на эмпирический опыте показал результат лучше, чем эксперимент с его улучшенной версией и большей степенью свободы в виде Router Reflex Attention. Необходимо провести больше экспериментов, а так посмотреть на влияние таких факторов как изменение конфигураций моделей, тюнинг весов, добавление сдвигов в пользу более дальних/ближних скрытых представлений в RA.  Также стоит исследовать тему перенасыщение информацией у модели такого вида: так, на эксперименте два можно было заметить, что когда мы лишаем модель степени свободы качество возрастает на некоторый сдвиг. Возможно, это связано с перемешиванием информации и может негативно сказаться на моделях похожих гипер-связей. Что касается масштабирования, модели большой глубины с RA показывают лучший скор по сравнению с похожим меньшего размера. Можно предположить, что увелечение модели (в т.ч. добавление преобладающего числа SA simple голов) улучшит ее показания и с точки зрения учитывания длинного контекста."
      ],
      "metadata": {
        "id": "aLEf6yjvUiLQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5T8125dUzUC"
      },
      "source": [
        "##preparing (data, modules)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fhxGcC-99zR",
        "outputId": "4cda1d07-f2ee-4611-ba34-cc65249d2133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIS4uTyLYYjr",
        "outputId": "ebedac0b-c1fa-4468-87c6-3a337d84fa01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/val.bin.zip\n",
            "  inflating: /content/val.bin        \n",
            "Archive:  /content/drive/MyDrive/train.bin.zip\n",
            "  inflating: /content/train.bin      \n"
          ]
        }
      ],
      "source": [
        "# prepared data from karpathy rep< please check prepare.py from karpathy repo -> (train.bin, val.bin)\n",
        "# data.zip: train https://drive.google.com/file/d/1gx2iQux-hNQ6Gq1a1XwrJigevgkrTyr1/view?usp=drive_link\n",
        "#           val: https://drive.google.com/file/d/1YZ4mzkLdy6UCfpYbJABgT7ymhflFkdoA/view?usp=drive_link\n",
        "!unzip '/content/drive/MyDrive/val.bin.zip' -d '/content'\n",
        "!unzip '/content/drive/MyDrive/train.bin.zip' -d '/content'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m73kaKW1N4M7",
        "outputId": "4485b4cb-cca5-4de0-b3b9-bc57e6459e33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Total 682 (delta 0), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (682/682), 952.47 KiB | 6.80 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyH_89eU9_pv",
        "outputId": "302cda2e-8275-4b6d-d129-6e3b5a4f76fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch tqdm matplotlib wandb tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1OYH2stCyMO"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "import tiktoken\n",
        "import pickle\n",
        "\n",
        "from contextlib import nullcontext\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.distributed import DistributedSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "SSKfjm02Rh26",
        "outputId": "f0991ef5-3b82-4952-b170-e67ba08619b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YymNa9v6BOy"
      },
      "source": [
        "## GPT: Self-Attention (6 layer, 6 head) (1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpP9EKpbV4Qi"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, prevs):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        if self.model_type == 'simpleTransformer': # len(prevs) == 0):\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y, (k, v)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = SelfAttention(config)\n",
        "        self.ln2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, prevs):\n",
        "        attn_kv = self.attn(self.ln1(x), prevs)\n",
        "        x = x + res_kv[0]\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x, attn_kv[1]\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        prevs = []\n",
        "\n",
        "        #attention\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "MACXOyJ4N871",
        "outputId": "52b735a2-2259-48ce-d63b-ffb1f97636cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ],
            "text/plain": [
              "__main__.GPT"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4576raQfP08a"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'simpleTransformer' # or 'reflex'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T796ehMaTAjL",
        "outputId": "8a7c155a-5761-4398-b9c5-58c10b95875d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='simpleTransformer')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Q-nldJcP9vn"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'SA:{config}'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ],
      "metadata": {
        "id": "A0RF70jRex2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nCCN76kQrrY"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/SimpleGPT'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000"
      ],
      "metadata": {
        "id": "C2OrHqoae5CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr_9AaYPNayT",
        "outputId": "be136059-3e8a-43b0-b0ca-92896c770f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 32,768\n"
          ]
        }
      ],
      "source": [
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGyVO_rLQiHH"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcqZ11QXOKpB",
        "outputId": "7264861f-07c0-47d1-b850-11ecb46ecf30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xg8WJ3iTpxu",
        "outputId": "db1c776d-3a20-48bc-c336-5f804aa2b381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 13, with 9,984 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b9zrMJqdTfJ5",
        "outputId": "cfd94bf4-ad96-4555-c9c3-64d64f4c48c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:6cvt0vi1) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">SA:CA-1h:CA-2h=2:2:2-simpleTransformer-1024-768</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/6cvt0vi1' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/6cvt0vi1</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241123_234831-6cvt0vi1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:6cvt0vi1). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241123_234904-m0t6fwba</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/m0t6fwba' target=\"_blank\">SA:CA-1h:CA-2h=2:2:2-simpleTransformer-1024-768</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/m0t6fwba' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/m0t6fwba</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-6-0fa6fba77f89> line 178 \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:12.268000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-6-0fa6fba77f89> line 125 \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.238000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-6-0fa6fba77f89> line 17 \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.329000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-6-0fa6fba77f89> line 35 \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.556000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-6-0fa6fba77f89> line 109 \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1123 23:49:13.958000 1227 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 11.0130, val loss 11.0141\n",
            "iter 0: loss 11.0115, time 27989.16ms, mfu -100.00%\n",
            "iter 10: loss 9.7645, time 141.54ms, mfu 40.31%\n",
            "iter 20: loss 9.2972, time 141.08ms, mfu 40.33%\n",
            "iter 30: loss 8.7169, time 141.23ms, mfu 40.33%\n",
            "iter 40: loss 8.0334, time 141.18ms, mfu 40.34%\n",
            "iter 50: loss 7.5373, time 141.25ms, mfu 40.35%\n",
            "iter 60: loss 7.2411, time 141.04ms, mfu 40.36%\n",
            "iter 70: loss 7.0573, time 141.59ms, mfu 40.35%\n",
            "iter 80: loss 6.9193, time 141.89ms, mfu 40.34%\n",
            "iter 90: loss 6.7982, time 141.65ms, mfu 40.33%\n",
            "iter 100: loss 6.5973, time 141.06ms, mfu 40.34%\n",
            "iter 110: loss 6.6217, time 141.01ms, mfu 40.36%\n",
            "iter 120: loss 6.4570, time 141.09ms, mfu 40.37%\n",
            "iter 130: loss 6.4833, time 141.36ms, mfu 40.37%\n",
            "iter 140: loss 6.3151, time 141.55ms, mfu 40.36%\n",
            "iter 150: loss 6.3043, time 141.16ms, mfu 40.37%\n",
            "iter 160: loss 6.2880, time 141.20ms, mfu 40.37%\n",
            "iter 170: loss 6.2788, time 141.81ms, mfu 40.36%\n",
            "iter 180: loss 6.2460, time 141.22ms, mfu 40.36%\n",
            "iter 190: loss 6.3344, time 141.38ms, mfu 40.36%\n",
            "iter 200: loss 6.1209, time 141.22ms, mfu 40.37%\n",
            "iter 210: loss 6.1693, time 141.23ms, mfu 40.37%\n",
            "iter 220: loss 6.1478, time 141.34ms, mfu 40.37%\n",
            "iter 230: loss 6.1209, time 141.71ms, mfu 40.36%\n",
            "iter 240: loss 6.2889, time 141.45ms, mfu 40.36%\n",
            "iter 250: loss 6.1469, time 141.61ms, mfu 40.35%\n",
            "iter 260: loss 5.8867, time 141.66ms, mfu 40.34%\n",
            "iter 270: loss 6.0797, time 142.12ms, mfu 40.32%\n",
            "iter 280: loss 6.1002, time 141.61ms, mfu 40.32%\n",
            "iter 290: loss 5.9561, time 142.07ms, mfu 40.30%\n",
            "step 300: train loss 6.0327, val loss 5.9827\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 300: loss 6.0253, time 23619.73ms, mfu 36.30%\n",
            "iter 310: loss 6.1245, time 141.79ms, mfu 36.69%\n",
            "iter 320: loss 6.0270, time 141.26ms, mfu 37.06%\n",
            "iter 330: loss 5.9426, time 141.21ms, mfu 37.40%\n",
            "iter 340: loss 5.9568, time 141.55ms, mfu 37.69%\n",
            "iter 350: loss 6.0448, time 141.07ms, mfu 37.96%\n",
            "iter 360: loss 5.8093, time 141.06ms, mfu 38.21%\n",
            "iter 370: loss 5.9027, time 141.07ms, mfu 38.44%\n",
            "iter 380: loss 5.9973, time 142.43ms, mfu 38.60%\n",
            "iter 390: loss 5.8625, time 141.55ms, mfu 38.77%\n",
            "iter 400: loss 5.8519, time 141.38ms, mfu 38.93%\n",
            "iter 410: loss 5.8362, time 141.51ms, mfu 39.07%\n",
            "iter 420: loss 5.8570, time 141.46ms, mfu 39.19%\n",
            "iter 430: loss 5.7944, time 141.39ms, mfu 39.31%\n",
            "iter 440: loss 5.7501, time 142.58ms, mfu 39.38%\n",
            "iter 450: loss 5.8527, time 141.86ms, mfu 39.47%\n",
            "iter 460: loss 5.7554, time 141.08ms, mfu 39.56%\n",
            "iter 470: loss 5.7735, time 141.66ms, mfu 39.64%\n",
            "iter 480: loss 5.6355, time 141.44ms, mfu 39.71%\n",
            "iter 490: loss 5.6749, time 141.29ms, mfu 39.77%\n",
            "iter 500: loss 5.7128, time 141.40ms, mfu 39.83%\n",
            "iter 510: loss 5.5835, time 142.13ms, mfu 39.86%\n",
            "iter 520: loss 5.5870, time 141.82ms, mfu 39.90%\n",
            "iter 530: loss 5.5901, time 141.70ms, mfu 39.94%\n",
            "iter 540: loss 5.5676, time 141.60ms, mfu 39.97%\n",
            "iter 550: loss 5.5224, time 141.50ms, mfu 40.01%\n",
            "iter 560: loss 5.5539, time 141.46ms, mfu 40.04%\n",
            "iter 570: loss 5.4013, time 141.47ms, mfu 40.07%\n",
            "iter 580: loss 5.5965, time 141.47ms, mfu 40.10%\n",
            "iter 590: loss 5.5307, time 141.84ms, mfu 40.11%\n",
            "step 600: train loss 5.5172, val loss 5.4694\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 600: loss 5.5940, time 24359.68ms, mfu 36.12%\n",
            "iter 610: loss 5.5550, time 141.11ms, mfu 36.55%\n",
            "iter 620: loss 5.4566, time 141.65ms, mfu 36.93%\n",
            "iter 630: loss 5.4691, time 141.63ms, mfu 37.26%\n",
            "iter 640: loss 5.5265, time 141.62ms, mfu 37.57%\n",
            "iter 650: loss 5.3322, time 141.78ms, mfu 37.83%\n",
            "iter 660: loss 5.4054, time 141.41ms, mfu 38.08%\n",
            "iter 670: loss 5.4190, time 141.37ms, mfu 38.31%\n",
            "iter 680: loss 5.3615, time 141.27ms, mfu 38.52%\n",
            "iter 690: loss 5.3747, time 141.60ms, mfu 38.70%\n",
            "iter 700: loss 5.3001, time 142.27ms, mfu 38.84%\n",
            "iter 710: loss 5.3291, time 141.24ms, mfu 38.99%\n",
            "iter 720: loss 5.4638, time 141.54ms, mfu 39.13%\n",
            "iter 730: loss 5.3948, time 141.50ms, mfu 39.25%\n",
            "iter 740: loss 5.2158, time 141.33ms, mfu 39.36%\n",
            "iter 750: loss 5.2476, time 141.35ms, mfu 39.46%\n",
            "iter 760: loss 5.3679, time 141.63ms, mfu 39.54%\n",
            "iter 770: loss 5.3223, time 142.26ms, mfu 39.60%\n",
            "iter 780: loss 5.2531, time 142.00ms, mfu 39.66%\n",
            "iter 790: loss 5.2430, time 141.65ms, mfu 39.72%\n",
            "iter 800: loss 5.1225, time 141.62ms, mfu 39.78%\n",
            "iter 810: loss 5.3134, time 141.64ms, mfu 39.83%\n",
            "iter 820: loss 5.1418, time 141.55ms, mfu 39.88%\n",
            "iter 830: loss 5.1244, time 141.15ms, mfu 39.93%\n",
            "iter 840: loss 5.1429, time 141.39ms, mfu 39.97%\n",
            "iter 850: loss 5.1293, time 141.37ms, mfu 40.01%\n",
            "iter 860: loss 4.9887, time 141.66ms, mfu 40.04%\n",
            "iter 870: loss 5.1098, time 141.63ms, mfu 40.06%\n",
            "iter 880: loss 5.1218, time 141.76ms, mfu 40.08%\n",
            "iter 890: loss 5.0620, time 141.80ms, mfu 40.10%\n",
            "step 900: train loss 5.0952, val loss 5.0394\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 900: loss 5.0927, time 24090.84ms, mfu 36.11%\n",
            "iter 910: loss 5.1054, time 141.88ms, mfu 36.52%\n",
            "iter 920: loss 4.9557, time 141.75ms, mfu 36.90%\n",
            "iter 930: loss 5.0884, time 141.73ms, mfu 37.23%\n",
            "iter 940: loss 5.1419, time 141.47ms, mfu 37.54%\n",
            "iter 950: loss 5.0119, time 141.45ms, mfu 37.82%\n",
            "iter 960: loss 5.0725, time 141.37ms, mfu 38.08%\n",
            "iter 970: loss 5.1817, time 141.59ms, mfu 38.30%\n",
            "iter 980: loss 4.9286, time 142.32ms, mfu 38.48%\n",
            "iter 990: loss 5.0169, time 141.69ms, mfu 38.66%\n",
            "iter 1000: loss 5.0229, time 141.37ms, mfu 38.83%\n",
            "iter 1010: loss 4.8939, time 141.45ms, mfu 38.98%\n",
            "iter 1020: loss 5.0504, time 141.67ms, mfu 39.11%\n",
            "iter 1030: loss 4.8928, time 142.53ms, mfu 39.20%\n",
            "iter 1040: loss 4.9346, time 141.63ms, mfu 39.31%\n",
            "iter 1050: loss 4.9346, time 141.22ms, mfu 39.42%\n",
            "iter 1060: loss 5.0307, time 141.40ms, mfu 39.51%\n",
            "iter 1070: loss 5.0940, time 141.57ms, mfu 39.59%\n",
            "iter 1080: loss 4.8979, time 142.40ms, mfu 39.64%\n",
            "iter 1090: loss 4.9217, time 141.17ms, mfu 39.72%\n",
            "iter 1100: loss 4.9334, time 141.51ms, mfu 39.78%\n",
            "iter 1110: loss 4.9356, time 141.56ms, mfu 39.83%\n",
            "iter 1120: loss 4.9105, time 141.47ms, mfu 39.88%\n",
            "iter 1130: loss 4.8737, time 141.18ms, mfu 39.93%\n",
            "iter 1140: loss 4.8519, time 141.34ms, mfu 39.98%\n",
            "iter 1150: loss 4.8367, time 141.39ms, mfu 40.02%\n",
            "iter 1160: loss 4.6898, time 141.72ms, mfu 40.04%\n",
            "iter 1170: loss 4.7684, time 141.52ms, mfu 40.07%\n",
            "iter 1180: loss 4.8205, time 142.37ms, mfu 40.07%\n",
            "iter 1190: loss 4.7603, time 141.68ms, mfu 40.09%\n",
            "step 1200: train loss 4.8031, val loss 4.7642\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 1200: loss 4.7935, time 24073.28ms, mfu 36.10%\n",
            "iter 1210: loss 4.7486, time 142.33ms, mfu 36.50%\n",
            "iter 1220: loss 4.7199, time 141.66ms, mfu 36.88%\n",
            "iter 1230: loss 4.8108, time 141.10ms, mfu 37.24%\n",
            "iter 1240: loss 4.6902, time 141.68ms, mfu 37.54%\n",
            "iter 1250: loss 4.7847, time 141.41ms, mfu 37.82%\n",
            "iter 1260: loss 4.8751, time 141.43ms, mfu 38.07%\n",
            "iter 1270: loss 4.6855, time 141.67ms, mfu 38.29%\n",
            "iter 1280: loss 4.7241, time 141.53ms, mfu 38.50%\n",
            "iter 1290: loss 4.6546, time 141.98ms, mfu 38.67%\n",
            "iter 1300: loss 4.6631, time 141.65ms, mfu 38.83%\n",
            "iter 1310: loss 4.7930, time 141.40ms, mfu 38.98%\n",
            "iter 1320: loss 4.6964, time 141.40ms, mfu 39.12%\n",
            "iter 1330: loss 4.6087, time 142.61ms, mfu 39.21%\n",
            "iter 1340: loss 4.7041, time 141.80ms, mfu 39.31%\n",
            "iter 1350: loss 4.5317, time 141.99ms, mfu 39.40%\n",
            "iter 1360: loss 4.6328, time 141.60ms, mfu 39.49%\n",
            "iter 1370: loss 4.7667, time 141.75ms, mfu 39.56%\n",
            "iter 1380: loss 4.5785, time 142.39ms, mfu 39.61%\n",
            "iter 1390: loss 4.6404, time 141.73ms, mfu 39.68%\n",
            "iter 1400: loss 4.8214, time 141.65ms, mfu 39.74%\n",
            "iter 1410: loss 4.6208, time 141.54ms, mfu 39.80%\n",
            "iter 1420: loss 4.5581, time 141.59ms, mfu 39.85%\n",
            "iter 1430: loss 4.6623, time 141.59ms, mfu 39.89%\n",
            "iter 1440: loss 4.4958, time 141.62ms, mfu 39.93%\n",
            "iter 1450: loss 4.6235, time 141.73ms, mfu 39.96%\n",
            "iter 1460: loss 4.7425, time 142.58ms, mfu 39.97%\n",
            "iter 1470: loss 4.6976, time 141.65ms, mfu 40.00%\n",
            "iter 1480: loss 4.5556, time 141.63ms, mfu 40.03%\n",
            "iter 1490: loss 4.6212, time 141.63ms, mfu 40.06%\n",
            "step 1500: train loss 4.6082, val loss 4.5780\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 1500: loss 4.6204, time 24093.06ms, mfu 36.07%\n",
            "iter 1510: loss 4.5951, time 141.65ms, mfu 36.49%\n",
            "iter 1520: loss 4.4618, time 141.05ms, mfu 36.89%\n",
            "iter 1530: loss 4.6197, time 141.66ms, mfu 37.23%\n",
            "iter 1540: loss 4.5230, time 141.67ms, mfu 37.53%\n",
            "iter 1550: loss 4.6032, time 141.42ms, mfu 37.82%\n",
            "iter 1560: loss 4.5300, time 141.40ms, mfu 38.07%\n",
            "iter 1570: loss 4.4901, time 141.27ms, mfu 38.30%\n",
            "iter 1580: loss 4.5263, time 141.49ms, mfu 38.50%\n",
            "iter 1590: loss 4.4353, time 142.39ms, mfu 38.66%\n",
            "iter 1600: loss 4.5985, time 141.85ms, mfu 38.82%\n",
            "iter 1610: loss 4.6741, time 141.25ms, mfu 38.97%\n",
            "iter 1620: loss 4.6444, time 141.45ms, mfu 39.11%\n",
            "iter 1630: loss 4.5342, time 141.35ms, mfu 39.24%\n",
            "iter 1640: loss 4.5896, time 142.72ms, mfu 39.31%\n",
            "iter 1650: loss 4.5980, time 141.64ms, mfu 39.41%\n",
            "iter 1660: loss 4.4515, time 141.33ms, mfu 39.50%\n",
            "iter 1670: loss 4.5123, time 141.66ms, mfu 39.58%\n",
            "iter 1680: loss 4.5475, time 142.59ms, mfu 39.63%\n",
            "iter 1690: loss 4.5405, time 141.77ms, mfu 39.69%\n",
            "iter 1700: loss 4.6176, time 141.46ms, mfu 39.75%\n",
            "iter 1710: loss 4.4387, time 141.38ms, mfu 39.81%\n",
            "iter 1720: loss 4.3791, time 141.64ms, mfu 39.86%\n",
            "iter 1730: loss 4.3891, time 142.91ms, mfu 39.87%\n",
            "iter 1740: loss 4.4468, time 141.84ms, mfu 39.90%\n",
            "iter 1750: loss 4.5895, time 141.51ms, mfu 39.94%\n",
            "iter 1760: loss 4.4518, time 141.67ms, mfu 39.98%\n",
            "iter 1770: loss 4.3694, time 141.64ms, mfu 40.01%\n",
            "iter 1780: loss 4.4831, time 142.17ms, mfu 40.02%\n",
            "iter 1790: loss 4.4729, time 141.64ms, mfu 40.05%\n",
            "step 1800: train loss 4.4511, val loss 4.4360\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 1800: loss 4.5286, time 24096.59ms, mfu 36.07%\n",
            "iter 1810: loss 4.4913, time 142.81ms, mfu 36.46%\n",
            "iter 1820: loss 4.5612, time 141.69ms, mfu 36.84%\n",
            "iter 1830: loss 4.4057, time 141.51ms, mfu 37.19%\n",
            "iter 1840: loss 4.4178, time 141.27ms, mfu 37.51%\n",
            "iter 1850: loss 4.4344, time 141.55ms, mfu 37.79%\n",
            "iter 1860: loss 4.5419, time 141.34ms, mfu 38.04%\n",
            "iter 1870: loss 4.4265, time 142.58ms, mfu 38.24%\n",
            "iter 1880: loss 4.4824, time 141.71ms, mfu 38.44%\n",
            "iter 1890: loss 4.4933, time 141.47ms, mfu 38.63%\n",
            "iter 1900: loss 4.4971, time 141.61ms, mfu 38.80%\n",
            "iter 1910: loss 4.2818, time 141.37ms, mfu 38.96%\n",
            "iter 1920: loss 4.3518, time 141.89ms, mfu 39.08%\n",
            "iter 1930: loss 4.4162, time 141.65ms, mfu 39.20%\n",
            "iter 1940: loss 4.3429, time 142.88ms, mfu 39.27%\n",
            "iter 1950: loss 4.3165, time 141.76ms, mfu 39.37%\n",
            "iter 1960: loss 4.3680, time 141.46ms, mfu 39.47%\n",
            "iter 1970: loss 4.3931, time 141.33ms, mfu 39.56%\n",
            "iter 1980: loss 4.4948, time 141.79ms, mfu 39.63%\n",
            "iter 1990: loss 4.3954, time 142.42ms, mfu 39.67%\n",
            "iter 2000: loss 4.3275, time 141.78ms, mfu 39.73%\n",
            "iter 2010: loss 4.3797, time 141.55ms, mfu 39.79%\n",
            "iter 2020: loss 4.4219, time 141.60ms, mfu 39.84%\n",
            "iter 2030: loss 4.4109, time 141.58ms, mfu 39.88%\n",
            "iter 2040: loss 4.5337, time 141.44ms, mfu 39.93%\n",
            "iter 2050: loss 4.3572, time 141.61ms, mfu 39.97%\n",
            "iter 2060: loss 4.4142, time 141.77ms, mfu 39.99%\n",
            "iter 2070: loss 4.3484, time 141.56ms, mfu 40.03%\n",
            "iter 2080: loss 4.3813, time 142.37ms, mfu 40.03%\n",
            "iter 2090: loss 4.3879, time 141.55ms, mfu 40.06%\n",
            "step 2100: train loss 4.3476, val loss 4.3478\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 2100: loss 4.2881, time 24083.54ms, mfu 36.08%\n",
            "iter 2110: loss 4.4055, time 141.63ms, mfu 36.50%\n",
            "iter 2120: loss 4.2165, time 141.63ms, mfu 36.88%\n",
            "iter 2130: loss 4.3672, time 141.65ms, mfu 37.22%\n",
            "iter 2140: loss 4.3395, time 141.41ms, mfu 37.53%\n",
            "iter 2150: loss 4.4310, time 141.43ms, mfu 37.81%\n",
            "iter 2160: loss 4.3007, time 141.51ms, mfu 38.06%\n",
            "iter 2170: loss 4.3348, time 141.56ms, mfu 38.29%\n",
            "iter 2180: loss 4.2371, time 141.99ms, mfu 38.48%\n",
            "iter 2190: loss 4.2764, time 141.87ms, mfu 38.65%\n",
            "iter 2200: loss 4.2161, time 141.46ms, mfu 38.82%\n",
            "iter 2210: loss 4.2392, time 141.28ms, mfu 38.98%\n",
            "iter 2220: loss 4.2380, time 141.45ms, mfu 39.11%\n",
            "iter 2230: loss 4.3773, time 141.67ms, mfu 39.23%\n",
            "iter 2240: loss 4.2729, time 141.67ms, mfu 39.33%\n",
            "iter 2250: loss 4.3006, time 142.51ms, mfu 39.40%\n",
            "iter 2260: loss 4.2600, time 141.67ms, mfu 39.49%\n",
            "iter 2270: loss 4.3237, time 141.39ms, mfu 39.58%\n",
            "iter 2280: loss 4.3412, time 141.53ms, mfu 39.65%\n",
            "iter 2290: loss 4.2227, time 141.70ms, mfu 39.71%\n",
            "iter 2300: loss 4.2800, time 141.55ms, mfu 39.77%\n",
            "iter 2310: loss 4.3370, time 142.46ms, mfu 39.80%\n",
            "iter 2320: loss 4.2936, time 141.68ms, mfu 39.85%\n",
            "iter 2330: loss 4.3511, time 141.60ms, mfu 39.89%\n",
            "iter 2340: loss 4.3096, time 141.76ms, mfu 39.93%\n",
            "iter 2350: loss 4.3209, time 142.78ms, mfu 39.93%\n",
            "iter 2360: loss 4.3735, time 141.65ms, mfu 39.97%\n",
            "iter 2370: loss 4.2779, time 141.50ms, mfu 40.00%\n",
            "iter 2380: loss 4.0789, time 141.30ms, mfu 40.04%\n",
            "iter 2390: loss 4.2549, time 141.31ms, mfu 40.07%\n",
            "step 2400: train loss 4.2666, val loss 4.2488\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 2400: loss 4.2730, time 24118.06ms, mfu 36.09%\n",
            "iter 2410: loss 4.2163, time 141.33ms, mfu 36.52%\n",
            "iter 2420: loss 4.1745, time 141.50ms, mfu 36.90%\n",
            "iter 2430: loss 4.3305, time 141.37ms, mfu 37.25%\n",
            "iter 2440: loss 4.4179, time 141.54ms, mfu 37.55%\n",
            "iter 2450: loss 4.4069, time 141.45ms, mfu 37.83%\n",
            "iter 2460: loss 4.2120, time 142.48ms, mfu 38.05%\n",
            "iter 2470: loss 4.2434, time 141.53ms, mfu 38.28%\n",
            "iter 2480: loss 4.2393, time 141.41ms, mfu 38.49%\n",
            "iter 2490: loss 4.1621, time 141.45ms, mfu 38.67%\n",
            "iter 2500: loss 4.1540, time 141.31ms, mfu 38.84%\n",
            "iter 2510: loss 4.2352, time 141.69ms, mfu 38.98%\n",
            "iter 2520: loss 4.2451, time 142.55ms, mfu 39.09%\n",
            "iter 2530: loss 4.2248, time 141.74ms, mfu 39.21%\n",
            "iter 2540: loss 4.0778, time 141.55ms, mfu 39.32%\n",
            "iter 2550: loss 4.2736, time 141.46ms, mfu 39.42%\n",
            "iter 2560: loss 4.1941, time 141.58ms, mfu 39.51%\n",
            "iter 2570: loss 4.2073, time 141.57ms, mfu 39.59%\n",
            "iter 2580: loss 4.1910, time 142.48ms, mfu 39.63%\n",
            "iter 2590: loss 4.1222, time 141.50ms, mfu 39.70%\n",
            "iter 2600: loss 4.1907, time 141.51ms, mfu 39.76%\n",
            "iter 2610: loss 4.2734, time 141.35ms, mfu 39.82%\n",
            "iter 2620: loss 4.2160, time 141.64ms, mfu 39.87%\n",
            "iter 2630: loss 4.2822, time 141.80ms, mfu 39.91%\n",
            "iter 2640: loss 4.1762, time 142.13ms, mfu 39.93%\n",
            "iter 2650: loss 4.2834, time 141.62ms, mfu 39.97%\n",
            "iter 2660: loss 4.2372, time 141.53ms, mfu 40.00%\n",
            "iter 2670: loss 4.1649, time 141.44ms, mfu 40.04%\n",
            "iter 2680: loss 4.2589, time 141.42ms, mfu 40.07%\n",
            "iter 2690: loss 4.2117, time 141.37ms, mfu 40.10%\n",
            "step 2700: train loss 4.1876, val loss 4.1677\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 2700: loss 4.2764, time 24157.44ms, mfu 36.11%\n",
            "iter 2710: loss 4.1094, time 141.94ms, mfu 36.52%\n",
            "iter 2720: loss 4.2797, time 141.71ms, mfu 36.89%\n",
            "iter 2730: loss 4.1180, time 141.60ms, mfu 37.23%\n",
            "iter 2740: loss 4.0655, time 141.48ms, mfu 37.54%\n",
            "iter 2750: loss 4.2861, time 141.51ms, mfu 37.82%\n",
            "iter 2760: loss 4.1872, time 141.30ms, mfu 38.08%\n",
            "iter 2770: loss 4.1484, time 141.73ms, mfu 38.30%\n",
            "iter 2780: loss 4.1409, time 141.68ms, mfu 38.49%\n",
            "iter 2790: loss 4.1125, time 141.61ms, mfu 38.67%\n",
            "iter 2800: loss 4.1142, time 142.32ms, mfu 38.82%\n",
            "iter 2810: loss 4.1594, time 141.45ms, mfu 38.97%\n",
            "iter 2820: loss 4.1906, time 141.57ms, mfu 39.10%\n",
            "iter 2830: loss 4.0822, time 141.48ms, mfu 39.22%\n",
            "iter 2840: loss 4.1111, time 141.36ms, mfu 39.34%\n",
            "iter 2850: loss 4.1465, time 142.35ms, mfu 39.41%\n",
            "iter 2860: loss 4.1290, time 141.64ms, mfu 39.50%\n",
            "iter 2870: loss 4.0986, time 141.50ms, mfu 39.58%\n",
            "iter 2880: loss 4.1759, time 141.72ms, mfu 39.65%\n",
            "iter 2890: loss 4.1399, time 142.48ms, mfu 39.69%\n",
            "iter 2900: loss 4.1815, time 141.61ms, mfu 39.75%\n",
            "iter 2910: loss 4.1700, time 141.51ms, mfu 39.81%\n",
            "iter 2920: loss 4.1452, time 141.67ms, mfu 39.85%\n",
            "iter 2930: loss 4.2207, time 141.65ms, mfu 39.90%\n",
            "iter 2940: loss 4.0604, time 142.40ms, mfu 39.91%\n",
            "iter 2950: loss 4.0815, time 141.74ms, mfu 39.95%\n",
            "iter 2960: loss 4.2167, time 141.38ms, mfu 39.99%\n",
            "iter 2970: loss 4.0389, time 141.43ms, mfu 40.03%\n",
            "iter 2980: loss 4.0037, time 141.60ms, mfu 40.05%\n",
            "iter 2990: loss 4.1955, time 141.54ms, mfu 40.08%\n",
            "step 3000: train loss 4.1226, val loss 4.1024\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 3000: loss 4.1633, time 24131.05ms, mfu 36.09%\n",
            "iter 3010: loss 4.1899, time 141.96ms, mfu 36.50%\n",
            "iter 3020: loss 3.9817, time 141.31ms, mfu 36.89%\n",
            "iter 3030: loss 4.1161, time 141.34ms, mfu 37.24%\n",
            "iter 3040: loss 4.0901, time 141.67ms, mfu 37.54%\n",
            "iter 3050: loss 4.1698, time 141.73ms, mfu 37.81%\n",
            "iter 3060: loss 4.0400, time 142.43ms, mfu 38.04%\n",
            "iter 3070: loss 3.9608, time 141.55ms, mfu 38.27%\n",
            "iter 3080: loss 4.1441, time 141.43ms, mfu 38.47%\n",
            "iter 3090: loss 4.0583, time 141.81ms, mfu 38.65%\n",
            "iter 3100: loss 4.0945, time 142.14ms, mfu 38.80%\n",
            "iter 3110: loss 3.9584, time 141.70ms, mfu 38.95%\n",
            "iter 3120: loss 4.1149, time 141.35ms, mfu 39.09%\n",
            "iter 3130: loss 4.1234, time 141.62ms, mfu 39.21%\n",
            "iter 3140: loss 4.1484, time 141.41ms, mfu 39.32%\n",
            "iter 3150: loss 4.1187, time 141.65ms, mfu 39.42%\n",
            "iter 3160: loss 4.0543, time 142.36ms, mfu 39.49%\n",
            "iter 3170: loss 4.2371, time 141.64ms, mfu 39.56%\n",
            "iter 3180: loss 4.0052, time 141.42ms, mfu 39.64%\n",
            "iter 3190: loss 4.1094, time 141.40ms, mfu 39.71%\n",
            "iter 3200: loss 4.1060, time 141.39ms, mfu 39.78%\n",
            "iter 3210: loss 4.2354, time 142.61ms, mfu 39.80%\n",
            "iter 3220: loss 4.0275, time 141.81ms, mfu 39.85%\n",
            "iter 3230: loss 4.1313, time 141.40ms, mfu 39.90%\n",
            "iter 3240: loss 4.1035, time 141.46ms, mfu 39.94%\n",
            "iter 3250: loss 4.0612, time 141.71ms, mfu 39.97%\n",
            "iter 3260: loss 4.1088, time 141.54ms, mfu 40.01%\n",
            "iter 3270: loss 4.1015, time 142.18ms, mfu 40.02%\n",
            "iter 3280: loss 4.0568, time 141.58ms, mfu 40.05%\n",
            "iter 3290: loss 4.1355, time 141.56ms, mfu 40.07%\n",
            "step 3300: train loss 4.0585, val loss 4.0560\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 3300: loss 4.1703, time 24183.75ms, mfu 36.09%\n",
            "iter 3310: loss 4.1572, time 141.23ms, mfu 36.52%\n",
            "iter 3320: loss 4.0869, time 141.81ms, mfu 36.89%\n",
            "iter 3330: loss 4.0209, time 141.28ms, mfu 37.24%\n",
            "iter 3340: loss 4.0142, time 141.47ms, mfu 37.55%\n",
            "iter 3350: loss 4.0548, time 141.60ms, mfu 37.83%\n",
            "iter 3360: loss 4.0206, time 142.30ms, mfu 38.05%\n",
            "iter 3370: loss 4.1032, time 141.68ms, mfu 38.27%\n",
            "iter 3380: loss 4.1101, time 141.41ms, mfu 38.48%\n",
            "iter 3390: loss 4.0968, time 141.37ms, mfu 38.67%\n",
            "iter 3400: loss 4.0621, time 141.81ms, mfu 38.83%\n",
            "iter 3410: loss 4.0911, time 141.95ms, mfu 38.96%\n",
            "iter 3420: loss 4.0250, time 141.36ms, mfu 39.10%\n",
            "iter 3430: loss 4.0336, time 141.86ms, mfu 39.22%\n",
            "iter 3440: loss 4.0950, time 141.63ms, mfu 39.32%\n",
            "iter 3450: loss 4.0276, time 142.63ms, mfu 39.39%\n",
            "iter 3460: loss 3.9993, time 141.95ms, mfu 39.47%\n",
            "iter 3470: loss 3.9945, time 141.52ms, mfu 39.56%\n",
            "iter 3480: loss 4.0350, time 141.77ms, mfu 39.63%\n",
            "iter 3490: loss 3.9733, time 141.58ms, mfu 39.69%\n",
            "iter 3500: loss 4.0134, time 142.38ms, mfu 39.73%\n",
            "iter 3510: loss 4.0693, time 141.71ms, mfu 39.78%\n",
            "iter 3520: loss 4.1005, time 141.55ms, mfu 39.84%\n",
            "iter 3530: loss 3.9292, time 141.77ms, mfu 39.88%\n",
            "iter 3540: loss 4.0327, time 141.67ms, mfu 39.92%\n",
            "iter 3550: loss 3.9744, time 142.04ms, mfu 39.94%\n",
            "iter 3560: loss 4.1188, time 141.40ms, mfu 39.98%\n",
            "iter 3570: loss 3.9775, time 141.72ms, mfu 40.01%\n",
            "iter 3580: loss 4.1406, time 141.57ms, mfu 40.04%\n",
            "iter 3590: loss 4.0069, time 142.89ms, mfu 40.03%\n",
            "step 3600: train loss 4.0075, val loss 4.0097\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 3600: loss 4.0250, time 24139.37ms, mfu 36.05%\n",
            "iter 3610: loss 3.9667, time 141.76ms, mfu 36.47%\n",
            "iter 3620: loss 3.8854, time 141.69ms, mfu 36.85%\n",
            "iter 3630: loss 4.0552, time 141.55ms, mfu 37.20%\n",
            "iter 3640: loss 4.0483, time 141.44ms, mfu 37.51%\n",
            "iter 3650: loss 3.9690, time 141.68ms, mfu 37.79%\n",
            "iter 3660: loss 3.9912, time 142.47ms, mfu 38.01%\n",
            "iter 3670: loss 4.0491, time 141.59ms, mfu 38.24%\n",
            "iter 3680: loss 4.0100, time 141.44ms, mfu 38.45%\n",
            "iter 3690: loss 3.8546, time 141.76ms, mfu 38.63%\n",
            "iter 3700: loss 3.9743, time 141.47ms, mfu 38.80%\n",
            "iter 3710: loss 4.0638, time 142.48ms, mfu 38.93%\n",
            "iter 3720: loss 3.9349, time 141.64ms, mfu 39.06%\n",
            "iter 3730: loss 4.0477, time 141.43ms, mfu 39.19%\n",
            "iter 3740: loss 3.9884, time 141.44ms, mfu 39.31%\n",
            "iter 3750: loss 3.9728, time 141.51ms, mfu 39.41%\n",
            "iter 3760: loss 4.0520, time 142.37ms, mfu 39.47%\n",
            "iter 3770: loss 4.0365, time 141.54ms, mfu 39.56%\n",
            "iter 3780: loss 3.9780, time 141.42ms, mfu 39.64%\n",
            "iter 3790: loss 4.0561, time 141.99ms, mfu 39.69%\n",
            "iter 3800: loss 4.0319, time 141.63ms, mfu 39.75%\n",
            "iter 3810: loss 3.8650, time 142.10ms, mfu 39.79%\n",
            "iter 3820: loss 4.0519, time 141.51ms, mfu 39.84%\n",
            "iter 3830: loss 3.9133, time 141.73ms, mfu 39.89%\n",
            "iter 3840: loss 4.0184, time 141.58ms, mfu 39.93%\n",
            "iter 3850: loss 4.0543, time 141.63ms, mfu 39.96%\n",
            "iter 3860: loss 3.9923, time 142.64ms, mfu 39.97%\n",
            "iter 3870: loss 3.9210, time 141.57ms, mfu 40.00%\n",
            "iter 3880: loss 4.1035, time 141.53ms, mfu 40.03%\n",
            "iter 3890: loss 3.9852, time 141.49ms, mfu 40.06%\n",
            "step 3900: train loss 3.9743, val loss 3.9726\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 3900: loss 3.9895, time 24143.35ms, mfu 36.08%\n",
            "iter 3910: loss 3.8928, time 141.41ms, mfu 36.51%\n",
            "iter 3920: loss 3.8936, time 141.40ms, mfu 36.89%\n",
            "iter 3930: loss 3.9886, time 142.52ms, mfu 37.21%\n",
            "iter 3940: loss 3.8142, time 141.19ms, mfu 37.53%\n",
            "iter 3950: loss 3.9078, time 141.50ms, mfu 37.81%\n",
            "iter 3960: loss 3.9280, time 141.23ms, mfu 38.07%\n",
            "iter 3970: loss 3.9532, time 141.37ms, mfu 38.30%\n",
            "iter 3980: loss 3.8674, time 141.61ms, mfu 38.50%\n",
            "iter 3990: loss 3.9505, time 142.65ms, mfu 38.65%\n",
            "iter 4000: loss 3.9383, time 141.58ms, mfu 38.81%\n",
            "iter 4010: loss 3.9276, time 141.46ms, mfu 38.96%\n",
            "iter 4020: loss 3.8661, time 141.45ms, mfu 39.10%\n",
            "iter 4030: loss 3.9247, time 141.66ms, mfu 39.22%\n",
            "iter 4040: loss 3.9907, time 141.53ms, mfu 39.33%\n",
            "iter 4050: loss 3.8859, time 142.29ms, mfu 39.41%\n",
            "iter 4060: loss 3.8826, time 141.67ms, mfu 39.49%\n",
            "iter 4070: loss 3.9261, time 141.44ms, mfu 39.58%\n",
            "iter 4080: loss 3.9855, time 141.62ms, mfu 39.65%\n",
            "iter 4090: loss 3.7697, time 141.37ms, mfu 39.72%\n",
            "iter 4100: loss 3.9131, time 142.55ms, mfu 39.75%\n",
            "iter 4110: loss 3.9578, time 141.82ms, mfu 39.80%\n",
            "iter 4120: loss 3.9716, time 141.93ms, mfu 39.84%\n",
            "iter 4130: loss 4.0120, time 141.66ms, mfu 39.88%\n",
            "iter 4140: loss 3.9364, time 142.04ms, mfu 39.91%\n",
            "iter 4150: loss 3.9811, time 141.32ms, mfu 39.96%\n",
            "iter 4160: loss 3.9891, time 142.63ms, mfu 39.96%\n",
            "iter 4170: loss 3.9370, time 141.51ms, mfu 40.00%\n",
            "iter 4180: loss 3.9131, time 141.43ms, mfu 40.03%\n",
            "iter 4190: loss 3.9977, time 141.70ms, mfu 40.06%\n",
            "step 4200: train loss 3.9435, val loss 3.9423\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 4200: loss 3.9300, time 24123.02ms, mfu 36.08%\n",
            "iter 4210: loss 3.9679, time 141.69ms, mfu 36.49%\n",
            "iter 4220: loss 3.9350, time 141.65ms, mfu 36.87%\n",
            "iter 4230: loss 3.9375, time 141.12ms, mfu 37.23%\n",
            "iter 4240: loss 3.9668, time 142.66ms, mfu 37.51%\n",
            "iter 4250: loss 3.9834, time 141.61ms, mfu 37.78%\n",
            "iter 4260: loss 3.7992, time 141.34ms, mfu 38.04%\n",
            "iter 4270: loss 3.9780, time 141.46ms, mfu 38.27%\n",
            "iter 4280: loss 4.0074, time 142.64ms, mfu 38.45%\n",
            "iter 4290: loss 3.9852, time 141.79ms, mfu 38.63%\n",
            "iter 4300: loss 3.9394, time 141.43ms, mfu 38.80%\n",
            "iter 4310: loss 3.9937, time 141.69ms, mfu 38.94%\n",
            "iter 4320: loss 4.0319, time 141.66ms, mfu 39.08%\n",
            "iter 4330: loss 4.0660, time 142.90ms, mfu 39.16%\n",
            "iter 4340: loss 3.9029, time 141.64ms, mfu 39.28%\n",
            "iter 4350: loss 3.9510, time 141.59ms, mfu 39.38%\n",
            "iter 4360: loss 3.9945, time 141.62ms, mfu 39.47%\n",
            "iter 4370: loss 3.9624, time 141.73ms, mfu 39.55%\n",
            "iter 4380: loss 3.9405, time 142.76ms, mfu 39.59%\n",
            "iter 4390: loss 4.0386, time 141.78ms, mfu 39.66%\n",
            "iter 4400: loss 4.0670, time 141.41ms, mfu 39.73%\n",
            "iter 4410: loss 3.8851, time 141.63ms, mfu 39.78%\n",
            "iter 4420: loss 3.8538, time 141.56ms, mfu 39.83%\n",
            "iter 4430: loss 4.0202, time 142.24ms, mfu 39.86%\n",
            "iter 4440: loss 3.9822, time 141.65ms, mfu 39.90%\n",
            "iter 4450: loss 3.9698, time 141.48ms, mfu 39.95%\n",
            "iter 4460: loss 3.8135, time 141.37ms, mfu 39.99%\n",
            "iter 4470: loss 4.0107, time 141.49ms, mfu 40.02%\n",
            "iter 4480: loss 3.8480, time 142.58ms, mfu 40.02%\n",
            "iter 4490: loss 3.9931, time 141.29ms, mfu 40.06%\n",
            "step 4500: train loss 3.9306, val loss 3.9167\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 4500: loss 3.8740, time 24122.38ms, mfu 36.08%\n",
            "iter 4510: loss 3.9811, time 142.41ms, mfu 36.47%\n",
            "iter 4520: loss 3.9429, time 141.98ms, mfu 36.85%\n",
            "iter 4530: loss 4.0298, time 141.63ms, mfu 37.19%\n",
            "iter 4540: loss 3.9253, time 141.54ms, mfu 37.50%\n",
            "iter 4550: loss 3.9098, time 141.38ms, mfu 37.79%\n",
            "iter 4560: loss 3.8400, time 141.33ms, mfu 38.05%\n",
            "iter 4570: loss 4.0019, time 141.71ms, mfu 38.27%\n",
            "iter 4580: loss 3.9201, time 141.71ms, mfu 38.47%\n",
            "iter 4590: loss 3.9287, time 141.83ms, mfu 38.64%\n",
            "iter 4600: loss 4.0434, time 141.71ms, mfu 38.81%\n",
            "iter 4610: loss 3.9469, time 141.27ms, mfu 38.96%\n",
            "iter 4620: loss 3.9631, time 141.65ms, mfu 39.10%\n",
            "iter 4630: loss 3.9964, time 141.37ms, mfu 39.22%\n",
            "iter 4640: loss 3.8777, time 141.61ms, mfu 39.33%\n",
            "iter 4650: loss 3.8751, time 142.19ms, mfu 39.41%\n",
            "iter 4660: loss 3.8546, time 141.47ms, mfu 39.50%\n",
            "iter 4670: loss 3.8633, time 141.42ms, mfu 39.59%\n",
            "iter 4680: loss 3.9082, time 141.45ms, mfu 39.66%\n",
            "iter 4690: loss 3.8757, time 141.56ms, mfu 39.73%\n",
            "iter 4700: loss 3.7913, time 142.44ms, mfu 39.76%\n",
            "iter 4710: loss 3.9942, time 141.58ms, mfu 39.81%\n",
            "iter 4720: loss 3.8949, time 141.77ms, mfu 39.86%\n",
            "iter 4730: loss 3.7943, time 141.42ms, mfu 39.91%\n",
            "iter 4740: loss 3.8720, time 141.40ms, mfu 39.95%\n",
            "iter 4750: loss 4.0156, time 142.55ms, mfu 39.96%\n",
            "iter 4760: loss 4.0054, time 141.81ms, mfu 39.99%\n",
            "iter 4770: loss 3.7780, time 141.56ms, mfu 40.02%\n",
            "iter 4780: loss 3.8172, time 141.23ms, mfu 40.06%\n",
            "iter 4790: loss 3.8924, time 142.68ms, mfu 40.05%\n",
            "step 4800: train loss 3.9159, val loss 3.9076\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 4800: loss 3.8502, time 24150.93ms, mfu 36.07%\n",
            "iter 4810: loss 3.9140, time 142.10ms, mfu 36.48%\n",
            "iter 4820: loss 3.9634, time 141.74ms, mfu 36.86%\n",
            "iter 4830: loss 3.9431, time 141.74ms, mfu 37.20%\n",
            "iter 4840: loss 3.9393, time 141.69ms, mfu 37.50%\n",
            "iter 4850: loss 3.9275, time 141.71ms, mfu 37.78%\n",
            "iter 4860: loss 3.8414, time 141.54ms, mfu 38.03%\n",
            "iter 4870: loss 3.9691, time 141.48ms, mfu 38.26%\n",
            "iter 4880: loss 3.9612, time 141.67ms, mfu 38.46%\n",
            "iter 4890: loss 3.8595, time 141.31ms, mfu 38.65%\n",
            "iter 4900: loss 3.8962, time 142.90ms, mfu 38.78%\n",
            "iter 4910: loss 3.9081, time 141.82ms, mfu 38.93%\n",
            "iter 4920: loss 3.9757, time 141.42ms, mfu 39.07%\n",
            "iter 4930: loss 3.9804, time 141.57ms, mfu 39.19%\n",
            "iter 4940: loss 3.9203, time 141.75ms, mfu 39.30%\n",
            "iter 4950: loss 3.8923, time 142.61ms, mfu 39.37%\n",
            "iter 4960: loss 3.8824, time 141.81ms, mfu 39.46%\n",
            "iter 4970: loss 3.9936, time 141.49ms, mfu 39.54%\n",
            "iter 4980: loss 3.9714, time 141.63ms, mfu 39.62%\n",
            "iter 4990: loss 3.8769, time 141.46ms, mfu 39.69%\n",
            "iter 5000: loss 3.8803, time 141.83ms, mfu 39.74%\n",
            "iter 5010: loss 3.8890, time 141.61ms, mfu 39.80%\n",
            "iter 5020: loss 3.9550, time 142.49ms, mfu 39.82%\n",
            "iter 5030: loss 3.9595, time 141.72ms, mfu 39.87%\n",
            "iter 5040: loss 3.9170, time 141.63ms, mfu 39.91%\n",
            "iter 5050: loss 4.0259, time 141.73ms, mfu 39.94%\n",
            "iter 5060: loss 3.9707, time 141.61ms, mfu 39.98%\n",
            "iter 5070: loss 3.7678, time 142.41ms, mfu 39.99%\n",
            "iter 5080: loss 3.9138, time 141.69ms, mfu 40.02%\n",
            "iter 5090: loss 4.0682, time 141.30ms, mfu 40.05%\n",
            "step 5100: train loss 3.9152, val loss 3.9014\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 5100: loss 3.9568, time 24146.16ms, mfu 36.07%\n",
            "iter 5110: loss 3.9196, time 141.29ms, mfu 36.50%\n",
            "iter 5120: loss 3.9313, time 141.41ms, mfu 36.89%\n",
            "iter 5130: loss 3.9001, time 141.34ms, mfu 37.24%\n",
            "iter 5140: loss 4.0128, time 141.53ms, mfu 37.54%\n",
            "iter 5150: loss 3.8701, time 141.65ms, mfu 37.82%\n",
            "iter 5160: loss 4.0719, time 141.73ms, mfu 38.06%\n",
            "iter 5170: loss 4.0451, time 141.59ms, mfu 38.29%\n",
            "iter 5180: loss 3.9344, time 141.91ms, mfu 38.48%\n",
            "iter 5190: loss 3.8735, time 141.60ms, mfu 38.66%\n",
            "iter 5200: loss 3.7307, time 141.50ms, mfu 38.83%\n",
            "iter 5210: loss 3.8249, time 141.52ms, mfu 38.98%\n",
            "iter 5220: loss 3.7595, time 141.69ms, mfu 39.10%\n",
            "iter 5230: loss 3.9478, time 141.53ms, mfu 39.23%\n",
            "iter 5240: loss 3.8629, time 142.45ms, mfu 39.31%\n",
            "iter 5250: loss 3.9175, time 141.63ms, mfu 39.41%\n",
            "iter 5260: loss 3.8818, time 141.37ms, mfu 39.50%\n",
            "iter 5270: loss 3.9962, time 141.64ms, mfu 39.58%\n",
            "iter 5280: loss 3.8613, time 142.71ms, mfu 39.62%\n",
            "iter 5290: loss 3.7452, time 141.59ms, mfu 39.69%\n",
            "iter 5300: loss 3.8205, time 141.41ms, mfu 39.75%\n",
            "iter 5310: loss 3.7353, time 141.64ms, mfu 39.81%\n",
            "iter 5320: loss 3.8459, time 142.35ms, mfu 39.84%\n",
            "iter 5330: loss 3.9875, time 141.68ms, mfu 39.88%\n",
            "iter 5340: loss 4.0057, time 141.37ms, mfu 39.93%\n",
            "iter 5350: loss 3.9313, time 141.57ms, mfu 39.96%\n",
            "iter 5360: loss 3.9898, time 142.08ms, mfu 39.98%\n",
            "iter 5370: loss 3.8935, time 142.59ms, mfu 39.99%\n",
            "iter 5380: loss 3.9779, time 141.39ms, mfu 40.02%\n",
            "iter 5390: loss 3.8546, time 141.56ms, mfu 40.05%\n",
            "step 5400: train loss 3.9095, val loss 3.9018\n",
            "iter 5400: loss 3.8497, time 22095.45ms, mfu 36.07%\n",
            "iter 5410: loss 4.0325, time 141.40ms, mfu 36.50%\n",
            "iter 5420: loss 3.8881, time 141.63ms, mfu 36.88%\n",
            "iter 5430: loss 3.8740, time 142.32ms, mfu 37.20%\n",
            "iter 5440: loss 3.8499, time 142.23ms, mfu 37.49%\n",
            "iter 5450: loss 3.8035, time 141.73ms, mfu 37.77%\n",
            "iter 5460: loss 3.9269, time 141.76ms, mfu 38.02%\n",
            "iter 5470: loss 4.0353, time 141.71ms, mfu 38.24%\n",
            "iter 5480: loss 3.8451, time 141.61ms, mfu 38.45%\n",
            "iter 5490: loss 3.9729, time 141.50ms, mfu 38.64%\n",
            "iter 5500: loss 3.9214, time 141.61ms, mfu 38.80%\n",
            "iter 5510: loss 4.0261, time 141.79ms, mfu 38.94%\n",
            "iter 5520: loss 3.9016, time 142.12ms, mfu 39.07%\n",
            "iter 5530: loss 3.8341, time 141.62ms, mfu 39.19%\n",
            "iter 5540: loss 3.8619, time 141.62ms, mfu 39.30%\n",
            "iter 5550: loss 3.8733, time 141.64ms, mfu 39.40%\n",
            "iter 5560: loss 3.9488, time 142.57ms, mfu 39.46%\n",
            "iter 5570: loss 4.0125, time 141.49ms, mfu 39.55%\n",
            "iter 5580: loss 3.9120, time 141.80ms, mfu 39.62%\n",
            "iter 5590: loss 3.9079, time 142.04ms, mfu 39.67%\n",
            "iter 5600: loss 4.0020, time 142.43ms, mfu 39.71%\n",
            "iter 5610: loss 3.8078, time 141.63ms, mfu 39.77%\n",
            "iter 5620: loss 3.9291, time 141.81ms, mfu 39.81%\n",
            "iter 5630: loss 3.8440, time 141.63ms, mfu 39.86%\n",
            "iter 5640: loss 4.0107, time 142.32ms, mfu 39.88%\n",
            "iter 5650: loss 4.0107, time 141.54ms, mfu 39.93%\n",
            "iter 5660: loss 3.9956, time 141.75ms, mfu 39.96%\n",
            "iter 5670: loss 3.9025, time 142.86ms, mfu 39.96%\n",
            "iter 5680: loss 3.9739, time 141.64ms, mfu 39.99%\n",
            "iter 5690: loss 3.9648, time 141.48ms, mfu 40.02%\n",
            "step 5700: train loss 3.9009, val loss 3.8968\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 5700: loss 3.9375, time 24145.84ms, mfu 36.05%\n",
            "iter 5710: loss 3.8678, time 141.48ms, mfu 36.47%\n",
            "iter 5720: loss 3.9669, time 142.11ms, mfu 36.84%\n",
            "iter 5730: loss 3.8683, time 141.62ms, mfu 37.19%\n",
            "iter 5740: loss 3.8966, time 141.57ms, mfu 37.50%\n",
            "iter 5750: loss 3.8809, time 141.53ms, mfu 37.78%\n",
            "iter 5760: loss 3.8565, time 141.63ms, mfu 38.03%\n",
            "iter 5770: loss 3.8776, time 141.60ms, mfu 38.26%\n",
            "iter 5780: loss 3.8330, time 141.64ms, mfu 38.46%\n",
            "iter 5790: loss 3.9219, time 141.71ms, mfu 38.64%\n",
            "iter 5800: loss 3.7147, time 141.54ms, mfu 38.81%\n",
            "iter 5810: loss 3.9317, time 141.56ms, mfu 38.96%\n",
            "iter 5820: loss 3.8934, time 141.67ms, mfu 39.09%\n",
            "iter 5830: loss 3.7822, time 141.59ms, mfu 39.21%\n",
            "iter 5840: loss 3.9841, time 142.53ms, mfu 39.29%\n",
            "iter 5850: loss 3.9113, time 141.58ms, mfu 39.39%\n",
            "iter 5860: loss 3.9260, time 141.50ms, mfu 39.49%\n",
            "iter 5870: loss 3.9005, time 141.21ms, mfu 39.58%\n",
            "iter 5880: loss 3.9688, time 141.39ms, mfu 39.66%\n",
            "iter 5890: loss 3.9760, time 141.51ms, mfu 39.72%\n",
            "iter 5900: loss 4.0182, time 142.37ms, mfu 39.76%\n",
            "iter 5910: loss 3.8809, time 141.65ms, mfu 39.81%\n",
            "iter 5920: loss 3.8732, time 141.69ms, mfu 39.86%\n",
            "iter 5930: loss 3.8398, time 141.61ms, mfu 39.90%\n",
            "iter 5940: loss 3.9560, time 141.27ms, mfu 39.95%\n",
            "iter 5950: loss 3.8495, time 141.63ms, mfu 39.98%\n",
            "iter 5960: loss 3.8214, time 142.68ms, mfu 39.98%\n",
            "iter 5970: loss 4.0010, time 141.65ms, mfu 40.01%\n",
            "iter 5980: loss 3.8761, time 141.50ms, mfu 40.05%\n",
            "iter 5990: loss 3.8543, time 141.37ms, mfu 40.08%\n",
            "step 6000: train loss 3.9024, val loss 3.8983\n",
            "iter 6000: loss 3.8352, time 22078.48ms, mfu 36.09%\n",
            "iter 6010: loss 3.8926, time 141.69ms, mfu 36.51%\n",
            "iter 6020: loss 3.9644, time 141.88ms, mfu 36.88%\n",
            "iter 6030: loss 4.0551, time 141.63ms, mfu 37.22%\n",
            "iter 6040: loss 3.8223, time 141.59ms, mfu 37.53%\n",
            "iter 6050: loss 3.8305, time 141.69ms, mfu 37.80%\n",
            "iter 6060: loss 3.9836, time 141.81ms, mfu 38.05%\n",
            "iter 6070: loss 3.9212, time 141.28ms, mfu 38.28%\n",
            "iter 6080: loss 3.7948, time 141.70ms, mfu 38.48%\n",
            "iter 6090: loss 3.7173, time 141.81ms, mfu 38.66%\n",
            "iter 6100: loss 3.8140, time 141.64ms, mfu 38.82%\n",
            "iter 6110: loss 3.9261, time 141.43ms, mfu 38.97%\n",
            "iter 6120: loss 3.9470, time 141.42ms, mfu 39.11%\n",
            "iter 6130: loss 3.8028, time 141.74ms, mfu 39.22%\n",
            "iter 6140: loss 3.8534, time 141.37ms, mfu 39.34%\n",
            "iter 6150: loss 3.8504, time 142.69ms, mfu 39.40%\n",
            "iter 6160: loss 3.8272, time 141.90ms, mfu 39.48%\n",
            "iter 6170: loss 3.9850, time 141.60ms, mfu 39.56%\n",
            "iter 6180: loss 3.9289, time 141.75ms, mfu 39.63%\n",
            "iter 6190: loss 3.8916, time 141.57ms, mfu 39.70%\n",
            "iter 6200: loss 3.8701, time 142.12ms, mfu 39.75%\n",
            "iter 6210: loss 3.8719, time 141.68ms, mfu 39.80%\n",
            "iter 6220: loss 3.8380, time 141.65ms, mfu 39.85%\n",
            "iter 6230: loss 3.8355, time 141.84ms, mfu 39.88%\n",
            "iter 6240: loss 3.8535, time 142.04ms, mfu 39.91%\n",
            "iter 6250: loss 3.9128, time 141.48ms, mfu 39.96%\n",
            "iter 6260: loss 3.8679, time 141.64ms, mfu 39.99%\n",
            "iter 6270: loss 3.8586, time 141.67ms, mfu 40.02%\n",
            "iter 6280: loss 3.8174, time 142.02ms, mfu 40.03%\n",
            "iter 6290: loss 3.8597, time 141.91ms, mfu 40.05%\n",
            "step 6300: train loss 3.8984, val loss 3.8966\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 6300: loss 3.8220, time 24116.03ms, mfu 36.07%\n",
            "iter 6310: loss 3.8478, time 142.12ms, mfu 36.48%\n",
            "iter 6320: loss 3.8345, time 141.98ms, mfu 36.85%\n",
            "iter 6330: loss 3.9481, time 142.23ms, mfu 37.18%\n",
            "iter 6340: loss 3.8824, time 141.99ms, mfu 37.48%\n",
            "iter 6350: loss 4.0072, time 141.71ms, mfu 37.75%\n",
            "iter 6360: loss 3.9333, time 141.65ms, mfu 38.01%\n",
            "iter 6370: loss 3.8522, time 141.62ms, mfu 38.24%\n",
            "iter 6380: loss 3.8683, time 141.57ms, mfu 38.44%\n",
            "iter 6390: loss 3.8525, time 141.55ms, mfu 38.63%\n",
            "iter 6400: loss 3.9977, time 141.70ms, mfu 38.79%\n",
            "iter 6410: loss 3.9728, time 141.63ms, mfu 38.94%\n",
            "iter 6420: loss 3.9540, time 141.63ms, mfu 39.08%\n",
            "iter 6430: loss 3.8501, time 141.52ms, mfu 39.20%\n",
            "iter 6440: loss 3.9956, time 141.42ms, mfu 39.32%\n",
            "iter 6450: loss 3.9120, time 141.28ms, mfu 39.42%\n",
            "iter 6460: loss 3.7722, time 142.64ms, mfu 39.48%\n",
            "iter 6470: loss 3.9387, time 141.50ms, mfu 39.57%\n",
            "iter 6480: loss 3.9061, time 141.40ms, mfu 39.64%\n",
            "iter 6490: loss 3.8882, time 141.67ms, mfu 39.71%\n",
            "iter 6500: loss 3.9379, time 141.25ms, mfu 39.78%\n",
            "iter 6510: loss 3.8962, time 141.58ms, mfu 39.83%\n",
            "iter 6520: loss 3.8800, time 142.41ms, mfu 39.85%\n",
            "iter 6530: loss 3.8855, time 141.66ms, mfu 39.90%\n",
            "iter 6540: loss 3.9075, time 141.56ms, mfu 39.94%\n",
            "iter 6550: loss 3.9876, time 141.30ms, mfu 39.98%\n",
            "iter 6560: loss 4.0195, time 141.75ms, mfu 40.01%\n",
            "iter 6570: loss 3.8329, time 142.59ms, mfu 40.01%\n",
            "iter 6580: loss 3.8674, time 141.63ms, mfu 40.04%\n",
            "iter 6590: loss 3.9586, time 141.42ms, mfu 40.07%\n",
            "step 6600: train loss 3.9013, val loss 3.9017\n",
            "iter 6600: loss 3.8402, time 22118.40ms, mfu 36.09%\n",
            "iter 6610: loss 3.8273, time 142.26ms, mfu 36.49%\n",
            "iter 6620: loss 3.8479, time 141.74ms, mfu 36.87%\n",
            "iter 6630: loss 3.8793, time 142.14ms, mfu 37.19%\n",
            "iter 6640: loss 3.8871, time 142.48ms, mfu 37.48%\n",
            "iter 6650: loss 3.9579, time 142.22ms, mfu 37.74%\n",
            "iter 6660: loss 3.8528, time 141.85ms, mfu 37.99%\n",
            "iter 6670: loss 3.8913, time 141.64ms, mfu 38.22%\n",
            "iter 6680: loss 3.9128, time 141.56ms, mfu 38.43%\n",
            "iter 6690: loss 3.7608, time 141.69ms, mfu 38.61%\n",
            "iter 6700: loss 3.9409, time 141.76ms, mfu 38.78%\n",
            "iter 6710: loss 3.8847, time 141.62ms, mfu 38.93%\n",
            "iter 6720: loss 3.8539, time 141.73ms, mfu 39.06%\n",
            "iter 6730: loss 3.8920, time 141.52ms, mfu 39.19%\n",
            "iter 6740: loss 3.8769, time 141.67ms, mfu 39.30%\n",
            "iter 6750: loss 3.9125, time 141.53ms, mfu 39.40%\n",
            "iter 6760: loss 3.8769, time 141.63ms, mfu 39.49%\n",
            "iter 6770: loss 3.9708, time 141.71ms, mfu 39.56%\n",
            "iter 6780: loss 3.9246, time 142.47ms, mfu 39.61%\n",
            "iter 6790: loss 3.8147, time 141.55ms, mfu 39.68%\n",
            "iter 6800: loss 3.9205, time 141.56ms, mfu 39.75%\n",
            "iter 6810: loss 3.8522, time 141.61ms, mfu 39.80%\n",
            "iter 6820: loss 3.8893, time 141.44ms, mfu 39.85%\n",
            "iter 6830: loss 3.7737, time 142.62ms, mfu 39.87%\n",
            "iter 6840: loss 3.9637, time 141.56ms, mfu 39.91%\n",
            "iter 6850: loss 3.9397, time 141.49ms, mfu 39.95%\n",
            "iter 6860: loss 3.8640, time 141.73ms, mfu 39.99%\n",
            "iter 6870: loss 3.9302, time 141.51ms, mfu 40.02%\n",
            "iter 6880: loss 3.8878, time 142.41ms, mfu 40.02%\n",
            "iter 6890: loss 3.8574, time 141.53ms, mfu 40.05%\n",
            "step 6900: train loss 3.9115, val loss 3.8932\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 6900: loss 3.9377, time 24119.55ms, mfu 36.07%\n",
            "iter 6910: loss 3.8789, time 142.18ms, mfu 36.48%\n",
            "iter 6920: loss 3.9914, time 141.77ms, mfu 36.85%\n",
            "iter 6930: loss 3.8753, time 141.64ms, mfu 37.20%\n",
            "iter 6940: loss 3.9515, time 141.95ms, mfu 37.50%\n",
            "iter 6950: loss 4.1238, time 142.04ms, mfu 37.76%\n",
            "iter 6960: loss 3.9145, time 141.88ms, mfu 38.01%\n",
            "iter 6970: loss 3.9585, time 141.65ms, mfu 38.24%\n",
            "iter 6980: loss 3.9453, time 141.77ms, mfu 38.44%\n",
            "iter 6990: loss 3.8167, time 141.73ms, mfu 38.62%\n",
            "iter 7000: loss 3.8908, time 141.76ms, mfu 38.78%\n",
            "iter 7010: loss 3.8490, time 141.59ms, mfu 38.94%\n",
            "iter 7020: loss 3.8820, time 141.55ms, mfu 39.07%\n",
            "iter 7030: loss 3.8395, time 141.46ms, mfu 39.20%\n",
            "iter 7040: loss 3.8174, time 141.84ms, mfu 39.30%\n",
            "iter 7050: loss 3.8515, time 141.56ms, mfu 39.40%\n",
            "iter 7060: loss 3.9598, time 141.69ms, mfu 39.49%\n",
            "iter 7070: loss 3.8710, time 141.35ms, mfu 39.58%\n",
            "iter 7080: loss 3.8467, time 142.09ms, mfu 39.64%\n",
            "iter 7090: loss 3.8534, time 141.48ms, mfu 39.70%\n",
            "iter 7100: loss 3.9303, time 141.69ms, mfu 39.76%\n",
            "iter 7110: loss 4.0808, time 142.55ms, mfu 39.79%\n",
            "iter 7120: loss 3.8537, time 141.71ms, mfu 39.84%\n",
            "iter 7130: loss 3.9370, time 141.51ms, mfu 39.88%\n",
            "iter 7140: loss 3.9194, time 141.67ms, mfu 39.92%\n",
            "iter 7150: loss 3.9041, time 141.63ms, mfu 39.96%\n",
            "iter 7160: loss 3.8645, time 142.41ms, mfu 39.97%\n",
            "iter 7170: loss 3.9710, time 141.71ms, mfu 40.00%\n",
            "iter 7180: loss 3.8853, time 141.59ms, mfu 40.03%\n",
            "iter 7190: loss 3.8500, time 141.57ms, mfu 40.06%\n",
            "step 7200: train loss 3.8991, val loss 3.8947\n",
            "iter 7200: loss 3.9873, time 22135.24ms, mfu 36.08%\n",
            "iter 7210: loss 3.8805, time 141.81ms, mfu 36.49%\n",
            "iter 7220: loss 3.8712, time 141.68ms, mfu 36.87%\n",
            "iter 7230: loss 3.8273, time 141.61ms, mfu 37.21%\n",
            "iter 7240: loss 3.9882, time 141.39ms, mfu 37.53%\n",
            "iter 7250: loss 3.8848, time 141.67ms, mfu 37.80%\n",
            "iter 7260: loss 3.9492, time 141.69ms, mfu 38.05%\n",
            "iter 7270: loss 3.8222, time 141.70ms, mfu 38.27%\n",
            "iter 7280: loss 3.9782, time 141.50ms, mfu 38.48%\n",
            "iter 7290: loss 3.9023, time 141.38ms, mfu 38.66%\n",
            "iter 7300: loss 3.8670, time 141.88ms, mfu 38.82%\n",
            "iter 7310: loss 3.8306, time 142.06ms, mfu 38.95%\n",
            "iter 7320: loss 3.8238, time 141.73ms, mfu 39.08%\n",
            "iter 7330: loss 3.9248, time 141.70ms, mfu 39.20%\n",
            "iter 7340: loss 3.8719, time 141.59ms, mfu 39.31%\n",
            "iter 7350: loss 3.9827, time 141.67ms, mfu 39.41%\n",
            "iter 7360: loss 3.8557, time 141.55ms, mfu 39.50%\n",
            "iter 7370: loss 4.0087, time 141.53ms, mfu 39.58%\n",
            "iter 7380: loss 3.9148, time 141.33ms, mfu 39.66%\n",
            "iter 7390: loss 3.9721, time 141.60ms, mfu 39.72%\n",
            "iter 7400: loss 3.9106, time 142.59ms, mfu 39.75%\n",
            "iter 7410: loss 3.9096, time 141.76ms, mfu 39.80%\n",
            "iter 7420: loss 3.8881, time 141.26ms, mfu 39.86%\n",
            "iter 7430: loss 3.8980, time 141.53ms, mfu 39.91%\n",
            "iter 7440: loss 3.9257, time 142.81ms, mfu 39.91%\n",
            "iter 7450: loss 3.9630, time 141.72ms, mfu 39.95%\n",
            "iter 7460: loss 3.9578, time 141.53ms, mfu 39.98%\n",
            "iter 7470: loss 4.0039, time 141.49ms, mfu 40.02%\n",
            "iter 7480: loss 3.8883, time 142.98ms, mfu 40.01%\n",
            "iter 7490: loss 4.0254, time 141.70ms, mfu 40.03%\n",
            "step 7500: train loss 3.8976, val loss 3.9012\n",
            "iter 7500: loss 3.8540, time 22114.01ms, mfu 36.06%\n",
            "iter 7510: loss 3.9439, time 141.76ms, mfu 36.47%\n",
            "iter 7520: loss 3.9555, time 141.64ms, mfu 36.86%\n",
            "iter 7530: loss 3.9767, time 141.45ms, mfu 37.20%\n",
            "iter 7540: loss 3.8689, time 141.73ms, mfu 37.51%\n",
            "iter 7550: loss 3.9232, time 141.64ms, mfu 37.79%\n",
            "iter 7560: loss 3.8665, time 142.89ms, mfu 38.00%\n",
            "iter 7570: loss 3.9167, time 141.82ms, mfu 38.22%\n",
            "iter 7580: loss 3.8456, time 141.80ms, mfu 38.43%\n",
            "iter 7590: loss 3.8856, time 141.39ms, mfu 38.62%\n",
            "iter 7600: loss 3.9217, time 141.88ms, mfu 38.78%\n",
            "iter 7610: loss 3.8012, time 141.80ms, mfu 38.92%\n",
            "iter 7620: loss 3.9796, time 141.73ms, mfu 39.06%\n",
            "iter 7630: loss 3.8042, time 141.79ms, mfu 39.18%\n",
            "iter 7640: loss 3.9548, time 141.59ms, mfu 39.29%\n",
            "iter 7650: loss 4.0466, time 141.44ms, mfu 39.39%\n",
            "iter 7660: loss 3.8947, time 141.68ms, mfu 39.48%\n",
            "iter 7670: loss 3.9231, time 141.68ms, mfu 39.56%\n",
            "iter 7680: loss 3.9244, time 141.67ms, mfu 39.63%\n",
            "iter 7690: loss 3.9086, time 141.80ms, mfu 39.69%\n",
            "iter 7700: loss 3.8027, time 141.41ms, mfu 39.76%\n",
            "iter 7710: loss 3.9589, time 141.79ms, mfu 39.81%\n",
            "iter 7720: loss 3.9098, time 142.86ms, mfu 39.82%\n",
            "iter 7730: loss 3.9757, time 141.60ms, mfu 39.87%\n",
            "iter 7740: loss 3.9044, time 141.69ms, mfu 39.91%\n",
            "iter 7750: loss 3.8212, time 142.77ms, mfu 39.91%\n",
            "iter 7760: loss 3.9671, time 141.50ms, mfu 39.96%\n",
            "iter 7770: loss 3.8319, time 141.57ms, mfu 39.99%\n",
            "iter 7780: loss 3.8424, time 141.39ms, mfu 40.03%\n",
            "iter 7790: loss 3.8912, time 141.70ms, mfu 40.05%\n",
            "step 7800: train loss 3.9002, val loss 3.8940\n",
            "iter 7800: loss 3.8534, time 22131.47ms, mfu 36.07%\n",
            "iter 7810: loss 3.8857, time 141.69ms, mfu 36.49%\n",
            "iter 7820: loss 3.8311, time 142.93ms, mfu 36.83%\n",
            "iter 7830: loss 3.8762, time 141.60ms, mfu 37.18%\n",
            "iter 7840: loss 3.8803, time 141.47ms, mfu 37.50%\n",
            "iter 7850: loss 3.8915, time 141.82ms, mfu 37.77%\n",
            "iter 7860: loss 3.7752, time 142.52ms, mfu 38.00%\n",
            "iter 7870: loss 3.9530, time 141.82ms, mfu 38.22%\n",
            "iter 7880: loss 4.0332, time 141.52ms, mfu 38.43%\n",
            "iter 7890: loss 3.9264, time 141.60ms, mfu 38.62%\n",
            "iter 7900: loss 3.8563, time 141.57ms, mfu 38.79%\n",
            "iter 7910: loss 3.8287, time 141.71ms, mfu 38.93%\n",
            "iter 7920: loss 3.9349, time 141.59ms, mfu 39.07%\n",
            "iter 7930: loss 3.8523, time 142.07ms, mfu 39.18%\n",
            "iter 7940: loss 3.8050, time 142.41ms, mfu 39.27%\n",
            "iter 7950: loss 3.8154, time 141.59ms, mfu 39.37%\n",
            "iter 7960: loss 3.9364, time 141.73ms, mfu 39.46%\n",
            "iter 7970: loss 4.0887, time 141.56ms, mfu 39.54%\n",
            "iter 7980: loss 3.9939, time 141.39ms, mfu 39.63%\n",
            "iter 7990: loss 3.8175, time 141.49ms, mfu 39.70%\n",
            "iter 8000: loss 3.8499, time 141.81ms, mfu 39.75%\n",
            "iter 8010: loss 3.9687, time 141.73ms, mfu 39.80%\n",
            "iter 8020: loss 3.7868, time 142.23ms, mfu 39.83%\n",
            "iter 8030: loss 3.9088, time 141.83ms, mfu 39.87%\n",
            "iter 8040: loss 3.9782, time 141.60ms, mfu 39.91%\n",
            "iter 8050: loss 3.8545, time 141.70ms, mfu 39.95%\n",
            "iter 8060: loss 3.8202, time 141.82ms, mfu 39.98%\n",
            "iter 8070: loss 3.9682, time 142.40ms, mfu 39.99%\n",
            "iter 8080: loss 4.0484, time 141.49ms, mfu 40.02%\n",
            "iter 8090: loss 4.0568, time 141.48ms, mfu 40.05%\n",
            "step 8100: train loss 3.8912, val loss 3.8943\n",
            "iter 8100: loss 3.9014, time 22154.48ms, mfu 36.07%\n",
            "iter 8110: loss 3.8961, time 141.77ms, mfu 36.49%\n",
            "iter 8120: loss 3.8834, time 141.56ms, mfu 36.87%\n",
            "iter 8130: loss 3.8856, time 141.43ms, mfu 37.22%\n",
            "iter 8140: loss 3.8610, time 141.59ms, mfu 37.53%\n",
            "iter 8150: loss 3.8521, time 142.68ms, mfu 37.77%\n",
            "iter 8160: loss 3.9664, time 141.72ms, mfu 38.02%\n",
            "iter 8170: loss 3.9261, time 141.48ms, mfu 38.25%\n",
            "iter 8180: loss 3.9767, time 141.61ms, mfu 38.46%\n",
            "iter 8190: loss 3.9619, time 141.38ms, mfu 38.65%\n",
            "iter 8200: loss 3.7475, time 142.62ms, mfu 38.78%\n",
            "iter 8210: loss 3.8001, time 141.82ms, mfu 38.93%\n",
            "iter 8220: loss 3.8827, time 141.67ms, mfu 39.06%\n",
            "iter 8230: loss 3.9786, time 141.65ms, mfu 39.19%\n",
            "iter 8240: loss 4.0417, time 141.68ms, mfu 39.29%\n",
            "iter 8250: loss 3.8608, time 142.38ms, mfu 39.37%\n",
            "iter 8260: loss 3.9385, time 142.12ms, mfu 39.45%\n",
            "iter 8270: loss 3.8431, time 141.76ms, mfu 39.53%\n",
            "iter 8280: loss 3.9186, time 141.62ms, mfu 39.61%\n",
            "iter 8290: loss 3.9085, time 141.92ms, mfu 39.67%\n",
            "iter 8300: loss 3.9774, time 141.79ms, mfu 39.72%\n",
            "iter 8310: loss 3.8360, time 141.58ms, mfu 39.78%\n",
            "iter 8320: loss 3.9279, time 141.74ms, mfu 39.83%\n",
            "iter 8330: loss 3.8140, time 141.50ms, mfu 39.88%\n",
            "iter 8340: loss 4.0212, time 141.49ms, mfu 39.92%\n",
            "iter 8350: loss 3.9848, time 141.47ms, mfu 39.96%\n",
            "iter 8360: loss 3.9814, time 141.78ms, mfu 39.99%\n",
            "iter 8370: loss 3.8417, time 142.48ms, mfu 40.00%\n",
            "iter 8380: loss 3.8319, time 141.61ms, mfu 40.03%\n",
            "iter 8390: loss 3.8898, time 141.31ms, mfu 40.06%\n",
            "step 8400: train loss 3.8918, val loss 3.8835\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 8400: loss 3.9592, time 24194.87ms, mfu 36.08%\n",
            "iter 8410: loss 3.9530, time 141.71ms, mfu 36.50%\n",
            "iter 8420: loss 3.9310, time 141.93ms, mfu 36.87%\n",
            "iter 8430: loss 3.8351, time 141.40ms, mfu 37.22%\n",
            "iter 8440: loss 3.9265, time 141.32ms, mfu 37.53%\n",
            "iter 8450: loss 3.7278, time 141.62ms, mfu 37.81%\n",
            "iter 8460: loss 3.9151, time 142.56ms, mfu 38.03%\n",
            "iter 8470: loss 3.9227, time 141.81ms, mfu 38.25%\n",
            "iter 8480: loss 3.8260, time 141.40ms, mfu 38.46%\n",
            "iter 8490: loss 3.8282, time 141.37ms, mfu 38.65%\n",
            "iter 8500: loss 3.9052, time 141.71ms, mfu 38.81%\n",
            "iter 8510: loss 3.9141, time 142.58ms, mfu 38.93%\n",
            "iter 8520: loss 3.9916, time 142.15ms, mfu 39.05%\n",
            "iter 8530: loss 3.8954, time 142.15ms, mfu 39.16%\n",
            "iter 8540: loss 3.9298, time 141.89ms, mfu 39.27%\n",
            "iter 8550: loss 3.9209, time 141.82ms, mfu 39.36%\n",
            "iter 8560: loss 3.8996, time 141.82ms, mfu 39.45%\n",
            "iter 8570: loss 3.8261, time 141.75ms, mfu 39.53%\n",
            "iter 8580: loss 3.9501, time 141.70ms, mfu 39.60%\n",
            "iter 8590: loss 3.9040, time 141.67ms, mfu 39.67%\n",
            "iter 8600: loss 3.9009, time 141.60ms, mfu 39.73%\n",
            "iter 8610: loss 3.8828, time 141.98ms, mfu 39.78%\n",
            "iter 8620: loss 3.8338, time 141.93ms, mfu 39.82%\n",
            "iter 8630: loss 3.7258, time 141.81ms, mfu 39.86%\n",
            "iter 8640: loss 4.0249, time 141.50ms, mfu 39.91%\n",
            "iter 8650: loss 3.9727, time 141.70ms, mfu 39.95%\n",
            "iter 8660: loss 3.9621, time 141.71ms, mfu 39.98%\n",
            "iter 8670: loss 3.9852, time 141.54ms, mfu 40.01%\n",
            "iter 8680: loss 3.8204, time 142.31ms, mfu 40.02%\n",
            "iter 8690: loss 3.8519, time 141.46ms, mfu 40.05%\n",
            "step 8700: train loss 3.8878, val loss 3.8908\n",
            "iter 8700: loss 3.8428, time 22118.65ms, mfu 36.07%\n",
            "iter 8710: loss 3.8777, time 141.62ms, mfu 36.49%\n",
            "iter 8720: loss 3.9381, time 141.39ms, mfu 36.88%\n",
            "iter 8730: loss 3.9771, time 141.54ms, mfu 37.22%\n",
            "iter 8740: loss 3.8818, time 141.83ms, mfu 37.52%\n",
            "iter 8750: loss 3.8280, time 142.36ms, mfu 37.78%\n",
            "iter 8760: loss 3.8901, time 141.42ms, mfu 38.04%\n",
            "iter 8770: loss 3.7353, time 141.48ms, mfu 38.27%\n",
            "iter 8780: loss 3.8678, time 141.64ms, mfu 38.47%\n",
            "iter 8790: loss 3.9618, time 142.16ms, mfu 38.63%\n",
            "iter 8800: loss 3.9461, time 142.09ms, mfu 38.79%\n",
            "iter 8810: loss 3.9257, time 141.41ms, mfu 38.94%\n",
            "iter 8820: loss 3.7944, time 141.84ms, mfu 39.07%\n",
            "iter 8830: loss 3.9909, time 141.91ms, mfu 39.19%\n",
            "iter 8840: loss 3.9200, time 142.49ms, mfu 39.27%\n",
            "iter 8850: loss 3.7434, time 142.02ms, mfu 39.36%\n",
            "iter 8860: loss 3.8342, time 141.64ms, mfu 39.45%\n",
            "iter 8870: loss 3.9064, time 142.06ms, mfu 39.53%\n",
            "iter 8880: loss 3.9513, time 141.96ms, mfu 39.59%\n",
            "iter 8890: loss 3.9822, time 141.62ms, mfu 39.66%\n",
            "iter 8900: loss 3.8707, time 141.78ms, mfu 39.72%\n",
            "iter 8910: loss 3.8921, time 141.56ms, mfu 39.78%\n",
            "iter 8920: loss 4.0031, time 141.54ms, mfu 39.83%\n",
            "iter 8930: loss 3.8635, time 141.54ms, mfu 39.88%\n",
            "iter 8940: loss 3.8973, time 141.53ms, mfu 39.92%\n",
            "iter 8950: loss 3.9059, time 141.80ms, mfu 39.96%\n",
            "iter 8960: loss 3.8752, time 142.11ms, mfu 39.98%\n",
            "iter 8970: loss 3.9180, time 142.20ms, mfu 39.99%\n",
            "iter 8980: loss 3.9483, time 141.53ms, mfu 40.02%\n",
            "iter 8990: loss 3.9001, time 141.71ms, mfu 40.05%\n",
            "step 9000: train loss 3.8881, val loss 3.8846\n",
            "iter 9000: loss 3.8238, time 22099.09ms, mfu 36.07%\n",
            "iter 9010: loss 3.8584, time 142.78ms, mfu 36.46%\n",
            "iter 9020: loss 3.8530, time 141.64ms, mfu 36.84%\n",
            "iter 9030: loss 3.9011, time 141.69ms, mfu 37.18%\n",
            "iter 9040: loss 3.9488, time 141.71ms, mfu 37.49%\n",
            "iter 9050: loss 3.8983, time 142.88ms, mfu 37.74%\n",
            "iter 9060: loss 3.8205, time 141.70ms, mfu 37.99%\n",
            "iter 9070: loss 3.7168, time 141.64ms, mfu 38.22%\n",
            "iter 9080: loss 3.8310, time 141.61ms, mfu 38.43%\n",
            "iter 9090: loss 3.9586, time 141.65ms, mfu 38.61%\n",
            "iter 9100: loss 3.8387, time 142.36ms, mfu 38.76%\n",
            "iter 9110: loss 3.8074, time 141.62ms, mfu 38.91%\n",
            "iter 9120: loss 3.8457, time 141.55ms, mfu 39.05%\n",
            "iter 9130: loss 3.9070, time 141.72ms, mfu 39.17%\n",
            "iter 9140: loss 3.9270, time 141.47ms, mfu 39.29%\n",
            "iter 9150: loss 3.8968, time 142.65ms, mfu 39.36%\n",
            "iter 9160: loss 3.8575, time 141.76ms, mfu 39.45%\n",
            "iter 9170: loss 3.8003, time 141.99ms, mfu 39.52%\n",
            "iter 9180: loss 3.9328, time 141.90ms, mfu 39.59%\n",
            "iter 9190: loss 3.9887, time 142.17ms, mfu 39.65%\n",
            "iter 9200: loss 3.8784, time 141.87ms, mfu 39.70%\n",
            "iter 9210: loss 3.9159, time 141.83ms, mfu 39.76%\n",
            "iter 9220: loss 3.9502, time 141.89ms, mfu 39.80%\n",
            "iter 9230: loss 3.7597, time 141.60ms, mfu 39.85%\n",
            "iter 9240: loss 3.9094, time 141.59ms, mfu 39.90%\n",
            "iter 9250: loss 3.8450, time 141.63ms, mfu 39.94%\n",
            "iter 9260: loss 3.8951, time 141.77ms, mfu 39.97%\n",
            "iter 9270: loss 3.9472, time 141.53ms, mfu 40.00%\n",
            "iter 9280: loss 3.9049, time 141.78ms, mfu 40.03%\n",
            "iter 9290: loss 3.9557, time 141.74ms, mfu 40.05%\n",
            "step 9300: train loss 3.8857, val loss 3.8808\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 9300: loss 3.8687, time 24132.59ms, mfu 36.07%\n",
            "iter 9310: loss 3.9151, time 141.30ms, mfu 36.50%\n",
            "iter 9320: loss 3.7980, time 141.75ms, mfu 36.87%\n",
            "iter 9330: loss 3.9078, time 141.56ms, mfu 37.22%\n",
            "iter 9340: loss 3.9569, time 141.62ms, mfu 37.52%\n",
            "iter 9350: loss 3.9147, time 141.65ms, mfu 37.80%\n",
            "iter 9360: loss 3.9290, time 141.38ms, mfu 38.06%\n",
            "iter 9370: loss 3.7539, time 142.99ms, mfu 38.24%\n",
            "iter 9380: loss 3.8507, time 141.57ms, mfu 38.45%\n",
            "iter 9390: loss 3.8756, time 141.49ms, mfu 38.64%\n",
            "iter 9400: loss 3.8193, time 141.45ms, mfu 38.81%\n",
            "iter 9410: loss 3.8600, time 141.71ms, mfu 38.95%\n",
            "iter 9420: loss 3.8213, time 141.57ms, mfu 39.09%\n",
            "iter 9430: loss 3.8991, time 142.76ms, mfu 39.18%\n",
            "iter 9440: loss 3.8960, time 141.64ms, mfu 39.29%\n",
            "iter 9450: loss 3.9020, time 141.55ms, mfu 39.39%\n",
            "iter 9460: loss 3.9823, time 141.57ms, mfu 39.48%\n",
            "iter 9470: loss 3.8937, time 141.56ms, mfu 39.56%\n",
            "iter 9480: loss 3.8496, time 141.69ms, mfu 39.63%\n",
            "iter 9490: loss 3.8705, time 141.95ms, mfu 39.69%\n",
            "iter 9500: loss 3.9568, time 141.68ms, mfu 39.75%\n",
            "iter 9510: loss 3.8794, time 142.04ms, mfu 39.79%\n",
            "iter 9520: loss 3.9052, time 141.77ms, mfu 39.84%\n",
            "iter 9530: loss 4.0128, time 141.78ms, mfu 39.88%\n",
            "iter 9540: loss 3.8434, time 141.82ms, mfu 39.91%\n",
            "iter 9550: loss 3.7798, time 141.72ms, mfu 39.95%\n",
            "iter 9560: loss 3.9162, time 141.73ms, mfu 39.98%\n",
            "iter 9570: loss 3.9156, time 141.44ms, mfu 40.01%\n",
            "iter 9580: loss 3.9292, time 141.66ms, mfu 40.04%\n",
            "iter 9590: loss 3.7347, time 141.54ms, mfu 40.07%\n",
            "step 9600: train loss 3.8878, val loss 3.8840\n",
            "iter 9600: loss 3.9639, time 22108.06ms, mfu 36.09%\n",
            "iter 9610: loss 3.9596, time 143.33ms, mfu 36.46%\n",
            "iter 9620: loss 3.9528, time 141.66ms, mfu 36.84%\n",
            "iter 9630: loss 3.9565, time 141.45ms, mfu 37.19%\n",
            "iter 9640: loss 3.8190, time 141.75ms, mfu 37.50%\n",
            "iter 9650: loss 3.7637, time 143.04ms, mfu 37.74%\n",
            "iter 9660: loss 3.9503, time 141.61ms, mfu 37.99%\n",
            "iter 9670: loss 3.9260, time 141.47ms, mfu 38.23%\n",
            "iter 9680: loss 3.9136, time 141.57ms, mfu 38.43%\n",
            "iter 9690: loss 3.8643, time 142.15ms, mfu 38.61%\n",
            "iter 9700: loss 3.8096, time 141.68ms, mfu 38.77%\n",
            "iter 9710: loss 3.8995, time 141.44ms, mfu 38.93%\n",
            "iter 9720: loss 3.8548, time 141.79ms, mfu 39.06%\n",
            "iter 9730: loss 3.8781, time 141.57ms, mfu 39.18%\n",
            "iter 9740: loss 3.9926, time 142.13ms, mfu 39.28%\n",
            "iter 9750: loss 3.8413, time 141.45ms, mfu 39.39%\n",
            "iter 9760: loss 3.9497, time 141.41ms, mfu 39.48%\n",
            "iter 9770: loss 3.9835, time 141.65ms, mfu 39.56%\n",
            "iter 9780: loss 3.8246, time 141.57ms, mfu 39.64%\n",
            "iter 9790: loss 3.8291, time 142.29ms, mfu 39.68%\n",
            "iter 9800: loss 3.8938, time 141.84ms, mfu 39.74%\n",
            "iter 9810: loss 3.7725, time 141.83ms, mfu 39.79%\n",
            "iter 9820: loss 3.8576, time 141.80ms, mfu 39.83%\n",
            "iter 9830: loss 3.9459, time 141.97ms, mfu 39.87%\n",
            "iter 9840: loss 3.8507, time 142.17ms, mfu 39.89%\n",
            "iter 9850: loss 3.9149, time 141.74ms, mfu 39.93%\n",
            "iter 9860: loss 3.9178, time 141.94ms, mfu 39.96%\n",
            "iter 9870: loss 3.9414, time 141.57ms, mfu 39.99%\n",
            "iter 9880: loss 3.9182, time 141.69ms, mfu 40.02%\n",
            "iter 9890: loss 3.9403, time 141.81ms, mfu 40.04%\n",
            "step 9900: train loss 3.9002, val loss 3.8806\n",
            "saving checkpoint to content/SimpleGPT\n",
            "iter 9900: loss 3.8108, time 24158.67ms, mfu 36.06%\n",
            "iter 9910: loss 3.9303, time 142.04ms, mfu 36.47%\n",
            "iter 9920: loss 3.9471, time 141.55ms, mfu 36.86%\n",
            "iter 9930: loss 3.8822, time 141.81ms, mfu 37.19%\n",
            "iter 9940: loss 3.7788, time 141.59ms, mfu 37.50%\n",
            "iter 9950: loss 3.8065, time 141.62ms, mfu 37.78%\n",
            "iter 9960: loss 3.8550, time 142.44ms, mfu 38.01%\n",
            "iter 9970: loss 3.8723, time 141.63ms, mfu 38.24%\n",
            "iter 9980: loss 3.9468, time 141.47ms, mfu 38.45%\n",
            "iter 9990: loss 4.0133, time 141.68ms, mfu 38.63%\n",
            "iter 10000: loss 3.8704, time 141.28ms, mfu 38.81%\n"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5XyGS6NKgjwn",
        "outputId": "83c40522-06a3-4a1a-d86d-a0c5f4b342cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:m0t6fwba) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▆███▇▇▆▅▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mfu</td><td>▁█████████████████████████████████</td></tr><tr><td>train/loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>█▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>9900</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>mfu</td><td>40.04216</td></tr><tr><td>train/loss</td><td>3.90016</td></tr><tr><td>val/loss</td><td>3.88064</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">SA:CA-1h:CA-2h=2:2:2-simpleTransformer-1024-768</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/m0t6fwba' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/m0t6fwba</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241123_234904-m0t6fwba/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:m0t6fwba). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241124_002823-bow8ylpp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/bow8ylpp' target=\"_blank\">SA:CA-1h:CA-2h=2:2:2-simpleTransformer-1024-768</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/bow8ylpp' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/bow8ylpp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.9497, val loss 10.9499\n",
            "iter 0: loss 10.9504, time 24529.40ms, mfu -100.00%\n",
            "iter 10: loss 9.6326, time 163.59ms, mfu 41.54%\n",
            "iter 20: loss 9.2106, time 163.59ms, mfu 41.54%\n",
            "iter 30: loss 8.5639, time 164.18ms, mfu 41.53%\n",
            "iter 40: loss 8.0578, time 164.49ms, mfu 41.51%\n",
            "iter 50: loss 7.3758, time 163.44ms, mfu 41.52%\n",
            "iter 60: loss 7.2529, time 163.71ms, mfu 41.52%\n",
            "iter 70: loss 7.2918, time 163.93ms, mfu 41.51%\n",
            "iter 80: loss 7.0256, time 164.93ms, mfu 41.48%\n",
            "iter 90: loss 6.8288, time 164.12ms, mfu 41.47%\n",
            "iter 100: loss 6.7967, time 163.78ms, mfu 41.48%\n",
            "iter 110: loss 6.5470, time 164.88ms, mfu 41.45%\n",
            "iter 120: loss 6.5472, time 163.97ms, mfu 41.45%\n",
            "iter 130: loss 6.4576, time 165.01ms, mfu 41.42%\n",
            "iter 140: loss 6.4775, time 163.89ms, mfu 41.43%\n",
            "iter 150: loss 6.3820, time 164.79ms, mfu 41.41%\n",
            "iter 160: loss 6.4546, time 163.87ms, mfu 41.42%\n",
            "iter 170: loss 6.2390, time 164.58ms, mfu 41.40%\n",
            "iter 180: loss 6.4073, time 164.10ms, mfu 41.41%\n",
            "iter 190: loss 6.1933, time 164.78ms, mfu 41.39%\n",
            "iter 200: loss 6.2962, time 164.07ms, mfu 41.39%\n",
            "iter 210: loss 6.2113, time 164.56ms, mfu 41.38%\n",
            "iter 220: loss 6.3212, time 164.18ms, mfu 41.38%\n",
            "iter 230: loss 6.0993, time 164.82ms, mfu 41.37%\n",
            "iter 240: loss 6.0275, time 164.07ms, mfu 41.38%\n",
            "iter 250: loss 6.2215, time 164.38ms, mfu 41.37%\n",
            "iter 260: loss 6.1231, time 165.40ms, mfu 41.34%\n",
            "iter 270: loss 6.1835, time 164.28ms, mfu 41.35%\n",
            "iter 280: loss 6.1721, time 164.50ms, mfu 41.34%\n",
            "iter 290: loss 5.9720, time 165.03ms, mfu 41.33%\n",
            "step 300: train loss 6.0393, val loss 5.9931\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 300: loss 5.9831, time 26671.48ms, mfu 37.22%\n",
            "iter 310: loss 5.9607, time 165.00ms, mfu 37.62%\n",
            "iter 320: loss 5.9146, time 164.32ms, mfu 37.99%\n",
            "iter 330: loss 6.0488, time 165.30ms, mfu 38.30%\n",
            "iter 340: loss 5.8955, time 164.15ms, mfu 38.61%\n",
            "iter 350: loss 5.8621, time 165.42ms, mfu 38.86%\n",
            "iter 360: loss 5.9763, time 164.58ms, mfu 39.10%\n",
            "iter 370: loss 5.9530, time 164.10ms, mfu 39.34%\n",
            "iter 380: loss 5.9794, time 165.18ms, mfu 39.52%\n",
            "iter 390: loss 5.9305, time 164.10ms, mfu 39.71%\n",
            "iter 400: loss 5.8446, time 165.14ms, mfu 39.85%\n",
            "iter 410: loss 6.1171, time 164.26ms, mfu 40.00%\n",
            "iter 420: loss 5.9633, time 165.16ms, mfu 40.12%\n",
            "iter 430: loss 5.8772, time 163.92ms, mfu 40.25%\n",
            "iter 440: loss 5.9273, time 164.89ms, mfu 40.35%\n",
            "iter 450: loss 5.9840, time 164.09ms, mfu 40.46%\n",
            "iter 460: loss 5.8102, time 165.17ms, mfu 40.53%\n",
            "iter 470: loss 5.8237, time 163.97ms, mfu 40.62%\n",
            "iter 480: loss 5.8086, time 165.16ms, mfu 40.67%\n",
            "iter 490: loss 5.7672, time 164.08ms, mfu 40.75%\n",
            "iter 500: loss 5.6794, time 165.17ms, mfu 40.79%\n",
            "iter 510: loss 5.6066, time 164.26ms, mfu 40.84%\n",
            "iter 520: loss 5.6878, time 164.79ms, mfu 40.88%\n",
            "iter 530: loss 5.4748, time 165.00ms, mfu 40.92%\n",
            "iter 540: loss 5.6937, time 164.43ms, mfu 40.96%\n",
            "iter 550: loss 5.5395, time 164.37ms, mfu 41.00%\n",
            "iter 560: loss 5.5998, time 165.22ms, mfu 41.01%\n",
            "iter 570: loss 5.7736, time 164.20ms, mfu 41.05%\n",
            "iter 580: loss 5.4215, time 165.05ms, mfu 41.06%\n",
            "iter 590: loss 5.3921, time 164.34ms, mfu 41.09%\n",
            "step 600: train loss 5.5453, val loss 5.5025\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 600: loss 5.6018, time 27079.11ms, mfu 37.01%\n",
            "iter 610: loss 5.5747, time 164.51ms, mfu 37.44%\n",
            "iter 620: loss 5.4162, time 164.42ms, mfu 37.83%\n",
            "iter 630: loss 5.3952, time 165.10ms, mfu 38.16%\n",
            "iter 640: loss 5.5181, time 164.72ms, mfu 38.47%\n",
            "iter 650: loss 5.5459, time 164.44ms, mfu 38.76%\n",
            "iter 660: loss 5.6132, time 165.39ms, mfu 38.99%\n",
            "iter 670: loss 5.3516, time 164.59ms, mfu 39.22%\n",
            "iter 680: loss 5.5186, time 164.32ms, mfu 39.43%\n",
            "iter 690: loss 5.4940, time 165.11ms, mfu 39.61%\n",
            "iter 700: loss 5.3271, time 164.44ms, mfu 39.78%\n",
            "iter 710: loss 5.4101, time 165.17ms, mfu 39.92%\n",
            "iter 720: loss 5.3108, time 165.21ms, mfu 40.04%\n",
            "iter 730: loss 5.4043, time 164.49ms, mfu 40.17%\n",
            "iter 740: loss 5.4633, time 165.23ms, mfu 40.26%\n",
            "iter 750: loss 5.3390, time 164.54ms, mfu 40.37%\n",
            "iter 760: loss 5.4859, time 164.65ms, mfu 40.46%\n",
            "iter 770: loss 5.2748, time 165.23ms, mfu 40.53%\n",
            "iter 780: loss 5.3238, time 164.25ms, mfu 40.61%\n",
            "iter 790: loss 5.3334, time 164.61ms, mfu 40.68%\n",
            "iter 800: loss 5.3241, time 165.12ms, mfu 40.73%\n",
            "iter 810: loss 5.1849, time 164.64ms, mfu 40.78%\n",
            "iter 820: loss 5.1699, time 164.66ms, mfu 40.83%\n",
            "iter 830: loss 5.2082, time 164.51ms, mfu 40.88%\n",
            "iter 840: loss 5.2432, time 164.51ms, mfu 40.92%\n",
            "iter 850: loss 5.0222, time 165.46ms, mfu 40.94%\n",
            "iter 860: loss 5.0808, time 164.70ms, mfu 40.97%\n",
            "iter 870: loss 5.3846, time 164.64ms, mfu 41.00%\n",
            "iter 880: loss 5.1716, time 165.42ms, mfu 41.01%\n",
            "iter 890: loss 5.2557, time 164.58ms, mfu 41.04%\n",
            "step 900: train loss 5.1509, val loss 5.1125\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 900: loss 5.1014, time 27081.49ms, mfu 36.96%\n",
            "iter 910: loss 5.0978, time 165.34ms, mfu 37.37%\n",
            "iter 920: loss 5.2556, time 164.36ms, mfu 37.77%\n",
            "iter 930: loss 5.0611, time 165.40ms, mfu 38.10%\n",
            "iter 940: loss 5.0878, time 164.60ms, mfu 38.42%\n",
            "iter 950: loss 5.1803, time 165.14ms, mfu 38.70%\n",
            "iter 960: loss 5.0413, time 165.40ms, mfu 38.94%\n",
            "iter 970: loss 5.0372, time 164.38ms, mfu 39.18%\n",
            "iter 980: loss 5.0946, time 165.15ms, mfu 39.37%\n",
            "iter 990: loss 5.1326, time 164.54ms, mfu 39.57%\n",
            "iter 1000: loss 5.0343, time 165.58ms, mfu 39.71%\n",
            "iter 1010: loss 4.9981, time 164.67ms, mfu 39.87%\n",
            "iter 1020: loss 4.9252, time 164.46ms, mfu 40.02%\n",
            "iter 1030: loss 5.0720, time 165.31ms, mfu 40.13%\n",
            "iter 1040: loss 4.9070, time 164.56ms, mfu 40.24%\n",
            "iter 1050: loss 5.0479, time 164.53ms, mfu 40.35%\n",
            "iter 1060: loss 4.9925, time 164.47ms, mfu 40.45%\n",
            "iter 1070: loss 5.0041, time 165.35ms, mfu 40.51%\n",
            "iter 1080: loss 4.9489, time 164.53ms, mfu 40.59%\n",
            "iter 1090: loss 5.1126, time 165.24ms, mfu 40.65%\n",
            "iter 1100: loss 4.9110, time 164.50ms, mfu 40.71%\n",
            "iter 1110: loss 5.0452, time 164.32ms, mfu 40.78%\n",
            "iter 1120: loss 4.9926, time 165.34ms, mfu 40.81%\n",
            "iter 1130: loss 4.8822, time 164.66ms, mfu 40.86%\n",
            "iter 1140: loss 4.9649, time 164.20ms, mfu 40.91%\n",
            "iter 1150: loss 5.0496, time 165.14ms, mfu 40.94%\n",
            "iter 1160: loss 4.8063, time 164.17ms, mfu 40.98%\n",
            "iter 1170: loss 4.9752, time 165.22ms, mfu 41.00%\n",
            "iter 1180: loss 4.7611, time 165.10ms, mfu 41.01%\n",
            "iter 1190: loss 4.8377, time 164.38ms, mfu 41.05%\n",
            "step 1200: train loss 4.8690, val loss 4.8282\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 1200: loss 4.8595, time 27387.97ms, mfu 36.97%\n",
            "iter 1210: loss 4.7830, time 164.64ms, mfu 37.40%\n",
            "iter 1220: loss 4.8466, time 164.80ms, mfu 37.78%\n",
            "iter 1230: loss 4.8120, time 165.21ms, mfu 38.12%\n",
            "iter 1240: loss 4.8061, time 165.25ms, mfu 38.42%\n",
            "iter 1250: loss 4.7534, time 164.46ms, mfu 38.71%\n",
            "iter 1260: loss 4.8398, time 165.95ms, mfu 38.93%\n",
            "iter 1270: loss 4.7965, time 164.42ms, mfu 39.17%\n",
            "iter 1280: loss 4.7593, time 164.31ms, mfu 39.39%\n",
            "iter 1290: loss 4.7689, time 165.33ms, mfu 39.56%\n",
            "iter 1300: loss 4.7665, time 164.23ms, mfu 39.75%\n",
            "iter 1310: loss 4.6669, time 165.02ms, mfu 39.89%\n",
            "iter 1320: loss 4.6895, time 164.42ms, mfu 40.03%\n",
            "iter 1330: loss 4.6813, time 165.09ms, mfu 40.15%\n",
            "iter 1340: loss 4.6237, time 164.19ms, mfu 40.27%\n",
            "iter 1350: loss 4.8100, time 165.29ms, mfu 40.36%\n",
            "iter 1360: loss 4.7492, time 164.50ms, mfu 40.45%\n",
            "iter 1370: loss 4.7548, time 164.86ms, mfu 40.53%\n",
            "iter 1380: loss 4.6702, time 164.21ms, mfu 40.62%\n",
            "iter 1390: loss 4.6123, time 164.62ms, mfu 40.68%\n",
            "iter 1400: loss 4.8039, time 165.25ms, mfu 40.73%\n",
            "iter 1410: loss 4.7619, time 164.27ms, mfu 40.79%\n",
            "iter 1420: loss 4.7469, time 165.47ms, mfu 40.82%\n",
            "iter 1430: loss 4.5426, time 164.74ms, mfu 40.86%\n",
            "iter 1440: loss 4.6441, time 164.33ms, mfu 40.91%\n",
            "iter 1450: loss 4.6971, time 165.44ms, mfu 40.93%\n",
            "iter 1460: loss 4.5895, time 164.54ms, mfu 40.97%\n",
            "iter 1470: loss 4.6078, time 164.08ms, mfu 41.01%\n",
            "iter 1480: loss 4.7363, time 165.12ms, mfu 41.03%\n",
            "iter 1490: loss 4.7321, time 164.42ms, mfu 41.06%\n",
            "step 1500: train loss 4.6306, val loss 4.5925\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 1500: loss 4.6224, time 27377.19ms, mfu 36.98%\n",
            "iter 1510: loss 4.5589, time 164.99ms, mfu 37.40%\n",
            "iter 1520: loss 4.6803, time 165.33ms, mfu 37.77%\n",
            "iter 1530: loss 4.3549, time 164.52ms, mfu 38.12%\n",
            "iter 1540: loss 4.5682, time 165.34ms, mfu 38.42%\n",
            "iter 1550: loss 4.6188, time 164.77ms, mfu 38.70%\n",
            "iter 1560: loss 4.6377, time 165.10ms, mfu 38.95%\n",
            "iter 1570: loss 4.6900, time 164.51ms, mfu 39.19%\n",
            "iter 1580: loss 4.5052, time 164.87ms, mfu 39.39%\n",
            "iter 1590: loss 4.5779, time 165.25ms, mfu 39.56%\n",
            "iter 1600: loss 4.5040, time 164.54ms, mfu 39.74%\n",
            "iter 1610: loss 4.6175, time 165.24ms, mfu 39.88%\n",
            "iter 1620: loss 4.4759, time 165.39ms, mfu 40.00%\n",
            "iter 1630: loss 4.5287, time 164.73ms, mfu 40.12%\n",
            "iter 1640: loss 4.5203, time 165.19ms, mfu 40.23%\n",
            "iter 1650: loss 4.5810, time 165.28ms, mfu 40.32%\n",
            "iter 1660: loss 4.5031, time 164.36ms, mfu 40.42%\n",
            "iter 1670: loss 4.6087, time 165.40ms, mfu 40.49%\n",
            "iter 1680: loss 4.4865, time 164.81ms, mfu 40.56%\n",
            "iter 1690: loss 4.4604, time 164.49ms, mfu 40.64%\n",
            "iter 1700: loss 4.4926, time 165.54ms, mfu 40.68%\n",
            "iter 1710: loss 4.6091, time 164.60ms, mfu 40.74%\n",
            "iter 1720: loss 4.4102, time 164.28ms, mfu 40.80%\n",
            "iter 1730: loss 4.4270, time 165.14ms, mfu 40.84%\n",
            "iter 1740: loss 4.5199, time 165.09ms, mfu 40.87%\n",
            "iter 1750: loss 4.5154, time 164.38ms, mfu 40.92%\n",
            "iter 1760: loss 4.4569, time 165.26ms, mfu 40.94%\n",
            "iter 1770: loss 4.4580, time 164.70ms, mfu 40.97%\n",
            "iter 1780: loss 4.4951, time 164.29ms, mfu 41.01%\n",
            "iter 1790: loss 4.3569, time 165.37ms, mfu 41.02%\n",
            "step 1800: train loss 4.4803, val loss 4.4545\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 1800: loss 4.4532, time 27100.43ms, mfu 36.94%\n",
            "iter 1810: loss 4.4339, time 164.36ms, mfu 37.38%\n",
            "iter 1820: loss 4.4987, time 165.32ms, mfu 37.76%\n",
            "iter 1830: loss 4.5211, time 164.62ms, mfu 38.11%\n",
            "iter 1840: loss 4.5502, time 164.84ms, mfu 38.42%\n",
            "iter 1850: loss 4.4839, time 165.52ms, mfu 38.69%\n",
            "iter 1860: loss 4.5183, time 164.64ms, mfu 38.95%\n",
            "iter 1870: loss 4.4324, time 164.59ms, mfu 39.18%\n",
            "iter 1880: loss 4.4402, time 165.44ms, mfu 39.37%\n",
            "iter 1890: loss 4.5169, time 164.63ms, mfu 39.56%\n",
            "iter 1900: loss 4.4785, time 164.60ms, mfu 39.73%\n",
            "iter 1910: loss 4.3557, time 165.27ms, mfu 39.87%\n",
            "iter 1920: loss 4.4786, time 164.46ms, mfu 40.02%\n",
            "iter 1930: loss 4.3530, time 165.07ms, mfu 40.13%\n",
            "iter 1940: loss 4.3868, time 165.43ms, mfu 40.23%\n",
            "iter 1950: loss 4.4683, time 164.43ms, mfu 40.34%\n",
            "iter 1960: loss 4.4749, time 165.06ms, mfu 40.42%\n",
            "iter 1970: loss 4.2981, time 165.14ms, mfu 40.50%\n",
            "iter 1980: loss 4.4775, time 164.52ms, mfu 40.58%\n",
            "iter 1990: loss 4.2891, time 165.23ms, mfu 40.63%\n",
            "iter 2000: loss 4.4350, time 164.74ms, mfu 40.70%\n",
            "iter 2010: loss 4.3398, time 164.34ms, mfu 40.76%\n",
            "iter 2020: loss 4.3974, time 165.14ms, mfu 40.80%\n",
            "iter 2030: loss 4.3897, time 164.44ms, mfu 40.85%\n",
            "iter 2040: loss 4.3390, time 164.33ms, mfu 40.90%\n",
            "iter 2050: loss 4.5066, time 165.44ms, mfu 40.92%\n",
            "iter 2060: loss 4.3152, time 164.47ms, mfu 40.96%\n",
            "iter 2070: loss 4.3652, time 164.66ms, mfu 40.99%\n",
            "iter 2080: loss 4.4348, time 165.45ms, mfu 41.00%\n",
            "iter 2090: loss 4.4149, time 164.33ms, mfu 41.04%\n",
            "step 2100: train loss 4.3476, val loss 4.3318\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 2100: loss 4.3753, time 27109.58ms, mfu 36.96%\n",
            "iter 2110: loss 4.3261, time 165.08ms, mfu 37.38%\n",
            "iter 2120: loss 4.4478, time 164.41ms, mfu 37.78%\n",
            "iter 2130: loss 4.2322, time 165.25ms, mfu 38.11%\n",
            "iter 2140: loss 4.3809, time 165.43ms, mfu 38.41%\n",
            "iter 2150: loss 4.3280, time 164.42ms, mfu 38.70%\n",
            "iter 2160: loss 4.3617, time 165.12ms, mfu 38.95%\n",
            "iter 2170: loss 4.2775, time 164.45ms, mfu 39.18%\n",
            "iter 2180: loss 4.3576, time 164.40ms, mfu 39.40%\n",
            "iter 2190: loss 4.3149, time 165.31ms, mfu 39.57%\n",
            "iter 2200: loss 4.3527, time 164.38ms, mfu 39.75%\n",
            "iter 2210: loss 4.2884, time 164.64ms, mfu 39.90%\n",
            "iter 2220: loss 4.3527, time 165.08ms, mfu 40.03%\n",
            "iter 2230: loss 4.2572, time 164.47ms, mfu 40.16%\n",
            "iter 2240: loss 4.3346, time 165.28ms, mfu 40.25%\n",
            "iter 2250: loss 4.3700, time 164.32ms, mfu 40.37%\n",
            "iter 2260: loss 4.3174, time 164.89ms, mfu 40.45%\n",
            "iter 2270: loss 4.3005, time 165.17ms, mfu 40.52%\n",
            "iter 2280: loss 4.3788, time 164.17ms, mfu 40.61%\n",
            "iter 2290: loss 4.3404, time 165.23ms, mfu 40.66%\n",
            "iter 2300: loss 4.2844, time 164.61ms, mfu 40.72%\n",
            "iter 2310: loss 4.3226, time 165.24ms, mfu 40.76%\n",
            "iter 2320: loss 4.3149, time 164.62ms, mfu 40.82%\n",
            "iter 2330: loss 4.3066, time 164.14ms, mfu 40.87%\n",
            "iter 2340: loss 4.2368, time 165.30ms, mfu 40.90%\n",
            "iter 2350: loss 4.3070, time 163.90ms, mfu 40.96%\n",
            "iter 2360: loss 4.2456, time 165.32ms, mfu 40.97%\n",
            "iter 2370: loss 4.2399, time 164.24ms, mfu 41.01%\n",
            "iter 2380: loss 4.3122, time 165.40ms, mfu 41.02%\n",
            "iter 2390: loss 4.1181, time 164.38ms, mfu 41.05%\n",
            "step 2400: train loss 4.2644, val loss 4.2499\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 2400: loss 4.2915, time 27134.22ms, mfu 36.97%\n",
            "iter 2410: loss 4.1661, time 163.87ms, mfu 37.42%\n",
            "iter 2420: loss 4.3138, time 165.33ms, mfu 37.79%\n",
            "iter 2430: loss 4.2340, time 165.55ms, mfu 38.12%\n",
            "iter 2440: loss 4.2060, time 164.35ms, mfu 38.44%\n",
            "iter 2450: loss 4.1885, time 165.30ms, mfu 38.71%\n",
            "iter 2460: loss 4.2478, time 164.70ms, mfu 38.96%\n",
            "iter 2470: loss 4.3160, time 165.13ms, mfu 39.18%\n",
            "iter 2480: loss 4.2540, time 165.30ms, mfu 39.38%\n",
            "iter 2490: loss 4.3019, time 164.41ms, mfu 39.57%\n",
            "iter 2500: loss 4.2618, time 165.41ms, mfu 39.72%\n",
            "iter 2510: loss 4.2431, time 164.27ms, mfu 39.89%\n",
            "iter 2520: loss 4.2988, time 165.04ms, mfu 40.02%\n",
            "iter 2530: loss 4.2833, time 165.12ms, mfu 40.13%\n",
            "iter 2540: loss 4.0714, time 164.20ms, mfu 40.26%\n",
            "iter 2550: loss 4.1877, time 165.13ms, mfu 40.35%\n",
            "iter 2560: loss 4.1796, time 164.40ms, mfu 40.45%\n",
            "iter 2570: loss 4.2414, time 164.63ms, mfu 40.53%\n",
            "iter 2580: loss 4.3240, time 165.01ms, mfu 40.60%\n",
            "iter 2590: loss 4.1695, time 163.85ms, mfu 40.68%\n",
            "iter 2600: loss 4.1536, time 165.39ms, mfu 40.73%\n",
            "iter 2610: loss 4.2082, time 164.37ms, mfu 40.79%\n",
            "iter 2620: loss 4.1912, time 165.21ms, mfu 40.82%\n",
            "iter 2630: loss 4.2370, time 164.42ms, mfu 40.87%\n",
            "iter 2640: loss 4.0970, time 164.92ms, mfu 40.91%\n",
            "iter 2650: loss 4.1709, time 165.00ms, mfu 40.94%\n",
            "iter 2660: loss 4.2567, time 164.46ms, mfu 40.97%\n",
            "iter 2670: loss 4.2243, time 164.70ms, mfu 41.00%\n",
            "iter 2680: loss 4.0724, time 165.15ms, mfu 41.02%\n",
            "iter 2690: loss 4.2779, time 164.66ms, mfu 41.04%\n",
            "step 2700: train loss 4.1958, val loss 4.1790\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 2700: loss 4.2554, time 27134.52ms, mfu 36.96%\n",
            "iter 2710: loss 4.1701, time 164.57ms, mfu 37.40%\n",
            "iter 2720: loss 4.1159, time 165.35ms, mfu 37.77%\n",
            "iter 2730: loss 4.1468, time 164.84ms, mfu 38.11%\n",
            "iter 2740: loss 4.1270, time 164.43ms, mfu 38.44%\n",
            "iter 2750: loss 4.1681, time 165.22ms, mfu 38.71%\n",
            "iter 2760: loss 4.0818, time 164.78ms, mfu 38.96%\n",
            "iter 2770: loss 4.1133, time 164.33ms, mfu 39.20%\n",
            "iter 2780: loss 4.1695, time 165.37ms, mfu 39.39%\n",
            "iter 2790: loss 4.1361, time 164.59ms, mfu 39.58%\n",
            "iter 2800: loss 4.1598, time 164.38ms, mfu 39.76%\n",
            "iter 2810: loss 4.1365, time 164.97ms, mfu 39.90%\n",
            "iter 2820: loss 4.0615, time 164.60ms, mfu 40.04%\n",
            "iter 2830: loss 4.1760, time 164.71ms, mfu 40.16%\n",
            "iter 2840: loss 4.2678, time 164.64ms, mfu 40.27%\n",
            "iter 2850: loss 4.2461, time 164.40ms, mfu 40.38%\n",
            "iter 2860: loss 4.1035, time 165.53ms, mfu 40.45%\n",
            "iter 2870: loss 4.0594, time 164.50ms, mfu 40.53%\n",
            "iter 2880: loss 4.1731, time 165.41ms, mfu 40.59%\n",
            "iter 2890: loss 4.2162, time 164.95ms, mfu 40.65%\n",
            "iter 2900: loss 4.1614, time 164.46ms, mfu 40.72%\n",
            "iter 2910: loss 4.1096, time 165.34ms, mfu 40.76%\n",
            "iter 2920: loss 4.0719, time 164.52ms, mfu 40.81%\n",
            "iter 2930: loss 4.1518, time 164.58ms, mfu 40.86%\n",
            "iter 2940: loss 4.0842, time 165.48ms, mfu 40.88%\n",
            "iter 2950: loss 4.1252, time 164.71ms, mfu 40.92%\n",
            "iter 2960: loss 4.0247, time 165.30ms, mfu 40.94%\n",
            "iter 2970: loss 4.1706, time 164.61ms, mfu 40.97%\n",
            "iter 2980: loss 4.0708, time 165.27ms, mfu 40.99%\n",
            "iter 2990: loss 4.0935, time 164.53ms, mfu 41.02%\n",
            "step 3000: train loss 4.1223, val loss 4.1132\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 3000: loss 4.2532, time 26983.80ms, mfu 36.94%\n",
            "iter 3010: loss 4.1742, time 164.24ms, mfu 37.39%\n",
            "iter 3020: loss 3.9759, time 165.24ms, mfu 37.76%\n",
            "iter 3030: loss 4.1121, time 164.48ms, mfu 38.12%\n",
            "iter 3040: loss 4.0459, time 165.26ms, mfu 38.42%\n",
            "iter 3050: loss 4.3110, time 164.57ms, mfu 38.71%\n",
            "iter 3060: loss 4.0834, time 164.80ms, mfu 38.96%\n",
            "iter 3070: loss 4.1575, time 165.19ms, mfu 39.18%\n",
            "iter 3080: loss 4.0046, time 164.28ms, mfu 39.40%\n",
            "iter 3090: loss 4.0988, time 165.15ms, mfu 39.57%\n",
            "iter 3100: loss 4.1161, time 165.36ms, mfu 39.73%\n",
            "iter 3110: loss 4.0880, time 164.63ms, mfu 39.88%\n",
            "iter 3120: loss 4.1351, time 165.45ms, mfu 40.00%\n",
            "iter 3130: loss 4.1581, time 164.40ms, mfu 40.14%\n",
            "iter 3140: loss 3.9851, time 165.51ms, mfu 40.23%\n",
            "iter 3150: loss 4.0747, time 165.20ms, mfu 40.32%\n",
            "iter 3160: loss 4.1014, time 164.59ms, mfu 40.42%\n",
            "iter 3170: loss 4.0882, time 165.47ms, mfu 40.48%\n",
            "iter 3180: loss 4.1357, time 164.31ms, mfu 40.57%\n",
            "iter 3190: loss 4.0551, time 165.34ms, mfu 40.62%\n",
            "iter 3200: loss 4.0922, time 164.52ms, mfu 40.69%\n",
            "iter 3210: loss 4.0715, time 165.34ms, mfu 40.73%\n",
            "iter 3220: loss 4.0440, time 164.63ms, mfu 40.79%\n",
            "iter 3230: loss 4.1433, time 164.70ms, mfu 40.84%\n",
            "iter 3240: loss 4.1661, time 165.36ms, mfu 40.86%\n",
            "iter 3250: loss 4.0819, time 164.52ms, mfu 40.91%\n",
            "iter 3260: loss 4.0307, time 164.75ms, mfu 40.94%\n",
            "iter 3270: loss 4.0511, time 165.50ms, mfu 40.95%\n",
            "iter 3280: loss 4.0379, time 164.56ms, mfu 40.99%\n",
            "iter 3290: loss 4.0257, time 165.14ms, mfu 41.01%\n",
            "step 3300: train loss 4.0576, val loss 4.0441\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 3300: loss 4.0853, time 27015.75ms, mfu 36.93%\n",
            "iter 3310: loss 4.0047, time 164.43ms, mfu 37.37%\n",
            "iter 3320: loss 3.9191, time 164.92ms, mfu 37.75%\n",
            "iter 3330: loss 3.9823, time 164.50ms, mfu 38.11%\n",
            "iter 3340: loss 3.9452, time 165.05ms, mfu 38.42%\n",
            "iter 3350: loss 4.1673, time 164.77ms, mfu 38.70%\n",
            "iter 3360: loss 4.1709, time 165.24ms, mfu 38.94%\n",
            "iter 3370: loss 3.9882, time 164.77ms, mfu 39.17%\n",
            "iter 3380: loss 4.0263, time 165.37ms, mfu 39.37%\n",
            "iter 3390: loss 4.0150, time 164.47ms, mfu 39.56%\n",
            "iter 3400: loss 3.9430, time 164.58ms, mfu 39.74%\n",
            "iter 3410: loss 4.1526, time 165.55ms, mfu 39.87%\n",
            "iter 3420: loss 4.0143, time 164.33ms, mfu 40.02%\n",
            "iter 3430: loss 3.9213, time 165.44ms, mfu 40.12%\n",
            "iter 3440: loss 4.1975, time 164.43ms, mfu 40.24%\n",
            "iter 3450: loss 4.0680, time 165.48ms, mfu 40.33%\n",
            "iter 3460: loss 4.0766, time 164.56ms, mfu 40.42%\n",
            "iter 3470: loss 3.9626, time 165.24ms, mfu 40.49%\n",
            "iter 3480: loss 4.0556, time 164.71ms, mfu 40.57%\n",
            "iter 3490: loss 3.9249, time 165.04ms, mfu 40.63%\n",
            "iter 3500: loss 4.2314, time 165.08ms, mfu 40.69%\n",
            "iter 3510: loss 3.9990, time 164.67ms, mfu 40.74%\n",
            "iter 3520: loss 3.9918, time 164.45ms, mfu 40.80%\n",
            "iter 3530: loss 3.9386, time 165.55ms, mfu 40.83%\n",
            "iter 3540: loss 4.0370, time 164.29ms, mfu 40.88%\n",
            "iter 3550: loss 4.0113, time 164.42ms, mfu 40.93%\n",
            "iter 3560: loss 4.1109, time 165.15ms, mfu 40.95%\n",
            "iter 3570: loss 4.0219, time 164.31ms, mfu 40.99%\n",
            "iter 3580: loss 4.0996, time 164.46ms, mfu 41.02%\n",
            "iter 3590: loss 4.0142, time 165.40ms, mfu 41.03%\n",
            "step 3600: train loss 4.0156, val loss 4.0034\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 3600: loss 4.0680, time 26940.16ms, mfu 36.95%\n",
            "iter 3610: loss 4.0989, time 164.52ms, mfu 37.39%\n",
            "iter 3620: loss 4.0685, time 165.40ms, mfu 37.76%\n",
            "iter 3630: loss 4.0653, time 165.12ms, mfu 38.10%\n",
            "iter 3640: loss 3.9805, time 164.69ms, mfu 38.42%\n",
            "iter 3650: loss 4.0334, time 165.33ms, mfu 38.69%\n",
            "iter 3660: loss 3.9518, time 164.44ms, mfu 38.95%\n",
            "iter 3670: loss 3.9590, time 165.34ms, mfu 39.17%\n",
            "iter 3680: loss 4.0837, time 164.37ms, mfu 39.38%\n",
            "iter 3690: loss 3.9925, time 165.35ms, mfu 39.56%\n",
            "iter 3700: loss 4.1197, time 164.46ms, mfu 39.73%\n",
            "iter 3710: loss 4.0112, time 164.86ms, mfu 39.88%\n",
            "iter 3720: loss 4.1620, time 165.53ms, mfu 40.00%\n",
            "iter 3730: loss 4.0881, time 164.34ms, mfu 40.14%\n",
            "iter 3740: loss 4.0795, time 165.69ms, mfu 40.22%\n",
            "iter 3750: loss 3.9957, time 164.78ms, mfu 40.33%\n",
            "iter 3760: loss 4.0108, time 164.52ms, mfu 40.42%\n",
            "iter 3770: loss 3.9688, time 165.38ms, mfu 40.49%\n",
            "iter 3780: loss 3.9968, time 164.61ms, mfu 40.57%\n",
            "iter 3790: loss 3.9285, time 164.51ms, mfu 40.65%\n",
            "iter 3800: loss 4.0641, time 165.30ms, mfu 40.69%\n",
            "iter 3810: loss 3.9782, time 164.89ms, mfu 40.74%\n",
            "iter 3820: loss 4.0271, time 164.44ms, mfu 40.80%\n",
            "iter 3830: loss 3.9532, time 165.64ms, mfu 40.83%\n",
            "iter 3840: loss 3.9734, time 164.57ms, mfu 40.87%\n",
            "iter 3850: loss 3.9487, time 164.40ms, mfu 40.92%\n",
            "iter 3860: loss 4.0204, time 165.36ms, mfu 40.94%\n",
            "iter 3870: loss 3.9267, time 165.24ms, mfu 40.96%\n",
            "iter 3880: loss 3.8953, time 164.41ms, mfu 41.00%\n",
            "iter 3890: loss 3.9682, time 165.01ms, mfu 41.01%\n",
            "step 3900: train loss 3.9658, val loss 3.9673\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 3900: loss 3.9327, time 26950.68ms, mfu 36.94%\n",
            "iter 3910: loss 4.0289, time 165.16ms, mfu 37.36%\n",
            "iter 3920: loss 3.8560, time 164.53ms, mfu 37.75%\n",
            "iter 3930: loss 3.9168, time 165.31ms, mfu 38.09%\n",
            "iter 3940: loss 3.9442, time 164.44ms, mfu 38.41%\n",
            "iter 3950: loss 4.0716, time 165.21ms, mfu 38.69%\n",
            "iter 3960: loss 4.0642, time 164.72ms, mfu 38.94%\n",
            "iter 3970: loss 3.9663, time 165.38ms, mfu 39.16%\n",
            "iter 3980: loss 3.9835, time 165.32ms, mfu 39.35%\n",
            "iter 3990: loss 3.9217, time 164.68ms, mfu 39.55%\n",
            "iter 4000: loss 3.9456, time 164.45ms, mfu 39.72%\n",
            "iter 4010: loss 3.8987, time 165.42ms, mfu 39.86%\n",
            "iter 4020: loss 3.9389, time 164.57ms, mfu 40.00%\n",
            "iter 4030: loss 4.0550, time 164.39ms, mfu 40.14%\n",
            "iter 4040: loss 3.9716, time 165.37ms, mfu 40.23%\n",
            "iter 4050: loss 3.9869, time 164.63ms, mfu 40.34%\n",
            "iter 4060: loss 3.9642, time 164.45ms, mfu 40.44%\n",
            "iter 4070: loss 3.9835, time 165.42ms, mfu 40.50%\n",
            "iter 4080: loss 3.8864, time 164.60ms, mfu 40.58%\n",
            "iter 4090: loss 3.9500, time 164.39ms, mfu 40.66%\n",
            "iter 4100: loss 3.8829, time 165.44ms, mfu 40.70%\n",
            "iter 4110: loss 3.9726, time 164.98ms, mfu 40.75%\n",
            "iter 4120: loss 3.8979, time 164.23ms, mfu 40.81%\n",
            "iter 4130: loss 4.0791, time 165.26ms, mfu 40.84%\n",
            "iter 4140: loss 3.9511, time 165.34ms, mfu 40.87%\n",
            "iter 4150: loss 3.9649, time 164.65ms, mfu 40.91%\n",
            "iter 4160: loss 3.8929, time 165.22ms, mfu 40.93%\n",
            "iter 4170: loss 4.1382, time 165.50ms, mfu 40.95%\n",
            "iter 4180: loss 3.9789, time 164.74ms, mfu 40.98%\n",
            "iter 4190: loss 3.9436, time 164.65ms, mfu 41.01%\n",
            "step 4200: train loss 3.9391, val loss 3.9334\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 4200: loss 3.9628, time 29203.57ms, mfu 36.93%\n",
            "iter 4210: loss 3.8613, time 164.42ms, mfu 37.37%\n",
            "iter 4220: loss 3.9931, time 164.17ms, mfu 37.77%\n",
            "iter 4230: loss 4.0064, time 164.95ms, mfu 38.12%\n",
            "iter 4240: loss 3.9768, time 164.47ms, mfu 38.44%\n",
            "iter 4250: loss 3.9879, time 165.35ms, mfu 38.70%\n",
            "iter 4260: loss 3.8595, time 164.54ms, mfu 38.96%\n",
            "iter 4270: loss 4.0514, time 164.40ms, mfu 39.20%\n",
            "iter 4280: loss 3.8655, time 165.42ms, mfu 39.39%\n",
            "iter 4290: loss 3.8687, time 164.69ms, mfu 39.58%\n",
            "iter 4300: loss 3.9843, time 165.30ms, mfu 39.73%\n",
            "iter 4310: loss 3.9453, time 164.49ms, mfu 39.89%\n",
            "iter 4320: loss 3.8217, time 164.38ms, mfu 40.04%\n",
            "iter 4330: loss 4.0218, time 165.26ms, mfu 40.14%\n",
            "iter 4340: loss 3.8679, time 164.44ms, mfu 40.26%\n",
            "iter 4350: loss 3.6918, time 164.98ms, mfu 40.36%\n",
            "iter 4360: loss 3.9102, time 164.50ms, mfu 40.45%\n",
            "iter 4370: loss 3.8693, time 164.24ms, mfu 40.55%\n",
            "iter 4380: loss 3.7849, time 165.13ms, mfu 40.61%\n",
            "iter 4390: loss 4.0462, time 164.48ms, mfu 40.68%\n",
            "iter 4400: loss 3.9417, time 164.38ms, mfu 40.74%\n",
            "iter 4410: loss 4.0526, time 165.25ms, mfu 40.78%\n",
            "iter 4420: loss 3.8836, time 164.53ms, mfu 40.84%\n",
            "iter 4430: loss 4.0449, time 165.21ms, mfu 40.87%\n",
            "iter 4440: loss 3.9255, time 165.61ms, mfu 40.88%\n",
            "iter 4450: loss 3.8761, time 164.51ms, mfu 40.93%\n",
            "iter 4460: loss 3.9208, time 165.20ms, mfu 40.95%\n",
            "iter 4470: loss 3.9747, time 164.45ms, mfu 40.99%\n",
            "iter 4480: loss 3.9003, time 164.04ms, mfu 41.03%\n",
            "iter 4490: loss 3.9136, time 165.43ms, mfu 41.04%\n",
            "step 4500: train loss 3.9247, val loss 3.9112\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 4500: loss 3.9304, time 27172.99ms, mfu 36.96%\n",
            "iter 4510: loss 3.9996, time 165.26ms, mfu 37.37%\n",
            "iter 4520: loss 3.8792, time 164.69ms, mfu 37.76%\n",
            "iter 4530: loss 3.8585, time 164.70ms, mfu 38.11%\n",
            "iter 4540: loss 4.0379, time 165.25ms, mfu 38.41%\n",
            "iter 4550: loss 3.9771, time 164.68ms, mfu 38.70%\n",
            "iter 4560: loss 3.8503, time 164.43ms, mfu 38.96%\n",
            "iter 4570: loss 3.9464, time 165.16ms, mfu 39.18%\n",
            "iter 4580: loss 3.8384, time 165.26ms, mfu 39.38%\n",
            "iter 4590: loss 3.9578, time 164.48ms, mfu 39.57%\n",
            "iter 4600: loss 3.9528, time 165.34ms, mfu 39.72%\n",
            "iter 4610: loss 3.9203, time 165.27ms, mfu 39.86%\n",
            "iter 4620: loss 3.8838, time 164.57ms, mfu 40.01%\n",
            "iter 4630: loss 3.7781, time 165.83ms, mfu 40.11%\n",
            "iter 4640: loss 3.8599, time 165.28ms, mfu 40.21%\n",
            "iter 4650: loss 3.9361, time 164.30ms, mfu 40.32%\n",
            "iter 4660: loss 3.9369, time 165.30ms, mfu 40.40%\n",
            "iter 4670: loss 3.9343, time 164.71ms, mfu 40.49%\n",
            "iter 4680: loss 3.9277, time 164.45ms, mfu 40.57%\n",
            "iter 4690: loss 3.8360, time 165.50ms, mfu 40.62%\n",
            "iter 4700: loss 3.8542, time 164.52ms, mfu 40.69%\n",
            "iter 4710: loss 3.9742, time 165.28ms, mfu 40.73%\n",
            "iter 4720: loss 3.8409, time 165.42ms, mfu 40.77%\n",
            "iter 4730: loss 3.9063, time 164.33ms, mfu 40.83%\n",
            "iter 4740: loss 3.8929, time 165.41ms, mfu 40.85%\n",
            "iter 4750: loss 3.9350, time 164.48ms, mfu 40.90%\n",
            "iter 4760: loss 3.8118, time 165.36ms, mfu 40.92%\n",
            "iter 4770: loss 3.9946, time 164.74ms, mfu 40.95%\n",
            "iter 4780: loss 3.9713, time 164.74ms, mfu 40.98%\n",
            "iter 4790: loss 3.8796, time 165.59ms, mfu 40.99%\n",
            "step 4800: train loss 3.9106, val loss 3.9019\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 4800: loss 3.9229, time 27183.98ms, mfu 36.92%\n",
            "iter 4810: loss 3.7996, time 164.57ms, mfu 37.35%\n",
            "iter 4820: loss 4.0140, time 164.79ms, mfu 37.74%\n",
            "iter 4830: loss 3.8455, time 165.39ms, mfu 38.08%\n",
            "iter 4840: loss 3.9332, time 164.52ms, mfu 38.40%\n",
            "iter 4850: loss 4.0089, time 164.62ms, mfu 38.69%\n",
            "iter 4860: loss 3.9091, time 165.34ms, mfu 38.93%\n",
            "iter 4870: loss 3.8748, time 164.73ms, mfu 39.16%\n",
            "iter 4880: loss 3.9046, time 164.31ms, mfu 39.38%\n",
            "iter 4890: loss 3.9993, time 165.30ms, mfu 39.56%\n",
            "iter 4900: loss 3.9108, time 164.47ms, mfu 39.73%\n",
            "iter 4910: loss 3.7950, time 165.19ms, mfu 39.87%\n",
            "iter 4920: loss 3.9594, time 165.16ms, mfu 40.00%\n",
            "iter 4930: loss 3.7256, time 164.06ms, mfu 40.14%\n",
            "iter 4940: loss 3.8279, time 165.15ms, mfu 40.24%\n",
            "iter 4950: loss 3.8914, time 164.35ms, mfu 40.36%\n",
            "iter 4960: loss 3.8390, time 165.18ms, mfu 40.43%\n",
            "iter 4970: loss 3.9198, time 164.65ms, mfu 40.52%\n",
            "iter 4980: loss 3.7979, time 164.27ms, mfu 40.60%\n",
            "iter 4990: loss 3.8283, time 164.40ms, mfu 40.68%\n",
            "iter 5000: loss 3.9126, time 165.15ms, mfu 40.73%\n",
            "iter 5010: loss 3.8996, time 164.55ms, mfu 40.78%\n",
            "iter 5020: loss 3.8022, time 165.28ms, mfu 40.82%\n",
            "iter 5030: loss 3.8227, time 164.76ms, mfu 40.86%\n",
            "iter 5040: loss 3.8733, time 164.37ms, mfu 40.91%\n",
            "iter 5050: loss 3.9619, time 165.33ms, mfu 40.93%\n",
            "iter 5060: loss 3.9583, time 164.24ms, mfu 40.97%\n",
            "iter 5070: loss 4.0538, time 165.27ms, mfu 40.99%\n",
            "iter 5080: loss 3.8846, time 164.17ms, mfu 41.03%\n",
            "iter 5090: loss 4.0180, time 165.33ms, mfu 41.04%\n",
            "step 5100: train loss 3.9035, val loss 3.9081\n",
            "iter 5100: loss 4.0236, time 24945.24ms, mfu 36.96%\n",
            "iter 5110: loss 3.8264, time 164.75ms, mfu 37.39%\n",
            "iter 5120: loss 3.9329, time 164.35ms, mfu 37.79%\n",
            "iter 5130: loss 3.7794, time 165.27ms, mfu 38.12%\n",
            "iter 5140: loss 3.9368, time 164.59ms, mfu 38.44%\n",
            "iter 5150: loss 3.8337, time 164.43ms, mfu 38.73%\n",
            "iter 5160: loss 3.8771, time 165.44ms, mfu 38.96%\n",
            "iter 5170: loss 4.0008, time 165.17ms, mfu 39.18%\n",
            "iter 5180: loss 3.8857, time 164.34ms, mfu 39.40%\n",
            "iter 5190: loss 3.7975, time 165.25ms, mfu 39.57%\n",
            "iter 5200: loss 3.8573, time 165.48ms, mfu 39.72%\n",
            "iter 5210: loss 4.1189, time 164.25ms, mfu 39.89%\n",
            "iter 5220: loss 3.9379, time 165.24ms, mfu 40.01%\n",
            "iter 5230: loss 3.8625, time 164.58ms, mfu 40.14%\n",
            "iter 5240: loss 3.9524, time 164.74ms, mfu 40.25%\n",
            "iter 5250: loss 3.9584, time 165.30ms, mfu 40.34%\n",
            "iter 5260: loss 3.9655, time 164.42ms, mfu 40.44%\n",
            "iter 5270: loss 4.0054, time 164.64ms, mfu 40.52%\n",
            "iter 5280: loss 3.8863, time 165.43ms, mfu 40.58%\n",
            "iter 5290: loss 3.9235, time 164.79ms, mfu 40.64%\n",
            "iter 5300: loss 3.9486, time 164.71ms, mfu 40.71%\n",
            "iter 5310: loss 3.9426, time 164.71ms, mfu 40.76%\n",
            "iter 5320: loss 3.9758, time 164.88ms, mfu 40.81%\n",
            "iter 5330: loss 3.9466, time 164.76ms, mfu 40.85%\n",
            "iter 5340: loss 3.9065, time 165.28ms, mfu 40.88%\n",
            "iter 5350: loss 3.8262, time 164.58ms, mfu 40.92%\n",
            "iter 5360: loss 3.9200, time 165.22ms, mfu 40.94%\n",
            "iter 5370: loss 3.9098, time 164.60ms, mfu 40.98%\n",
            "iter 5380: loss 3.9461, time 164.81ms, mfu 41.00%\n",
            "iter 5390: loss 3.7897, time 165.56ms, mfu 41.01%\n",
            "step 5400: train loss 3.9148, val loss 3.8938\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 5400: loss 4.0382, time 27194.65ms, mfu 36.93%\n",
            "iter 5410: loss 3.7751, time 165.42ms, mfu 37.35%\n",
            "iter 5420: loss 3.9858, time 164.59ms, mfu 37.74%\n",
            "iter 5430: loss 3.8771, time 165.33ms, mfu 38.08%\n",
            "iter 5440: loss 3.9432, time 165.46ms, mfu 38.38%\n",
            "iter 5450: loss 3.9406, time 164.46ms, mfu 38.67%\n",
            "iter 5460: loss 4.0183, time 165.36ms, mfu 38.92%\n",
            "iter 5470: loss 3.8656, time 165.44ms, mfu 39.13%\n",
            "iter 5480: loss 3.8289, time 164.42ms, mfu 39.35%\n",
            "iter 5490: loss 3.8299, time 165.11ms, mfu 39.53%\n",
            "iter 5500: loss 3.8240, time 164.41ms, mfu 39.71%\n",
            "iter 5510: loss 3.9520, time 165.31ms, mfu 39.85%\n",
            "iter 5520: loss 3.9389, time 164.52ms, mfu 40.00%\n",
            "iter 5530: loss 4.0770, time 164.86ms, mfu 40.12%\n",
            "iter 5540: loss 3.9911, time 165.43ms, mfu 40.22%\n",
            "iter 5550: loss 3.8995, time 164.52ms, mfu 40.33%\n",
            "iter 5560: loss 3.8202, time 165.45ms, mfu 40.40%\n",
            "iter 5570: loss 3.8245, time 164.45ms, mfu 40.49%\n",
            "iter 5580: loss 3.8669, time 165.40ms, mfu 40.55%\n",
            "iter 5590: loss 3.9079, time 164.54ms, mfu 40.63%\n",
            "iter 5600: loss 3.9266, time 165.42ms, mfu 40.68%\n",
            "iter 5610: loss 3.9660, time 164.80ms, mfu 40.73%\n",
            "iter 5620: loss 3.8186, time 165.18ms, mfu 40.77%\n",
            "iter 5630: loss 3.9145, time 165.50ms, mfu 40.80%\n",
            "iter 5640: loss 3.9016, time 164.52ms, mfu 40.85%\n",
            "iter 5650: loss 3.9091, time 164.37ms, mfu 40.90%\n",
            "iter 5660: loss 3.8477, time 165.77ms, mfu 40.91%\n",
            "iter 5670: loss 3.9879, time 164.47ms, mfu 40.95%\n",
            "iter 5680: loss 4.1427, time 164.96ms, mfu 40.98%\n",
            "iter 5690: loss 3.9039, time 165.22ms, mfu 40.99%\n",
            "step 5700: train loss 3.9050, val loss 3.8997\n",
            "iter 5700: loss 3.8070, time 24935.37ms, mfu 36.92%\n",
            "iter 5710: loss 3.9948, time 165.32ms, mfu 37.34%\n",
            "iter 5720: loss 3.9328, time 164.43ms, mfu 37.74%\n",
            "iter 5730: loss 3.9776, time 165.61ms, mfu 38.07%\n",
            "iter 5740: loss 3.8859, time 165.42ms, mfu 38.37%\n",
            "iter 5750: loss 3.8636, time 164.77ms, mfu 38.66%\n",
            "iter 5760: loss 3.8802, time 165.24ms, mfu 38.91%\n",
            "iter 5770: loss 3.8005, time 164.77ms, mfu 39.14%\n",
            "iter 5780: loss 3.9050, time 165.36ms, mfu 39.34%\n",
            "iter 5790: loss 3.9914, time 164.74ms, mfu 39.53%\n",
            "iter 5800: loss 3.9563, time 165.51ms, mfu 39.68%\n",
            "iter 5810: loss 3.8177, time 164.75ms, mfu 39.84%\n",
            "iter 5820: loss 4.0596, time 165.17ms, mfu 39.97%\n",
            "iter 5830: loss 4.0094, time 165.35ms, mfu 40.08%\n",
            "iter 5840: loss 4.0216, time 164.53ms, mfu 40.21%\n",
            "iter 5850: loss 3.8239, time 165.62ms, mfu 40.29%\n",
            "iter 5860: loss 3.8437, time 164.60ms, mfu 40.39%\n",
            "iter 5870: loss 4.0044, time 164.84ms, mfu 40.47%\n",
            "iter 5880: loss 3.8844, time 165.43ms, mfu 40.53%\n",
            "iter 5890: loss 3.9379, time 165.22ms, mfu 40.59%\n",
            "iter 5900: loss 3.9731, time 164.70ms, mfu 40.66%\n",
            "iter 5910: loss 4.0212, time 164.91ms, mfu 40.72%\n",
            "iter 5920: loss 3.7271, time 165.54ms, mfu 40.75%\n",
            "iter 5930: loss 3.8814, time 165.39ms, mfu 40.78%\n",
            "iter 5940: loss 3.8669, time 164.70ms, mfu 40.83%\n",
            "iter 5950: loss 3.7238, time 165.12ms, mfu 40.87%\n",
            "iter 5960: loss 3.9168, time 165.60ms, mfu 40.88%\n",
            "iter 5970: loss 3.9073, time 165.33ms, mfu 40.91%\n",
            "iter 5980: loss 3.8583, time 164.24ms, mfu 40.95%\n",
            "iter 5990: loss 4.0432, time 164.99ms, mfu 40.98%\n",
            "step 6000: train loss 3.9015, val loss 3.9011\n",
            "iter 6000: loss 3.8665, time 24931.52ms, mfu 36.91%\n",
            "iter 6010: loss 3.8471, time 164.79ms, mfu 37.34%\n",
            "iter 6020: loss 3.8674, time 165.48ms, mfu 37.71%\n",
            "iter 6030: loss 3.9322, time 164.25ms, mfu 38.08%\n",
            "iter 6040: loss 3.9432, time 165.47ms, mfu 38.38%\n",
            "iter 6050: loss 3.9965, time 164.70ms, mfu 38.67%\n",
            "iter 6060: loss 3.9449, time 165.18ms, mfu 38.92%\n",
            "iter 6070: loss 3.8900, time 164.92ms, mfu 39.14%\n",
            "iter 6080: loss 3.8099, time 165.52ms, mfu 39.34%\n",
            "iter 6090: loss 3.9795, time 164.46ms, mfu 39.54%\n",
            "iter 6100: loss 3.9365, time 165.45ms, mfu 39.69%\n",
            "iter 6110: loss 3.9765, time 164.47ms, mfu 39.85%\n",
            "iter 6120: loss 3.7586, time 164.72ms, mfu 39.99%\n",
            "iter 6130: loss 3.9640, time 166.04ms, mfu 40.09%\n",
            "iter 6140: loss 3.9065, time 164.62ms, mfu 40.21%\n",
            "iter 6150: loss 4.0504, time 164.52ms, mfu 40.32%\n",
            "iter 6160: loss 3.9255, time 165.53ms, mfu 40.39%\n",
            "iter 6170: loss 4.0148, time 164.89ms, mfu 40.47%\n",
            "iter 6180: loss 3.8076, time 164.46ms, mfu 40.56%\n",
            "iter 6190: loss 3.8340, time 165.61ms, mfu 40.61%\n",
            "iter 6200: loss 3.8261, time 165.68ms, mfu 40.65%\n",
            "iter 6210: loss 3.9692, time 164.69ms, mfu 40.71%\n",
            "iter 6220: loss 4.0978, time 164.49ms, mfu 40.77%\n",
            "iter 6230: loss 3.9602, time 165.01ms, mfu 40.81%\n",
            "iter 6240: loss 3.8843, time 165.41ms, mfu 40.84%\n",
            "iter 6250: loss 3.9188, time 164.76ms, mfu 40.88%\n",
            "iter 6260: loss 3.8573, time 164.59ms, mfu 40.92%\n",
            "iter 6270: loss 3.8570, time 165.26ms, mfu 40.94%\n",
            "iter 6280: loss 3.9007, time 165.53ms, mfu 40.95%\n",
            "iter 6290: loss 3.7665, time 164.74ms, mfu 40.98%\n",
            "step 6300: train loss 3.9040, val loss 3.9006\n",
            "iter 6300: loss 3.8942, time 24934.93ms, mfu 36.91%\n",
            "iter 6310: loss 3.9483, time 164.68ms, mfu 37.35%\n",
            "iter 6320: loss 3.9834, time 165.39ms, mfu 37.72%\n",
            "iter 6330: loss 3.9377, time 164.65ms, mfu 38.08%\n",
            "iter 6340: loss 3.9030, time 165.17ms, mfu 38.39%\n",
            "iter 6350: loss 3.8329, time 164.69ms, mfu 38.67%\n",
            "iter 6360: loss 3.7865, time 165.03ms, mfu 38.92%\n",
            "iter 6370: loss 3.9161, time 165.49ms, mfu 39.14%\n",
            "iter 6380: loss 3.8419, time 164.57ms, mfu 39.35%\n",
            "iter 6390: loss 4.0616, time 165.39ms, mfu 39.53%\n",
            "iter 6400: loss 3.7727, time 165.32ms, mfu 39.69%\n",
            "iter 6410: loss 3.9565, time 164.57ms, mfu 39.85%\n",
            "iter 6420: loss 3.8913, time 164.81ms, mfu 39.99%\n",
            "iter 6430: loss 3.8839, time 165.69ms, mfu 40.09%\n",
            "iter 6440: loss 3.8699, time 165.83ms, mfu 40.18%\n",
            "iter 6450: loss 3.9423, time 164.67ms, mfu 40.29%\n",
            "iter 6460: loss 3.8374, time 164.58ms, mfu 40.39%\n",
            "iter 6470: loss 3.9826, time 165.57ms, mfu 40.46%\n",
            "iter 6480: loss 3.8316, time 164.69ms, mfu 40.54%\n",
            "iter 6490: loss 3.8485, time 164.49ms, mfu 40.61%\n",
            "iter 6500: loss 3.7542, time 165.69ms, mfu 40.65%\n",
            "iter 6510: loss 3.8514, time 165.31ms, mfu 40.70%\n",
            "iter 6520: loss 3.9747, time 164.62ms, mfu 40.76%\n",
            "iter 6530: loss 3.9217, time 164.42ms, mfu 40.82%\n",
            "iter 6540: loss 3.9604, time 165.32ms, mfu 40.85%\n",
            "iter 6550: loss 3.9805, time 165.73ms, mfu 40.86%\n",
            "iter 6560: loss 3.8979, time 164.56ms, mfu 40.91%\n",
            "iter 6570: loss 3.8803, time 164.74ms, mfu 40.94%\n",
            "iter 6580: loss 4.0375, time 165.32ms, mfu 40.96%\n",
            "iter 6590: loss 3.8372, time 164.58ms, mfu 40.99%\n",
            "step 6600: train loss 3.8916, val loss 3.8885\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 6600: loss 3.9582, time 27231.52ms, mfu 36.92%\n",
            "iter 6610: loss 3.9959, time 164.89ms, mfu 37.35%\n",
            "iter 6620: loss 3.8610, time 165.12ms, mfu 37.73%\n",
            "iter 6630: loss 3.8457, time 164.37ms, mfu 38.09%\n",
            "iter 6640: loss 3.7446, time 165.37ms, mfu 38.39%\n",
            "iter 6650: loss 3.9885, time 164.61ms, mfu 38.68%\n",
            "iter 6660: loss 3.9595, time 164.55ms, mfu 38.94%\n",
            "iter 6670: loss 3.8724, time 165.14ms, mfu 39.16%\n",
            "iter 6680: loss 3.7225, time 164.44ms, mfu 39.38%\n",
            "iter 6690: loss 3.8781, time 165.13ms, mfu 39.56%\n",
            "iter 6700: loss 3.8640, time 164.62ms, mfu 39.73%\n",
            "iter 6710: loss 3.9676, time 164.48ms, mfu 39.89%\n",
            "iter 6720: loss 3.8409, time 165.36ms, mfu 40.01%\n",
            "iter 6730: loss 4.0494, time 164.45ms, mfu 40.14%\n",
            "iter 6740: loss 3.9686, time 164.46ms, mfu 40.26%\n",
            "iter 6750: loss 3.9689, time 165.10ms, mfu 40.35%\n",
            "iter 6760: loss 3.8273, time 164.23ms, mfu 40.45%\n",
            "iter 6770: loss 3.7517, time 165.25ms, mfu 40.52%\n",
            "iter 6780: loss 3.8738, time 165.18ms, mfu 40.58%\n",
            "iter 6790: loss 3.8761, time 164.08ms, mfu 40.67%\n",
            "iter 6800: loss 4.0716, time 165.11ms, mfu 40.72%\n",
            "iter 6810: loss 3.9016, time 164.53ms, mfu 40.78%\n",
            "iter 6820: loss 3.9090, time 164.29ms, mfu 40.84%\n",
            "iter 6830: loss 3.8776, time 165.35ms, mfu 40.86%\n",
            "iter 6840: loss 3.9732, time 164.59ms, mfu 40.91%\n",
            "iter 6850: loss 3.9436, time 165.05ms, mfu 40.93%\n",
            "iter 6860: loss 3.8355, time 165.38ms, mfu 40.95%\n",
            "iter 6870: loss 4.0684, time 164.20ms, mfu 40.99%\n",
            "iter 6880: loss 4.0393, time 165.04ms, mfu 41.01%\n",
            "iter 6890: loss 3.6704, time 164.61ms, mfu 41.04%\n",
            "step 6900: train loss 3.9013, val loss 3.8902\n",
            "iter 6900: loss 3.9464, time 24881.81ms, mfu 36.96%\n",
            "iter 6910: loss 4.0008, time 165.35ms, mfu 37.38%\n",
            "iter 6920: loss 3.8919, time 164.59ms, mfu 37.77%\n",
            "iter 6930: loss 3.8442, time 165.28ms, mfu 38.10%\n",
            "iter 6940: loss 3.9693, time 165.47ms, mfu 38.40%\n",
            "iter 6950: loss 3.9116, time 164.63ms, mfu 38.69%\n",
            "iter 6960: loss 3.8735, time 165.34ms, mfu 38.93%\n",
            "iter 6970: loss 3.8875, time 165.26ms, mfu 39.15%\n",
            "iter 6980: loss 3.9049, time 164.78ms, mfu 39.36%\n",
            "iter 6990: loss 3.9533, time 165.30ms, mfu 39.53%\n",
            "iter 7000: loss 3.9619, time 165.21ms, mfu 39.70%\n",
            "iter 7010: loss 3.8635, time 164.64ms, mfu 39.85%\n",
            "iter 7020: loss 3.9605, time 165.30ms, mfu 39.98%\n",
            "iter 7030: loss 3.8852, time 165.14ms, mfu 40.10%\n",
            "iter 7040: loss 4.0476, time 164.32ms, mfu 40.22%\n",
            "iter 7050: loss 3.8303, time 164.83ms, mfu 40.32%\n",
            "iter 7060: loss 3.8258, time 165.19ms, mfu 40.41%\n",
            "iter 7070: loss 3.9156, time 164.27ms, mfu 40.50%\n",
            "iter 7080: loss 4.0249, time 165.25ms, mfu 40.57%\n",
            "iter 7090: loss 3.9715, time 164.89ms, mfu 40.63%\n",
            "iter 7100: loss 3.9621, time 164.63ms, mfu 40.70%\n",
            "iter 7110: loss 3.9297, time 165.20ms, mfu 40.74%\n",
            "iter 7120: loss 3.8859, time 165.46ms, mfu 40.77%\n",
            "iter 7130: loss 3.8862, time 164.71ms, mfu 40.82%\n",
            "iter 7140: loss 3.9573, time 165.63ms, mfu 40.84%\n",
            "iter 7150: loss 3.9304, time 164.33ms, mfu 40.90%\n",
            "iter 7160: loss 3.8313, time 164.56ms, mfu 40.94%\n",
            "iter 7170: loss 3.7767, time 165.41ms, mfu 40.95%\n",
            "iter 7180: loss 3.9672, time 164.41ms, mfu 40.99%\n",
            "iter 7190: loss 3.9242, time 165.35ms, mfu 41.00%\n",
            "step 7200: train loss 3.8901, val loss 3.8830\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 7200: loss 3.9302, time 27131.25ms, mfu 36.93%\n",
            "iter 7210: loss 3.7856, time 164.10ms, mfu 37.38%\n",
            "iter 7220: loss 3.8900, time 166.58ms, mfu 37.72%\n",
            "iter 7230: loss 3.9071, time 164.89ms, mfu 38.07%\n",
            "iter 7240: loss 3.8300, time 164.41ms, mfu 38.39%\n",
            "iter 7250: loss 3.9138, time 165.34ms, mfu 38.67%\n",
            "iter 7260: loss 3.9711, time 164.66ms, mfu 38.93%\n",
            "iter 7270: loss 3.8425, time 164.51ms, mfu 39.17%\n",
            "iter 7280: loss 3.7900, time 165.35ms, mfu 39.36%\n",
            "iter 7290: loss 3.8793, time 164.69ms, mfu 39.55%\n",
            "iter 7300: loss 3.9674, time 164.43ms, mfu 39.73%\n",
            "iter 7310: loss 3.8347, time 165.58ms, mfu 39.86%\n",
            "iter 7320: loss 3.8000, time 164.60ms, mfu 40.00%\n",
            "iter 7330: loss 3.9719, time 164.67ms, mfu 40.13%\n",
            "iter 7340: loss 3.9617, time 165.50ms, mfu 40.22%\n",
            "iter 7350: loss 3.9071, time 164.57ms, mfu 40.33%\n",
            "iter 7360: loss 3.9105, time 165.26ms, mfu 40.41%\n",
            "iter 7370: loss 3.8730, time 165.18ms, mfu 40.48%\n",
            "iter 7380: loss 3.8343, time 164.45ms, mfu 40.57%\n",
            "iter 7390: loss 3.9782, time 165.18ms, mfu 40.63%\n",
            "iter 7400: loss 3.8281, time 164.49ms, mfu 40.69%\n",
            "iter 7410: loss 3.9958, time 164.57ms, mfu 40.76%\n",
            "iter 7420: loss 3.8565, time 165.39ms, mfu 40.79%\n",
            "iter 7430: loss 3.9528, time 164.34ms, mfu 40.85%\n",
            "iter 7440: loss 3.7695, time 165.50ms, mfu 40.87%\n",
            "iter 7450: loss 3.8900, time 164.45ms, mfu 40.91%\n",
            "iter 7460: loss 3.9374, time 164.47ms, mfu 40.95%\n",
            "iter 7470: loss 3.8944, time 165.31ms, mfu 40.97%\n",
            "iter 7480: loss 3.9353, time 164.70ms, mfu 41.00%\n",
            "iter 7490: loss 3.9385, time 164.79ms, mfu 41.02%\n",
            "step 7500: train loss 3.8942, val loss 3.8905\n",
            "iter 7500: loss 3.9710, time 24909.58ms, mfu 36.95%\n",
            "iter 7510: loss 3.8570, time 165.25ms, mfu 37.37%\n",
            "iter 7520: loss 3.8785, time 164.66ms, mfu 37.76%\n",
            "iter 7530: loss 3.9061, time 164.74ms, mfu 38.11%\n",
            "iter 7540: loss 3.9553, time 165.44ms, mfu 38.40%\n",
            "iter 7550: loss 3.8711, time 165.45ms, mfu 38.67%\n",
            "iter 7560: loss 3.9515, time 164.41ms, mfu 38.94%\n",
            "iter 7570: loss 3.9205, time 165.26ms, mfu 39.16%\n",
            "iter 7580: loss 3.9465, time 165.59ms, mfu 39.35%\n",
            "iter 7590: loss 4.0074, time 164.67ms, mfu 39.54%\n",
            "iter 7600: loss 3.9505, time 164.40ms, mfu 39.72%\n",
            "iter 7610: loss 3.9025, time 165.22ms, mfu 39.86%\n",
            "iter 7620: loss 3.9275, time 164.74ms, mfu 40.00%\n",
            "iter 7630: loss 3.8383, time 164.78ms, mfu 40.12%\n",
            "iter 7640: loss 3.8374, time 165.03ms, mfu 40.23%\n",
            "iter 7650: loss 3.9462, time 165.33ms, mfu 40.32%\n",
            "iter 7660: loss 3.9420, time 164.67ms, mfu 40.41%\n",
            "iter 7670: loss 3.7457, time 165.20ms, mfu 40.49%\n",
            "iter 7680: loss 3.7150, time 165.48ms, mfu 40.54%\n",
            "iter 7690: loss 3.7549, time 164.85ms, mfu 40.61%\n",
            "iter 7700: loss 3.9443, time 165.53ms, mfu 40.66%\n",
            "iter 7710: loss 3.9354, time 164.53ms, mfu 40.72%\n",
            "iter 7720: loss 3.9174, time 165.48ms, mfu 40.76%\n",
            "iter 7730: loss 3.7845, time 165.01ms, mfu 40.80%\n",
            "iter 7740: loss 3.8318, time 165.08ms, mfu 40.84%\n",
            "iter 7750: loss 3.8095, time 164.42ms, mfu 40.89%\n",
            "iter 7760: loss 3.9587, time 165.64ms, mfu 40.90%\n",
            "iter 7770: loss 3.9279, time 164.57ms, mfu 40.94%\n",
            "iter 7780: loss 3.8008, time 165.44ms, mfu 40.96%\n",
            "iter 7790: loss 3.9082, time 164.64ms, mfu 40.99%\n",
            "step 7800: train loss 3.8908, val loss 3.8817\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 7800: loss 3.8140, time 26951.93ms, mfu 36.91%\n",
            "iter 7810: loss 3.8119, time 164.37ms, mfu 37.36%\n",
            "iter 7820: loss 3.9032, time 165.27ms, mfu 37.73%\n",
            "iter 7830: loss 3.7574, time 164.42ms, mfu 38.09%\n",
            "iter 7840: loss 3.8269, time 165.13ms, mfu 38.40%\n",
            "iter 7850: loss 3.9437, time 165.49ms, mfu 38.67%\n",
            "iter 7860: loss 3.7940, time 164.38ms, mfu 38.94%\n",
            "iter 7870: loss 3.9246, time 165.22ms, mfu 39.16%\n",
            "iter 7880: loss 3.7477, time 164.59ms, mfu 39.37%\n",
            "iter 7890: loss 3.9636, time 164.52ms, mfu 39.56%\n",
            "iter 7900: loss 3.9537, time 165.43ms, mfu 39.72%\n",
            "iter 7910: loss 3.8762, time 164.54ms, mfu 39.87%\n",
            "iter 7920: loss 3.9842, time 164.91ms, mfu 40.01%\n",
            "iter 7930: loss 3.7540, time 165.27ms, mfu 40.12%\n",
            "iter 7940: loss 3.8695, time 164.79ms, mfu 40.23%\n",
            "iter 7950: loss 3.9250, time 165.42ms, mfu 40.32%\n",
            "iter 7960: loss 3.8722, time 165.23ms, mfu 40.40%\n",
            "iter 7970: loss 3.9104, time 164.62ms, mfu 40.49%\n",
            "iter 7980: loss 3.9485, time 165.30ms, mfu 40.55%\n",
            "iter 7990: loss 3.8097, time 164.77ms, mfu 40.62%\n",
            "iter 8000: loss 3.8788, time 164.58ms, mfu 40.69%\n",
            "iter 8010: loss 3.7319, time 164.99ms, mfu 40.74%\n",
            "iter 8020: loss 3.9440, time 164.56ms, mfu 40.79%\n",
            "iter 8030: loss 3.8053, time 165.27ms, mfu 40.83%\n",
            "iter 8040: loss 3.8424, time 164.59ms, mfu 40.87%\n",
            "iter 8050: loss 3.9613, time 165.10ms, mfu 40.90%\n",
            "iter 8060: loss 3.9713, time 164.58ms, mfu 40.94%\n",
            "iter 8070: loss 3.9838, time 164.46ms, mfu 40.98%\n",
            "iter 8080: loss 3.8434, time 165.07ms, mfu 41.00%\n",
            "iter 8090: loss 3.8669, time 165.32ms, mfu 41.01%\n",
            "step 8100: train loss 3.8920, val loss 3.8840\n",
            "iter 8100: loss 3.8592, time 24907.40ms, mfu 36.94%\n",
            "iter 8110: loss 3.8880, time 164.60ms, mfu 37.37%\n",
            "iter 8120: loss 3.9283, time 164.47ms, mfu 37.77%\n",
            "iter 8130: loss 4.0002, time 165.80ms, mfu 38.09%\n",
            "iter 8140: loss 3.8486, time 164.67ms, mfu 38.41%\n",
            "iter 8150: loss 3.8702, time 164.30ms, mfu 38.70%\n",
            "iter 8160: loss 3.9280, time 165.45ms, mfu 38.94%\n",
            "iter 8170: loss 3.8920, time 164.60ms, mfu 39.18%\n",
            "iter 8180: loss 3.9847, time 164.64ms, mfu 39.39%\n",
            "iter 8190: loss 3.8818, time 166.13ms, mfu 39.54%\n",
            "iter 8200: loss 3.8609, time 164.90ms, mfu 39.71%\n",
            "iter 8210: loss 3.8934, time 165.26ms, mfu 39.85%\n",
            "iter 8220: loss 4.0374, time 164.83ms, mfu 39.99%\n",
            "iter 8230: loss 3.8284, time 165.52ms, mfu 40.09%\n",
            "iter 8240: loss 3.9179, time 164.38ms, mfu 40.22%\n",
            "iter 8250: loss 3.8625, time 164.80ms, mfu 40.32%\n",
            "iter 8260: loss 3.8305, time 164.82ms, mfu 40.41%\n",
            "iter 8270: loss 3.8510, time 164.73ms, mfu 40.50%\n",
            "iter 8280: loss 3.9078, time 164.69ms, mfu 40.57%\n",
            "iter 8290: loss 4.0002, time 165.12ms, mfu 40.63%\n",
            "iter 8300: loss 3.7904, time 164.50ms, mfu 40.70%\n",
            "iter 8310: loss 3.7936, time 165.63ms, mfu 40.73%\n",
            "iter 8320: loss 3.7836, time 164.60ms, mfu 40.79%\n",
            "iter 8330: loss 3.8731, time 164.61ms, mfu 40.84%\n",
            "iter 8340: loss 3.8156, time 165.66ms, mfu 40.86%\n",
            "iter 8350: loss 3.9392, time 164.60ms, mfu 40.90%\n",
            "iter 8360: loss 3.9851, time 164.45ms, mfu 40.94%\n",
            "iter 8370: loss 3.7938, time 165.80ms, mfu 40.95%\n",
            "iter 8380: loss 3.8322, time 164.72ms, mfu 40.98%\n",
            "iter 8390: loss 3.9349, time 164.28ms, mfu 41.02%\n",
            "step 8400: train loss 3.8975, val loss 3.8835\n",
            "iter 8400: loss 3.8824, time 24939.16ms, mfu 36.94%\n",
            "iter 8410: loss 3.9274, time 165.36ms, mfu 37.36%\n",
            "iter 8420: loss 3.9153, time 164.44ms, mfu 37.76%\n",
            "iter 8430: loss 3.7380, time 165.36ms, mfu 38.09%\n",
            "iter 8440: loss 3.8023, time 165.35ms, mfu 38.39%\n",
            "iter 8450: loss 3.8600, time 164.76ms, mfu 38.68%\n",
            "iter 8460: loss 3.9855, time 165.43ms, mfu 38.92%\n",
            "iter 8470: loss 3.9052, time 164.54ms, mfu 39.16%\n",
            "iter 8480: loss 3.8242, time 165.74ms, mfu 39.34%\n",
            "iter 8490: loss 3.8485, time 164.58ms, mfu 39.54%\n",
            "iter 8500: loss 4.0462, time 165.45ms, mfu 39.69%\n",
            "iter 8510: loss 3.8806, time 164.74ms, mfu 39.85%\n",
            "iter 8520: loss 3.9020, time 165.72ms, mfu 39.96%\n",
            "iter 8530: loss 4.0021, time 164.57ms, mfu 40.10%\n",
            "iter 8540: loss 3.8912, time 164.52ms, mfu 40.22%\n",
            "iter 8550: loss 3.7416, time 165.29ms, mfu 40.31%\n",
            "iter 8560: loss 3.9042, time 164.14ms, mfu 40.42%\n",
            "iter 8570: loss 3.9465, time 165.62ms, mfu 40.48%\n",
            "iter 8580: loss 3.8913, time 164.44ms, mfu 40.57%\n",
            "iter 8590: loss 3.8621, time 165.27ms, mfu 40.62%\n",
            "iter 8600: loss 3.8730, time 165.70ms, mfu 40.66%\n",
            "iter 8610: loss 3.9333, time 164.60ms, mfu 40.72%\n",
            "iter 8620: loss 3.9927, time 164.45ms, mfu 40.78%\n",
            "iter 8630: loss 3.8100, time 165.27ms, mfu 40.82%\n",
            "iter 8640: loss 3.9532, time 165.48ms, mfu 40.84%\n",
            "iter 8650: loss 3.8471, time 164.42ms, mfu 40.89%\n",
            "iter 8660: loss 3.8026, time 165.09ms, mfu 40.92%\n",
            "iter 8670: loss 3.8723, time 165.50ms, mfu 40.93%\n",
            "iter 8680: loss 3.9790, time 164.70ms, mfu 40.97%\n",
            "iter 8690: loss 3.7840, time 164.50ms, mfu 41.00%\n",
            "step 8700: train loss 3.8821, val loss 3.8825\n",
            "iter 8700: loss 3.7773, time 24922.92ms, mfu 36.93%\n",
            "iter 8710: loss 3.8267, time 165.35ms, mfu 37.35%\n",
            "iter 8720: loss 3.8867, time 165.62ms, mfu 37.72%\n",
            "iter 8730: loss 3.9387, time 164.52ms, mfu 38.08%\n",
            "iter 8740: loss 3.8503, time 165.82ms, mfu 38.37%\n",
            "iter 8750: loss 3.8429, time 164.48ms, mfu 38.66%\n",
            "iter 8760: loss 3.8734, time 165.32ms, mfu 38.91%\n",
            "iter 8770: loss 3.8029, time 164.72ms, mfu 39.14%\n",
            "iter 8780: loss 3.8311, time 165.43ms, mfu 39.34%\n",
            "iter 8790: loss 3.9688, time 165.09ms, mfu 39.52%\n",
            "iter 8800: loss 3.9082, time 165.33ms, mfu 39.68%\n",
            "iter 8810: loss 3.9565, time 164.51ms, mfu 39.84%\n",
            "iter 8820: loss 3.9176, time 165.46ms, mfu 39.97%\n",
            "iter 8830: loss 3.7872, time 164.90ms, mfu 40.09%\n",
            "iter 8840: loss 4.0111, time 164.49ms, mfu 40.21%\n",
            "iter 8850: loss 3.8635, time 165.31ms, mfu 40.30%\n",
            "iter 8860: loss 3.8668, time 164.38ms, mfu 40.41%\n",
            "iter 8870: loss 3.9672, time 164.51ms, mfu 40.50%\n",
            "iter 8880: loss 3.8744, time 165.60ms, mfu 40.55%\n",
            "iter 8890: loss 3.9209, time 165.23ms, mfu 40.61%\n",
            "iter 8900: loss 4.0571, time 164.66ms, mfu 40.68%\n",
            "iter 8910: loss 3.8579, time 164.44ms, mfu 40.74%\n",
            "iter 8920: loss 3.9441, time 165.43ms, mfu 40.78%\n",
            "iter 8930: loss 3.8179, time 165.47ms, mfu 40.81%\n",
            "iter 8940: loss 3.8392, time 164.55ms, mfu 40.86%\n",
            "iter 8950: loss 3.8893, time 164.80ms, mfu 40.89%\n",
            "iter 8960: loss 3.9066, time 165.60ms, mfu 40.91%\n",
            "iter 8970: loss 3.8413, time 165.29ms, mfu 40.93%\n",
            "iter 8980: loss 3.9372, time 164.87ms, mfu 40.96%\n",
            "iter 8990: loss 3.9892, time 165.26ms, mfu 40.98%\n",
            "step 9000: train loss 3.8862, val loss 3.8752\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 9000: loss 3.9675, time 27159.69ms, mfu 36.90%\n",
            "iter 9010: loss 3.9266, time 165.32ms, mfu 37.32%\n",
            "iter 9020: loss 3.9541, time 164.50ms, mfu 37.72%\n",
            "iter 9030: loss 3.8912, time 165.62ms, mfu 38.05%\n",
            "iter 9040: loss 3.9114, time 164.46ms, mfu 38.38%\n",
            "iter 9050: loss 3.9557, time 165.65ms, mfu 38.65%\n",
            "iter 9060: loss 3.8664, time 164.79ms, mfu 38.91%\n",
            "iter 9070: loss 3.7765, time 164.45ms, mfu 39.15%\n",
            "iter 9080: loss 3.8505, time 165.62ms, mfu 39.34%\n",
            "iter 9090: loss 3.8887, time 164.27ms, mfu 39.54%\n",
            "iter 9100: loss 3.8584, time 165.58ms, mfu 39.69%\n",
            "iter 9110: loss 3.8116, time 164.55ms, mfu 39.85%\n",
            "iter 9120: loss 3.8940, time 164.58ms, mfu 40.00%\n",
            "iter 9130: loss 3.8202, time 165.51ms, mfu 40.10%\n",
            "iter 9140: loss 3.9233, time 164.65ms, mfu 40.22%\n",
            "iter 9150: loss 3.8458, time 164.72ms, mfu 40.32%\n",
            "iter 9160: loss 3.9563, time 165.53ms, mfu 40.40%\n",
            "iter 9170: loss 3.8945, time 164.64ms, mfu 40.49%\n",
            "iter 9180: loss 3.8753, time 165.44ms, mfu 40.55%\n",
            "iter 9190: loss 3.8986, time 165.35ms, mfu 40.60%\n",
            "iter 9200: loss 3.8232, time 164.42ms, mfu 40.67%\n",
            "iter 9210: loss 3.9503, time 165.39ms, mfu 40.72%\n",
            "iter 9220: loss 3.9472, time 164.83ms, mfu 40.77%\n",
            "iter 9230: loss 3.8700, time 164.48ms, mfu 40.82%\n",
            "iter 9240: loss 3.9544, time 165.45ms, mfu 40.85%\n",
            "iter 9250: loss 3.9258, time 164.53ms, mfu 40.89%\n",
            "iter 9260: loss 4.0419, time 164.32ms, mfu 40.94%\n",
            "iter 9270: loss 3.8820, time 165.59ms, mfu 40.95%\n",
            "iter 9280: loss 3.8172, time 164.67ms, mfu 40.98%\n",
            "iter 9290: loss 3.8110, time 164.38ms, mfu 41.02%\n",
            "step 9300: train loss 3.8811, val loss 3.8758\n",
            "iter 9300: loss 3.8510, time 24910.41ms, mfu 36.94%\n",
            "iter 9310: loss 3.9912, time 165.11ms, mfu 37.37%\n",
            "iter 9320: loss 3.9161, time 165.39ms, mfu 37.74%\n",
            "iter 9330: loss 3.9564, time 164.61ms, mfu 38.09%\n",
            "iter 9340: loss 3.9265, time 165.30ms, mfu 38.40%\n",
            "iter 9350: loss 3.8072, time 164.83ms, mfu 38.68%\n",
            "iter 9360: loss 3.8829, time 164.87ms, mfu 38.93%\n",
            "iter 9370: loss 3.8506, time 165.39ms, mfu 39.15%\n",
            "iter 9380: loss 4.0996, time 165.69ms, mfu 39.34%\n",
            "iter 9390: loss 3.9147, time 164.80ms, mfu 39.53%\n",
            "iter 9400: loss 3.9586, time 164.38ms, mfu 39.71%\n",
            "iter 9410: loss 3.8245, time 165.46ms, mfu 39.85%\n",
            "iter 9420: loss 3.8291, time 165.78ms, mfu 39.96%\n",
            "iter 9430: loss 3.8439, time 164.62ms, mfu 40.09%\n",
            "iter 9440: loss 3.9607, time 164.51ms, mfu 40.22%\n",
            "iter 9450: loss 3.8352, time 165.44ms, mfu 40.30%\n",
            "iter 9460: loss 3.8921, time 165.26ms, mfu 40.38%\n",
            "iter 9470: loss 3.9255, time 164.60ms, mfu 40.47%\n",
            "iter 9480: loss 3.8895, time 164.37ms, mfu 40.56%\n",
            "iter 9490: loss 3.8416, time 165.40ms, mfu 40.61%\n",
            "iter 9500: loss 3.8226, time 165.64ms, mfu 40.66%\n",
            "iter 9510: loss 3.8730, time 164.69ms, mfu 40.72%\n",
            "iter 9520: loss 3.9849, time 164.75ms, mfu 40.77%\n",
            "iter 9530: loss 3.9328, time 165.42ms, mfu 40.80%\n",
            "iter 9540: loss 3.8369, time 165.43ms, mfu 40.83%\n",
            "iter 9550: loss 3.8146, time 164.56ms, mfu 40.88%\n",
            "iter 9560: loss 3.9126, time 165.23ms, mfu 40.90%\n",
            "iter 9570: loss 3.9000, time 165.45ms, mfu 40.92%\n",
            "iter 9580: loss 3.9007, time 164.58ms, mfu 40.96%\n",
            "iter 9590: loss 3.8976, time 164.58ms, mfu 40.99%\n",
            "step 9600: train loss 3.8859, val loss 3.8700\n",
            "saving checkpoint to /content/SimpleGPT\n",
            "iter 9600: loss 3.9597, time 27165.59ms, mfu 36.92%\n",
            "iter 9610: loss 3.8467, time 165.30ms, mfu 37.34%\n",
            "iter 9620: loss 3.9767, time 164.36ms, mfu 37.74%\n",
            "iter 9630: loss 4.0216, time 165.37ms, mfu 38.07%\n",
            "iter 9640: loss 3.9899, time 165.02ms, mfu 38.39%\n",
            "iter 9650: loss 3.9351, time 164.57ms, mfu 38.68%\n",
            "iter 9660: loss 3.9143, time 165.10ms, mfu 38.93%\n",
            "iter 9670: loss 4.0149, time 165.35ms, mfu 39.14%\n",
            "iter 9680: loss 3.9033, time 164.54ms, mfu 39.36%\n",
            "iter 9690: loss 4.0728, time 165.39ms, mfu 39.53%\n",
            "iter 9700: loss 3.8203, time 164.27ms, mfu 39.72%\n",
            "iter 9710: loss 3.8616, time 164.92ms, mfu 39.87%\n",
            "iter 9720: loss 3.9654, time 164.22ms, mfu 40.02%\n",
            "iter 9730: loss 3.8795, time 164.49ms, mfu 40.15%\n",
            "iter 9740: loss 3.9546, time 165.35ms, mfu 40.24%\n",
            "iter 9750: loss 3.9336, time 164.65ms, mfu 40.35%\n",
            "iter 9760: loss 3.9013, time 164.56ms, mfu 40.44%\n",
            "iter 9770: loss 3.9489, time 164.62ms, mfu 40.53%\n",
            "iter 9780: loss 3.8010, time 164.51ms, mfu 40.61%\n",
            "iter 9790: loss 3.7835, time 165.11ms, mfu 40.66%\n",
            "iter 9800: loss 3.9183, time 164.51ms, mfu 40.73%\n",
            "iter 9810: loss 3.8570, time 164.58ms, mfu 40.78%\n",
            "iter 9820: loss 3.9984, time 165.18ms, mfu 40.82%\n",
            "iter 9830: loss 3.9321, time 164.48ms, mfu 40.87%\n",
            "iter 9840: loss 3.8786, time 165.20ms, mfu 40.90%\n",
            "iter 9850: loss 3.9204, time 165.33ms, mfu 40.92%\n",
            "iter 9860: loss 3.8975, time 164.43ms, mfu 40.96%\n",
            "iter 9870: loss 3.8855, time 165.23ms, mfu 40.98%\n",
            "iter 9880: loss 3.9031, time 164.68ms, mfu 41.01%\n",
            "iter 9890: loss 3.8936, time 165.46ms, mfu 41.01%\n",
            "step 9900: train loss 3.8838, val loss 3.8749\n",
            "iter 9900: loss 3.9147, time 24931.05ms, mfu 36.94%\n",
            "iter 9910: loss 3.8277, time 164.63ms, mfu 37.37%\n",
            "iter 9920: loss 3.9836, time 164.63ms, mfu 37.76%\n",
            "iter 9930: loss 3.8775, time 165.55ms, mfu 38.09%\n",
            "iter 9940: loss 3.8089, time 164.66ms, mfu 38.41%\n",
            "iter 9950: loss 3.8693, time 164.60ms, mfu 38.70%\n",
            "iter 9960: loss 3.9218, time 165.40ms, mfu 38.94%\n",
            "iter 9970: loss 3.9179, time 165.25ms, mfu 39.16%\n",
            "iter 9980: loss 3.8506, time 164.34ms, mfu 39.38%\n",
            "iter 9990: loss 3.7825, time 165.39ms, mfu 39.55%\n",
            "iter 10000: loss 3.8863, time 165.34ms, mfu 39.70%\n"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7C6D6nX6Q-9"
      },
      "source": [
        "## GPT: Reflex-Attention: (2)\n",
        "\n",
        "**1-6 layer: 2head: SA, 2head: CA(h of prev layer), 2head: CA(h of prev2x layer)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quykrA4b6Q-9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "class ReflexAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # fullSA for all 6heads:\n",
        "        # splitting embeds for 6heads\n",
        "\n",
        "        #i_hidden_state:\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        if self.model_type == 'reflex' and hidden_states.__len__() < 2:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v,\n",
        "                                                                 attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        else:\n",
        "            # reflex_attention 6heads from\n",
        "            k_prevx, v_prevx = hidden_states[1]\n",
        "            k_prevxx, v_prevxx = hidden_states[0]\n",
        "\n",
        "            #SA_i=SA(k_i, v_i, q_i) for 2head\n",
        "            q_i, k_i, v_i = q[:, :2, :, :], k[:, :2, :, :], v[:, :2, :, :]\n",
        "            SA_i = torch.nn.functional.scaled_dot_product_attention(q_i, k_i, v_i,\n",
        "                                                                    attn_mask=None,\n",
        "                                                                    dropout_p=self.dropout if self.training else 0,\n",
        "                                                                    is_causal=True)\n",
        "            #SA_i=SA(k_i, v_i, q_i) for 2head\n",
        "\n",
        "            q_i_for_prevx_head, k_prevx_head, v_prevx_head = q[:, 2:4, :, :], k_prevx[:, 2:4, :, :], v_prevx[:, 2:4, :, :]\n",
        "            CA_ix = torch.nn.functional.scaled_dot_product_attention(q_i_for_prevx_head, k_prevx_head, v_prevx_head,\n",
        "                                                                                attn_mask=None,\n",
        "                                                                                dropout_p=self.dropout if self.training else 0,\n",
        "                                                                                is_causal=True)\n",
        "            # 2 head for previous previous layer\n",
        "\n",
        "            q_i_for_prevxx_head, k_prevxx_head, v_prevxx_head = q[:, 4:, :, :], k_prevxx[:, 4:, :, :], v_prevxx[:, 4:, :, :]\n",
        "            CA_ixx = torch.nn.functional.scaled_dot_product_attention(q_i_for_prevxx_head, k_prevxx_head, v_prevxx_head,\n",
        "                                                                                attn_mask=None,\n",
        "                                                                                dropout_p=self.dropout if self.training else 0,\n",
        "                                                                                is_causal=True)\n",
        "            # concatenate: Attn_i= Cat[SA_i, CA_i_i-1, CA_i_i-2]\n",
        "            y = torch.cat([SA_i, CA_ix, CA_ixx], dim=1)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y, (k, v)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = ReflexAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        attn_kv = self.attn(self.ln_1(x), hidden_states)\n",
        "        x = x + attn_kv[0] #y for 0\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_kv[1] #hidden_states_i for k,v for 1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        hidden_states = []\n",
        "\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x, hidden_state = block(x, hidden_states)\n",
        "            # level1\n",
        "            if self.config.model_type == 'reflex':\n",
        "                if i >= 2:\n",
        "                  hidden_states.pop(0)\n",
        "                hidden_states.append(hidden_state)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "V1bwB0sk6Q-_",
        "outputId": "dd7747f7-5de8-4cee-ceb5-2328a78fd4e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ],
            "text/plain": [
              "__main__.GPT"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4Q3TtCD6Q_A"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'reflex' # or 'simple'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-foVR0n6Q_A",
        "outputId": "04da75ec-23ac-4d66-aafb-76675c95558f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA4wdZTa6Q_B"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'reflexAttnGPT={config}1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OarsF1-L6Q_B"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/reglex_attn_GPT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpy93W4y6Q_B",
        "outputId": "00b169da-2805-4c1f-ff7b-a1f0b267cb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 32,768\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fImf_yqR6Q_C"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYqehgHG6Q_C",
        "outputId": "fe0dd48f-ceaf-4463-c522-fa18837ba068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0BVz1fX6Q_D",
        "outputId": "8406f2da-a992-41b9-dd08-b43b805a648c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 13, with 9,984 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-37-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LTDm-SUN6Q_D",
        "outputId": "e4077e28-4c4e-4ae7-d521-2f4b843cd61c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:f25ngbln) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">reflexAttnGPT=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/f25ngbln' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/f25ngbln</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241124_214029-f25ngbln/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:f25ngbln). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241124_214140-oxit79vl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/oxit79vl' target=\"_blank\">reflexAttnGPT=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/oxit79vl' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/oxit79vl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 163 \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 110 \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 17 \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 35 \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 94 \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.9687, val loss 10.9696\n",
            "iter 0: loss 10.9723, time 27964.86ms, mfu -100.00%\n",
            "iter 10: loss 9.7752, time 158.71ms, mfu 35.95%\n",
            "iter 20: loss 9.2403, time 159.47ms, mfu 35.94%\n",
            "iter 30: loss 8.7068, time 160.25ms, mfu 35.90%\n",
            "iter 40: loss 8.0201, time 159.29ms, mfu 35.89%\n",
            "iter 50: loss 7.3922, time 158.90ms, mfu 35.90%\n",
            "iter 60: loss 7.2296, time 158.03ms, mfu 35.92%\n",
            "iter 70: loss 7.0705, time 158.37ms, mfu 35.93%\n",
            "iter 80: loss 7.1468, time 158.76ms, mfu 35.93%\n",
            "iter 90: loss 6.7766, time 159.08ms, mfu 35.92%\n",
            "iter 100: loss 6.7940, time 159.37ms, mfu 35.91%\n",
            "iter 110: loss 6.6914, time 158.85ms, mfu 35.91%\n",
            "iter 120: loss 6.5497, time 159.08ms, mfu 35.91%\n",
            "iter 130: loss 6.3929, time 158.87ms, mfu 35.91%\n",
            "iter 140: loss 6.3806, time 158.94ms, mfu 35.91%\n",
            "iter 150: loss 6.3009, time 159.19ms, mfu 35.90%\n",
            "iter 160: loss 6.4253, time 159.20ms, mfu 35.90%\n",
            "iter 170: loss 6.3922, time 158.84ms, mfu 35.90%\n",
            "iter 180: loss 6.1838, time 158.98ms, mfu 35.90%\n",
            "iter 190: loss 6.2093, time 159.48ms, mfu 35.89%\n",
            "iter 200: loss 6.2313, time 159.39ms, mfu 35.88%\n",
            "iter 210: loss 6.2009, time 159.64ms, mfu 35.86%\n",
            "iter 220: loss 6.0941, time 159.83ms, mfu 35.85%\n",
            "iter 230: loss 6.2819, time 160.34ms, mfu 35.82%\n",
            "iter 240: loss 6.0491, time 160.08ms, mfu 35.80%\n",
            "iter 250: loss 6.0417, time 159.84ms, mfu 35.79%\n",
            "iter 260: loss 6.0717, time 158.97ms, mfu 35.80%\n",
            "iter 270: loss 6.1343, time 158.90ms, mfu 35.81%\n",
            "iter 280: loss 6.1254, time 159.62ms, mfu 35.81%\n",
            "iter 290: loss 6.0618, time 159.06ms, mfu 35.81%\n",
            "step 300: train loss 6.0098, val loss 5.9581\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 300: loss 5.8438, time 24044.12ms, mfu 32.26%\n",
            "iter 310: loss 5.9054, time 160.43ms, mfu 32.59%\n",
            "iter 320: loss 5.9453, time 159.92ms, mfu 32.90%\n",
            "iter 330: loss 5.9252, time 160.00ms, mfu 33.17%\n",
            "iter 340: loss 6.0186, time 160.45ms, mfu 33.41%\n",
            "iter 350: loss 5.9117, time 160.07ms, mfu 33.63%\n",
            "iter 360: loss 5.7801, time 160.34ms, mfu 33.83%\n",
            "iter 370: loss 5.8110, time 160.70ms, mfu 34.00%\n",
            "iter 380: loss 5.8372, time 160.10ms, mfu 34.16%\n",
            "iter 390: loss 5.7918, time 159.52ms, mfu 34.32%\n",
            "iter 400: loss 5.7597, time 158.92ms, mfu 34.48%\n",
            "iter 410: loss 5.7818, time 159.37ms, mfu 34.61%\n",
            "iter 420: loss 5.7871, time 159.01ms, mfu 34.74%\n",
            "iter 430: loss 5.7067, time 159.30ms, mfu 34.85%\n",
            "iter 440: loss 5.7979, time 159.90ms, mfu 34.93%\n",
            "iter 450: loss 5.8350, time 160.33ms, mfu 35.00%\n",
            "iter 460: loss 5.7481, time 160.30ms, mfu 35.06%\n",
            "iter 470: loss 5.7544, time 159.60ms, mfu 35.13%\n",
            "iter 480: loss 5.6953, time 158.82ms, mfu 35.21%\n",
            "iter 490: loss 5.7643, time 159.25ms, mfu 35.27%\n",
            "iter 500: loss 5.6469, time 159.71ms, mfu 35.31%\n",
            "iter 510: loss 5.6120, time 160.52ms, mfu 35.34%\n",
            "iter 520: loss 5.5958, time 160.53ms, mfu 35.36%\n",
            "iter 530: loss 5.6727, time 159.45ms, mfu 35.40%\n",
            "iter 540: loss 5.6501, time 158.84ms, mfu 35.45%\n",
            "iter 550: loss 5.6794, time 159.30ms, mfu 35.49%\n",
            "iter 560: loss 5.6105, time 159.92ms, mfu 35.51%\n",
            "iter 570: loss 5.5731, time 160.27ms, mfu 35.52%\n",
            "iter 580: loss 5.6480, time 159.83ms, mfu 35.54%\n",
            "iter 590: loss 5.5176, time 158.94ms, mfu 35.57%\n",
            "step 600: train loss 5.5201, val loss 5.4791\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 600: loss 5.3081, time 24927.16ms, mfu 32.04%\n",
            "iter 610: loss 5.5490, time 159.38ms, mfu 32.41%\n",
            "iter 620: loss 5.2449, time 159.71ms, mfu 32.75%\n",
            "iter 630: loss 5.4314, time 159.41ms, mfu 33.05%\n",
            "iter 640: loss 5.4711, time 159.93ms, mfu 33.31%\n",
            "iter 650: loss 5.3135, time 160.12ms, mfu 33.55%\n",
            "iter 660: loss 5.4598, time 159.74ms, mfu 33.76%\n",
            "iter 670: loss 5.3846, time 160.42ms, mfu 33.94%\n",
            "iter 680: loss 5.3663, time 160.51ms, mfu 34.10%\n",
            "iter 690: loss 5.4706, time 159.87ms, mfu 34.26%\n",
            "iter 700: loss 5.2440, time 159.26ms, mfu 34.42%\n",
            "iter 710: loss 5.4993, time 159.32ms, mfu 34.56%\n",
            "iter 720: loss 5.2520, time 160.12ms, mfu 34.67%\n",
            "iter 730: loss 5.3302, time 160.55ms, mfu 34.75%\n",
            "iter 740: loss 5.2834, time 160.46ms, mfu 34.83%\n",
            "iter 750: loss 5.4660, time 159.40ms, mfu 34.93%\n",
            "iter 760: loss 5.0932, time 158.79ms, mfu 35.03%\n",
            "iter 770: loss 5.2244, time 159.15ms, mfu 35.11%\n",
            "iter 780: loss 5.1930, time 159.67ms, mfu 35.18%\n",
            "iter 790: loss 5.2482, time 161.02ms, mfu 35.20%\n",
            "iter 800: loss 5.1178, time 160.16ms, mfu 35.24%\n",
            "iter 810: loss 5.1857, time 159.13ms, mfu 35.31%\n",
            "iter 820: loss 5.1791, time 159.39ms, mfu 35.35%\n",
            "iter 830: loss 5.2377, time 159.81ms, mfu 35.39%\n",
            "iter 840: loss 5.1999, time 160.09ms, mfu 35.41%\n",
            "iter 850: loss 5.2142, time 160.50ms, mfu 35.43%\n",
            "iter 860: loss 5.1885, time 160.11ms, mfu 35.45%\n",
            "iter 870: loss 5.1122, time 158.95ms, mfu 35.49%\n",
            "iter 880: loss 5.0547, time 159.50ms, mfu 35.52%\n",
            "iter 890: loss 5.0582, time 159.61ms, mfu 35.54%\n",
            "step 900: train loss 5.1193, val loss 5.0778\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 900: loss 5.1538, time 24482.67ms, mfu 32.01%\n",
            "iter 910: loss 5.0937, time 158.68ms, mfu 32.41%\n",
            "iter 920: loss 5.0240, time 158.96ms, mfu 32.76%\n",
            "iter 930: loss 5.1107, time 158.72ms, mfu 33.08%\n",
            "iter 940: loss 5.2438, time 159.19ms, mfu 33.35%\n",
            "iter 950: loss 5.3007, time 159.35ms, mfu 33.60%\n",
            "iter 960: loss 5.0333, time 159.73ms, mfu 33.81%\n",
            "iter 970: loss 4.9773, time 160.14ms, mfu 33.99%\n",
            "iter 980: loss 4.9855, time 160.06ms, mfu 34.16%\n",
            "iter 990: loss 5.0335, time 160.00ms, mfu 34.31%\n",
            "iter 1000: loss 4.9374, time 158.87ms, mfu 34.47%\n",
            "iter 1010: loss 4.8034, time 160.14ms, mfu 34.59%\n",
            "iter 1020: loss 4.8237, time 160.19ms, mfu 34.69%\n",
            "iter 1030: loss 4.8585, time 160.30ms, mfu 34.78%\n",
            "iter 1040: loss 4.9504, time 160.34ms, mfu 34.86%\n",
            "iter 1050: loss 4.9508, time 159.79ms, mfu 34.95%\n",
            "iter 1060: loss 4.8588, time 159.72ms, mfu 35.02%\n",
            "iter 1070: loss 4.8920, time 160.09ms, mfu 35.09%\n",
            "iter 1080: loss 4.8257, time 160.72ms, mfu 35.13%\n",
            "iter 1090: loss 4.8724, time 160.43ms, mfu 35.17%\n",
            "iter 1100: loss 4.8356, time 159.48ms, mfu 35.23%\n",
            "iter 1110: loss 4.9028, time 158.96ms, mfu 35.30%\n",
            "iter 1120: loss 4.8590, time 159.21ms, mfu 35.35%\n",
            "iter 1130: loss 4.8465, time 160.09ms, mfu 35.38%\n",
            "iter 1140: loss 4.7866, time 160.42ms, mfu 35.40%\n",
            "iter 1150: loss 4.9028, time 159.71ms, mfu 35.43%\n",
            "iter 1160: loss 4.8385, time 159.40ms, mfu 35.47%\n",
            "iter 1170: loss 4.8666, time 158.89ms, mfu 35.51%\n",
            "iter 1180: loss 4.7932, time 159.38ms, mfu 35.54%\n",
            "iter 1190: loss 4.7774, time 159.91ms, mfu 35.56%\n",
            "step 1200: train loss 4.7741, val loss 4.7461\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1200: loss 4.7595, time 24647.31ms, mfu 32.02%\n",
            "iter 1210: loss 4.7241, time 159.82ms, mfu 32.39%\n",
            "iter 1220: loss 4.8469, time 159.64ms, mfu 32.73%\n",
            "iter 1230: loss 4.8212, time 160.32ms, mfu 33.01%\n",
            "iter 1240: loss 4.7148, time 159.84ms, mfu 33.28%\n",
            "iter 1250: loss 4.7800, time 159.10ms, mfu 33.54%\n",
            "iter 1260: loss 4.6979, time 159.42ms, mfu 33.76%\n",
            "iter 1270: loss 4.7242, time 160.24ms, mfu 33.95%\n",
            "iter 1280: loss 4.8569, time 160.51ms, mfu 34.11%\n",
            "iter 1290: loss 4.6738, time 160.33ms, mfu 34.26%\n",
            "iter 1300: loss 4.7328, time 159.57ms, mfu 34.41%\n",
            "iter 1310: loss 4.6631, time 159.44ms, mfu 34.55%\n",
            "iter 1320: loss 4.6817, time 159.96ms, mfu 34.66%\n",
            "iter 1330: loss 4.6336, time 160.38ms, mfu 34.75%\n",
            "iter 1340: loss 4.7153, time 160.50ms, mfu 34.83%\n",
            "iter 1350: loss 4.7314, time 160.20ms, mfu 34.91%\n",
            "iter 1360: loss 4.6471, time 159.03ms, mfu 35.01%\n",
            "iter 1370: loss 4.6626, time 159.21ms, mfu 35.09%\n",
            "iter 1380: loss 4.5594, time 159.61ms, mfu 35.16%\n",
            "iter 1390: loss 4.6328, time 160.06ms, mfu 35.20%\n",
            "iter 1400: loss 4.6690, time 160.24ms, mfu 35.24%\n",
            "iter 1410: loss 4.5082, time 160.27ms, mfu 35.28%\n",
            "iter 1420: loss 4.5821, time 159.40ms, mfu 35.33%\n",
            "iter 1430: loss 4.5430, time 159.23ms, mfu 35.38%\n",
            "iter 1440: loss 4.6622, time 159.03ms, mfu 35.43%\n",
            "iter 1450: loss 4.5930, time 160.09ms, mfu 35.45%\n",
            "iter 1460: loss 4.5764, time 160.38ms, mfu 35.47%\n",
            "iter 1470: loss 4.5010, time 160.49ms, mfu 35.47%\n",
            "iter 1480: loss 4.5572, time 160.81ms, mfu 35.47%\n",
            "iter 1490: loss 4.5655, time 160.19ms, mfu 35.49%\n",
            "step 1500: train loss 4.5717, val loss 4.5465\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1500: loss 4.5962, time 24503.82ms, mfu 31.96%\n",
            "iter 1510: loss 4.4909, time 159.20ms, mfu 32.35%\n",
            "iter 1520: loss 4.5522, time 158.96ms, mfu 32.71%\n",
            "iter 1530: loss 4.6539, time 160.03ms, mfu 33.00%\n",
            "iter 1540: loss 4.5507, time 160.34ms, mfu 33.26%\n",
            "iter 1550: loss 4.4875, time 160.14ms, mfu 33.50%\n",
            "iter 1560: loss 4.4704, time 158.98ms, mfu 33.74%\n",
            "iter 1570: loss 4.5489, time 159.27ms, mfu 33.94%\n",
            "iter 1580: loss 4.4167, time 159.38ms, mfu 34.13%\n",
            "iter 1590: loss 4.4819, time 159.89ms, mfu 34.29%\n",
            "iter 1600: loss 4.6455, time 160.17ms, mfu 34.42%\n",
            "iter 1610: loss 4.4688, time 159.97ms, mfu 34.54%\n",
            "iter 1620: loss 4.5188, time 159.44ms, mfu 34.67%\n",
            "iter 1630: loss 4.5777, time 159.63ms, mfu 34.78%\n",
            "iter 1640: loss 4.4513, time 160.56ms, mfu 34.85%\n",
            "iter 1650: loss 4.4967, time 160.42ms, mfu 34.92%\n",
            "iter 1660: loss 4.5514, time 159.97ms, mfu 35.00%\n",
            "iter 1670: loss 4.4845, time 159.19ms, mfu 35.08%\n",
            "iter 1680: loss 4.3266, time 159.08ms, mfu 35.16%\n",
            "iter 1690: loss 4.4460, time 158.88ms, mfu 35.24%\n",
            "iter 1700: loss 4.4394, time 159.44ms, mfu 35.29%\n",
            "iter 1710: loss 4.3751, time 159.89ms, mfu 35.33%\n",
            "iter 1720: loss 4.4365, time 160.26ms, mfu 35.36%\n",
            "iter 1730: loss 4.5347, time 160.22ms, mfu 35.38%\n",
            "iter 1740: loss 4.4473, time 160.26ms, mfu 35.41%\n",
            "iter 1750: loss 4.5203, time 159.97ms, mfu 35.43%\n",
            "iter 1760: loss 4.4531, time 160.32ms, mfu 35.45%\n",
            "iter 1770: loss 4.5901, time 160.30ms, mfu 35.46%\n",
            "iter 1780: loss 4.4221, time 161.13ms, mfu 35.46%\n",
            "iter 1790: loss 4.4979, time 160.39ms, mfu 35.47%\n",
            "step 1800: train loss 4.4358, val loss 4.4243\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1800: loss 4.5921, time 24651.85ms, mfu 31.95%\n",
            "iter 1810: loss 4.5015, time 160.61ms, mfu 32.30%\n",
            "iter 1820: loss 4.4728, time 159.38ms, mfu 32.65%\n",
            "iter 1830: loss 4.4489, time 159.00ms, mfu 32.98%\n",
            "iter 1840: loss 4.3906, time 159.01ms, mfu 33.27%\n",
            "iter 1850: loss 4.5018, time 159.32ms, mfu 33.52%\n",
            "iter 1860: loss 4.4511, time 159.94ms, mfu 33.74%\n",
            "iter 1870: loss 4.4327, time 160.11ms, mfu 33.93%\n",
            "iter 1880: loss 4.4269, time 160.41ms, mfu 34.09%\n",
            "iter 1890: loss 4.4014, time 159.64ms, mfu 34.26%\n",
            "iter 1900: loss 4.4040, time 158.97ms, mfu 34.42%\n",
            "iter 1910: loss 4.6024, time 159.54ms, mfu 34.55%\n",
            "iter 1920: loss 4.4107, time 160.07ms, mfu 34.66%\n",
            "iter 1930: loss 4.4247, time 160.38ms, mfu 34.76%\n",
            "iter 1940: loss 4.1894, time 160.55ms, mfu 34.83%\n",
            "iter 1950: loss 4.5191, time 159.72ms, mfu 34.92%\n",
            "iter 1960: loss 4.4915, time 158.99ms, mfu 35.02%\n",
            "iter 1970: loss 4.3943, time 159.09ms, mfu 35.10%\n",
            "iter 1980: loss 4.4074, time 159.75ms, mfu 35.17%\n",
            "iter 1990: loss 4.3803, time 159.63ms, mfu 35.22%\n",
            "iter 2000: loss 4.4684, time 160.29ms, mfu 35.26%\n",
            "iter 2010: loss 4.4285, time 160.14ms, mfu 35.30%\n",
            "iter 2020: loss 4.2808, time 160.38ms, mfu 35.33%\n",
            "iter 2030: loss 4.2704, time 160.11ms, mfu 35.36%\n",
            "iter 2040: loss 4.3013, time 161.79ms, mfu 35.35%\n",
            "iter 2050: loss 4.4072, time 161.59ms, mfu 35.34%\n",
            "iter 2060: loss 4.2040, time 160.04ms, mfu 35.38%\n",
            "iter 2070: loss 4.4591, time 160.48ms, mfu 35.39%\n",
            "iter 2080: loss 4.3609, time 160.40ms, mfu 35.41%\n",
            "iter 2090: loss 4.4022, time 158.97ms, mfu 35.46%\n",
            "step 2100: train loss 4.3525, val loss 4.3411\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2100: loss 4.3763, time 24271.25ms, mfu 31.94%\n",
            "iter 2110: loss 4.3727, time 158.65ms, mfu 32.34%\n",
            "iter 2120: loss 4.2367, time 159.42ms, mfu 32.69%\n",
            "iter 2130: loss 4.4296, time 159.87ms, mfu 32.99%\n",
            "iter 2140: loss 4.4671, time 160.31ms, mfu 33.25%\n",
            "iter 2150: loss 4.3579, time 160.06ms, mfu 33.49%\n",
            "iter 2160: loss 4.4261, time 159.39ms, mfu 33.72%\n",
            "iter 2170: loss 4.4320, time 159.36ms, mfu 33.93%\n",
            "iter 2180: loss 4.3139, time 158.79ms, mfu 34.13%\n",
            "iter 2190: loss 4.2813, time 159.21ms, mfu 34.30%\n",
            "iter 2200: loss 4.2657, time 159.15ms, mfu 34.45%\n",
            "iter 2210: loss 4.3506, time 159.96ms, mfu 34.58%\n",
            "iter 2220: loss 4.2336, time 160.41ms, mfu 34.67%\n",
            "iter 2230: loss 4.2920, time 160.19ms, mfu 34.77%\n",
            "iter 2240: loss 4.2945, time 160.18ms, mfu 34.85%\n",
            "iter 2250: loss 4.3022, time 160.00ms, mfu 34.94%\n",
            "iter 2260: loss 4.3042, time 159.38ms, mfu 35.02%\n",
            "iter 2270: loss 4.4005, time 159.10ms, mfu 35.11%\n",
            "iter 2280: loss 4.2786, time 159.63ms, mfu 35.17%\n",
            "iter 2290: loss 4.3604, time 159.20ms, mfu 35.24%\n",
            "iter 2300: loss 4.1306, time 159.20ms, mfu 35.30%\n",
            "iter 2310: loss 4.3683, time 159.90ms, mfu 35.34%\n",
            "iter 2320: loss 4.2766, time 159.88ms, mfu 35.37%\n",
            "iter 2330: loss 4.2657, time 159.94ms, mfu 35.40%\n",
            "iter 2340: loss 4.2788, time 159.50ms, mfu 35.44%\n",
            "iter 2350: loss 4.3771, time 159.16ms, mfu 35.48%\n",
            "iter 2360: loss 4.3171, time 158.95ms, mfu 35.52%\n",
            "iter 2370: loss 4.3063, time 159.93ms, mfu 35.54%\n",
            "iter 2380: loss 4.3162, time 160.95ms, mfu 35.53%\n",
            "iter 2390: loss 4.1931, time 159.77ms, mfu 35.55%\n",
            "step 2400: train loss 4.2600, val loss 4.2384\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2400: loss 4.1830, time 24263.17ms, mfu 32.02%\n",
            "iter 2410: loss 4.3101, time 159.38ms, mfu 32.39%\n",
            "iter 2420: loss 4.3128, time 159.56ms, mfu 32.73%\n",
            "iter 2430: loss 4.2362, time 160.37ms, mfu 33.02%\n",
            "iter 2440: loss 4.2671, time 160.42ms, mfu 33.27%\n",
            "iter 2450: loss 4.2846, time 159.65ms, mfu 33.52%\n",
            "iter 2460: loss 4.2492, time 158.74ms, mfu 33.76%\n",
            "iter 2470: loss 4.2132, time 159.16ms, mfu 33.97%\n",
            "iter 2480: loss 4.2943, time 160.07ms, mfu 34.14%\n",
            "iter 2490: loss 4.1454, time 160.25ms, mfu 34.28%\n",
            "iter 2500: loss 4.1782, time 160.42ms, mfu 34.41%\n",
            "iter 2510: loss 4.1902, time 160.41ms, mfu 34.53%\n",
            "iter 2520: loss 4.2000, time 159.64ms, mfu 34.65%\n",
            "iter 2530: loss 4.2479, time 159.59ms, mfu 34.76%\n",
            "iter 2540: loss 4.1834, time 159.52ms, mfu 34.86%\n",
            "iter 2550: loss 4.3335, time 159.87ms, mfu 34.94%\n",
            "iter 2560: loss 4.2790, time 159.54ms, mfu 35.03%\n",
            "iter 2570: loss 4.1597, time 159.60ms, mfu 35.10%\n",
            "iter 2580: loss 4.2097, time 159.23ms, mfu 35.17%\n",
            "iter 2590: loss 4.2517, time 158.92ms, mfu 35.25%\n",
            "iter 2600: loss 4.0654, time 158.82ms, mfu 35.31%\n",
            "iter 2610: loss 4.3465, time 158.91ms, mfu 35.37%\n",
            "iter 2620: loss 4.2303, time 159.10ms, mfu 35.42%\n",
            "iter 2630: loss 4.2768, time 159.41ms, mfu 35.46%\n",
            "iter 2640: loss 4.1892, time 159.69ms, mfu 35.49%\n",
            "iter 2650: loss 4.2536, time 159.86ms, mfu 35.51%\n",
            "iter 2660: loss 4.1215, time 158.98ms, mfu 35.55%\n",
            "iter 2670: loss 4.1683, time 159.68ms, mfu 35.56%\n",
            "iter 2680: loss 4.1534, time 159.98ms, mfu 35.57%\n",
            "iter 2690: loss 4.3042, time 160.09ms, mfu 35.58%\n",
            "step 2700: train loss 4.1859, val loss 4.1749\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2700: loss 4.2978, time 24221.33ms, mfu 32.05%\n",
            "iter 2710: loss 4.1140, time 159.05ms, mfu 32.43%\n",
            "iter 2720: loss 4.1902, time 159.21ms, mfu 32.77%\n",
            "iter 2730: loss 4.1569, time 159.64ms, mfu 33.07%\n",
            "iter 2740: loss 4.1343, time 160.19ms, mfu 33.32%\n",
            "iter 2750: loss 4.1884, time 160.36ms, mfu 33.55%\n",
            "iter 2760: loss 4.1648, time 159.65ms, mfu 33.77%\n",
            "iter 2770: loss 4.1947, time 158.93ms, mfu 33.98%\n",
            "iter 2780: loss 4.1559, time 158.86ms, mfu 34.17%\n",
            "iter 2790: loss 4.2039, time 159.22ms, mfu 34.34%\n",
            "iter 2800: loss 4.1165, time 159.71ms, mfu 34.48%\n",
            "iter 2810: loss 4.2237, time 159.85ms, mfu 34.60%\n",
            "iter 2820: loss 4.1093, time 160.14ms, mfu 34.70%\n",
            "iter 2830: loss 4.1422, time 159.78ms, mfu 34.80%\n",
            "iter 2840: loss 4.1414, time 159.83ms, mfu 34.89%\n",
            "iter 2850: loss 4.2237, time 159.70ms, mfu 34.98%\n",
            "iter 2860: loss 4.1363, time 159.47ms, mfu 35.06%\n",
            "iter 2870: loss 4.0745, time 159.42ms, mfu 35.13%\n",
            "iter 2880: loss 4.2832, time 159.73ms, mfu 35.19%\n",
            "iter 2890: loss 4.0758, time 160.61ms, mfu 35.22%\n",
            "iter 2900: loss 4.2519, time 160.23ms, mfu 35.26%\n",
            "iter 2910: loss 4.1905, time 160.31ms, mfu 35.30%\n",
            "iter 2920: loss 4.1511, time 159.64ms, mfu 35.34%\n",
            "iter 2930: loss 4.1363, time 159.00ms, mfu 35.39%\n",
            "iter 2940: loss 4.1549, time 158.98ms, mfu 35.44%\n",
            "iter 2950: loss 4.1977, time 159.23ms, mfu 35.48%\n",
            "iter 2960: loss 4.1918, time 159.45ms, mfu 35.51%\n",
            "iter 2970: loss 4.1293, time 160.11ms, mfu 35.53%\n",
            "iter 2980: loss 4.0745, time 160.32ms, mfu 35.53%\n",
            "iter 2990: loss 4.2115, time 160.36ms, mfu 35.54%\n",
            "step 3000: train loss 4.1196, val loss 4.1176\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3000: loss 4.2012, time 24385.05ms, mfu 32.01%\n",
            "iter 3010: loss 4.2226, time 159.52ms, mfu 32.38%\n",
            "iter 3020: loss 4.1124, time 159.54ms, mfu 32.72%\n",
            "iter 3030: loss 3.9776, time 159.79ms, mfu 33.02%\n",
            "iter 3040: loss 4.2078, time 160.13ms, mfu 33.28%\n",
            "iter 3050: loss 4.0873, time 160.38ms, mfu 33.51%\n",
            "iter 3060: loss 3.9918, time 160.04ms, mfu 33.73%\n",
            "iter 3070: loss 4.1066, time 159.01ms, mfu 33.94%\n",
            "iter 3080: loss 4.1030, time 159.12ms, mfu 34.13%\n",
            "iter 3090: loss 4.1271, time 159.48ms, mfu 34.30%\n",
            "iter 3100: loss 4.0851, time 159.27ms, mfu 34.45%\n",
            "iter 3110: loss 4.0203, time 159.45ms, mfu 34.58%\n",
            "iter 3120: loss 4.1835, time 159.26ms, mfu 34.71%\n",
            "iter 3130: loss 3.9814, time 159.61ms, mfu 34.81%\n",
            "iter 3140: loss 4.1184, time 159.54ms, mfu 34.91%\n",
            "iter 3150: loss 4.0795, time 159.56ms, mfu 34.99%\n",
            "iter 3160: loss 4.1413, time 160.02ms, mfu 35.06%\n",
            "iter 3170: loss 4.1642, time 160.71ms, mfu 35.10%\n",
            "iter 3180: loss 4.1760, time 160.57ms, mfu 35.15%\n",
            "iter 3190: loss 4.0747, time 160.00ms, mfu 35.20%\n",
            "iter 3200: loss 3.9918, time 159.92ms, mfu 35.25%\n",
            "iter 3210: loss 4.1052, time 159.91ms, mfu 35.29%\n",
            "iter 3220: loss 4.0239, time 159.44ms, mfu 35.34%\n",
            "iter 3230: loss 4.1206, time 159.06ms, mfu 35.39%\n",
            "iter 3240: loss 4.0267, time 158.96ms, mfu 35.44%\n",
            "iter 3250: loss 4.0879, time 159.21ms, mfu 35.48%\n",
            "iter 3260: loss 4.1077, time 159.72ms, mfu 35.51%\n",
            "iter 3270: loss 4.0257, time 160.50ms, mfu 35.51%\n",
            "iter 3280: loss 4.1386, time 160.44ms, mfu 35.52%\n",
            "iter 3290: loss 3.9895, time 158.92ms, mfu 35.56%\n",
            "step 3300: train loss 4.0715, val loss 4.0568\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3300: loss 4.1646, time 24527.07ms, mfu 32.02%\n",
            "iter 3310: loss 4.1237, time 158.78ms, mfu 32.41%\n",
            "iter 3320: loss 4.0189, time 158.90ms, mfu 32.76%\n",
            "iter 3330: loss 4.1248, time 159.14ms, mfu 33.07%\n",
            "iter 3340: loss 4.1734, time 159.67ms, mfu 33.34%\n",
            "iter 3350: loss 4.1069, time 160.24ms, mfu 33.57%\n",
            "iter 3360: loss 4.0436, time 160.38ms, mfu 33.77%\n",
            "iter 3370: loss 3.9608, time 160.27ms, mfu 33.95%\n",
            "iter 3380: loss 4.1163, time 159.98ms, mfu 34.12%\n",
            "iter 3390: loss 4.0708, time 160.27ms, mfu 34.27%\n",
            "iter 3400: loss 3.9636, time 159.53ms, mfu 34.42%\n",
            "iter 3410: loss 3.9044, time 159.60ms, mfu 34.55%\n",
            "iter 3420: loss 4.1278, time 159.64ms, mfu 34.67%\n",
            "iter 3430: loss 4.0172, time 160.13ms, mfu 34.77%\n",
            "iter 3440: loss 3.9730, time 160.85ms, mfu 34.84%\n",
            "iter 3450: loss 4.0498, time 161.04ms, mfu 34.90%\n",
            "iter 3460: loss 4.1678, time 160.58ms, mfu 34.96%\n",
            "iter 3470: loss 4.0967, time 160.27ms, mfu 35.03%\n",
            "iter 3480: loss 3.9040, time 159.51ms, mfu 35.10%\n",
            "iter 3490: loss 4.0710, time 158.87ms, mfu 35.18%\n",
            "iter 3500: loss 4.0376, time 159.04ms, mfu 35.25%\n",
            "iter 3510: loss 4.1043, time 159.57ms, mfu 35.30%\n",
            "iter 3520: loss 4.0251, time 159.84ms, mfu 35.34%\n",
            "iter 3530: loss 4.0624, time 160.17ms, mfu 35.37%\n",
            "iter 3540: loss 4.0474, time 160.17ms, mfu 35.40%\n",
            "iter 3550: loss 4.0966, time 159.17ms, mfu 35.44%\n",
            "iter 3560: loss 4.0784, time 159.48ms, mfu 35.47%\n",
            "iter 3570: loss 4.1040, time 160.28ms, mfu 35.49%\n",
            "iter 3580: loss 4.0461, time 160.81ms, mfu 35.49%\n",
            "iter 3590: loss 3.9631, time 160.35ms, mfu 35.50%\n",
            "step 3600: train loss 4.0167, val loss 4.0132\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3600: loss 3.8842, time 24686.98ms, mfu 31.97%\n",
            "iter 3610: loss 4.1114, time 159.15ms, mfu 32.36%\n",
            "iter 3620: loss 4.1163, time 159.78ms, mfu 32.69%\n",
            "iter 3630: loss 3.8803, time 160.07ms, mfu 32.99%\n",
            "iter 3640: loss 4.0484, time 160.22ms, mfu 33.25%\n",
            "iter 3650: loss 4.0251, time 160.01ms, mfu 33.49%\n",
            "iter 3660: loss 3.9536, time 159.95ms, mfu 33.71%\n",
            "iter 3670: loss 4.0572, time 159.58ms, mfu 33.91%\n",
            "iter 3680: loss 3.9221, time 160.05ms, mfu 34.09%\n",
            "iter 3690: loss 3.8849, time 160.09ms, mfu 34.24%\n",
            "iter 3700: loss 4.1286, time 160.81ms, mfu 34.37%\n",
            "iter 3710: loss 4.0519, time 160.54ms, mfu 34.48%\n",
            "iter 3720: loss 3.8510, time 160.50ms, mfu 34.59%\n",
            "iter 3730: loss 3.9594, time 159.37ms, mfu 34.71%\n",
            "iter 3740: loss 4.0016, time 159.08ms, mfu 34.83%\n",
            "iter 3750: loss 4.0915, time 159.18ms, mfu 34.93%\n",
            "iter 3760: loss 4.0047, time 159.01ms, mfu 35.03%\n",
            "iter 3770: loss 4.0189, time 159.44ms, mfu 35.10%\n",
            "iter 3780: loss 3.9460, time 160.27ms, mfu 35.15%\n",
            "iter 3790: loss 4.0219, time 160.26ms, mfu 35.20%\n",
            "iter 3800: loss 4.0017, time 160.62ms, mfu 35.23%\n",
            "iter 3810: loss 4.1075, time 159.87ms, mfu 35.28%\n",
            "iter 3820: loss 3.9369, time 159.51ms, mfu 35.33%\n",
            "iter 3830: loss 3.9810, time 159.40ms, mfu 35.37%\n",
            "iter 3840: loss 4.0502, time 160.18ms, mfu 35.40%\n",
            "iter 3850: loss 3.9680, time 160.56ms, mfu 35.41%\n",
            "iter 3860: loss 4.1248, time 159.92ms, mfu 35.44%\n",
            "iter 3870: loss 3.9085, time 159.37ms, mfu 35.47%\n",
            "iter 3880: loss 4.1068, time 159.31ms, mfu 35.51%\n",
            "iter 3890: loss 4.0046, time 161.37ms, mfu 35.49%\n",
            "step 3900: train loss 3.9874, val loss 3.9805\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3900: loss 3.8791, time 24286.07ms, mfu 31.97%\n",
            "iter 3910: loss 3.9342, time 159.57ms, mfu 32.35%\n",
            "iter 3920: loss 3.9260, time 159.19ms, mfu 32.70%\n",
            "iter 3930: loss 3.8285, time 159.33ms, mfu 33.01%\n",
            "iter 3940: loss 3.9107, time 158.90ms, mfu 33.30%\n",
            "iter 3950: loss 3.9430, time 158.98ms, mfu 33.56%\n",
            "iter 3960: loss 3.9911, time 158.94ms, mfu 33.79%\n",
            "iter 3970: loss 3.9404, time 159.12ms, mfu 34.00%\n",
            "iter 3980: loss 3.9421, time 159.41ms, mfu 34.18%\n",
            "iter 3990: loss 3.9819, time 159.94ms, mfu 34.33%\n",
            "iter 4000: loss 3.8600, time 160.17ms, mfu 34.46%\n",
            "iter 4010: loss 3.9515, time 161.21ms, mfu 34.55%\n",
            "iter 4020: loss 4.0068, time 160.52ms, mfu 34.65%\n",
            "iter 4030: loss 3.9411, time 160.73ms, mfu 34.74%\n",
            "iter 4040: loss 4.1237, time 160.57ms, mfu 34.82%\n",
            "iter 4050: loss 3.9957, time 160.48ms, mfu 34.89%\n",
            "iter 4060: loss 3.8761, time 160.04ms, mfu 34.97%\n",
            "iter 4070: loss 4.0161, time 159.58ms, mfu 35.04%\n",
            "iter 4080: loss 4.0651, time 159.20ms, mfu 35.12%\n",
            "iter 4090: loss 3.9518, time 159.85ms, mfu 35.18%\n",
            "iter 4100: loss 3.9513, time 160.70ms, mfu 35.21%\n",
            "iter 4110: loss 3.9611, time 160.07ms, mfu 35.26%\n",
            "iter 4120: loss 4.0270, time 159.05ms, mfu 35.32%\n",
            "iter 4130: loss 4.0139, time 159.18ms, mfu 35.37%\n",
            "iter 4140: loss 3.8781, time 160.05ms, mfu 35.40%\n",
            "iter 4150: loss 3.9291, time 160.49ms, mfu 35.42%\n",
            "iter 4160: loss 4.0156, time 160.41ms, mfu 35.43%\n",
            "iter 4170: loss 4.0385, time 159.41ms, mfu 35.47%\n",
            "iter 4180: loss 3.9566, time 159.10ms, mfu 35.51%\n",
            "iter 4190: loss 3.9435, time 159.91ms, mfu 35.52%\n",
            "step 4200: train loss 3.9584, val loss 3.9560\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4200: loss 4.0239, time 24708.31ms, mfu 31.99%\n",
            "iter 4210: loss 4.0483, time 159.03ms, mfu 32.38%\n",
            "iter 4220: loss 3.9340, time 159.22ms, mfu 32.73%\n",
            "iter 4230: loss 4.0826, time 158.81ms, mfu 33.05%\n",
            "iter 4240: loss 3.9734, time 159.31ms, mfu 33.33%\n",
            "iter 4250: loss 4.0020, time 158.81ms, mfu 33.59%\n",
            "iter 4260: loss 3.9160, time 158.77ms, mfu 33.82%\n",
            "iter 4270: loss 4.0299, time 159.07ms, mfu 34.03%\n",
            "iter 4280: loss 3.8947, time 159.52ms, mfu 34.20%\n",
            "iter 4290: loss 4.0398, time 159.11ms, mfu 34.37%\n",
            "iter 4300: loss 3.9572, time 159.80ms, mfu 34.50%\n",
            "iter 4310: loss 3.7592, time 159.63ms, mfu 34.62%\n",
            "iter 4320: loss 3.9334, time 159.56ms, mfu 34.74%\n",
            "iter 4330: loss 3.8010, time 159.98ms, mfu 34.83%\n",
            "iter 4340: loss 3.9189, time 159.15ms, mfu 34.93%\n",
            "iter 4350: loss 3.9483, time 159.30ms, mfu 35.02%\n",
            "iter 4360: loss 3.9632, time 160.78ms, mfu 35.07%\n",
            "iter 4370: loss 4.0243, time 160.76ms, mfu 35.11%\n",
            "iter 4380: loss 3.8946, time 159.84ms, mfu 35.17%\n",
            "iter 4390: loss 3.8826, time 158.85ms, mfu 35.24%\n",
            "iter 4400: loss 3.9033, time 159.72ms, mfu 35.29%\n",
            "iter 4410: loss 3.9511, time 160.23ms, mfu 35.32%\n",
            "iter 4420: loss 3.9174, time 160.08ms, mfu 35.36%\n",
            "iter 4430: loss 3.7597, time 159.54ms, mfu 35.40%\n",
            "iter 4440: loss 3.9701, time 159.47ms, mfu 35.44%\n",
            "iter 4450: loss 3.9128, time 159.70ms, mfu 35.46%\n",
            "iter 4460: loss 3.8348, time 160.37ms, mfu 35.48%\n",
            "iter 4470: loss 3.9857, time 160.47ms, mfu 35.48%\n",
            "iter 4480: loss 3.9165, time 159.91ms, mfu 35.50%\n",
            "iter 4490: loss 3.9813, time 159.04ms, mfu 35.54%\n",
            "step 4500: train loss 3.9296, val loss 3.9384\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4500: loss 3.9525, time 24510.51ms, mfu 32.01%\n",
            "iter 4510: loss 3.8958, time 159.26ms, mfu 32.39%\n",
            "iter 4520: loss 3.9217, time 159.69ms, mfu 32.73%\n",
            "iter 4530: loss 3.9311, time 159.76ms, mfu 33.03%\n",
            "iter 4540: loss 3.8406, time 160.28ms, mfu 33.28%\n",
            "iter 4550: loss 3.8656, time 160.31ms, mfu 33.51%\n",
            "iter 4560: loss 3.8110, time 160.14ms, mfu 33.73%\n",
            "iter 4570: loss 3.9442, time 159.82ms, mfu 33.92%\n",
            "iter 4580: loss 4.0467, time 159.94ms, mfu 34.10%\n",
            "iter 4590: loss 3.9147, time 159.27ms, mfu 34.27%\n",
            "iter 4600: loss 3.8906, time 158.92ms, mfu 34.43%\n",
            "iter 4610: loss 3.8781, time 158.82ms, mfu 34.58%\n",
            "iter 4620: loss 3.8605, time 159.94ms, mfu 34.69%\n",
            "iter 4630: loss 4.0062, time 160.20ms, mfu 34.79%\n",
            "iter 4640: loss 3.8954, time 160.47ms, mfu 34.86%\n",
            "iter 4650: loss 3.9597, time 159.41ms, mfu 34.96%\n",
            "iter 4660: loss 3.9096, time 159.04ms, mfu 35.05%\n",
            "iter 4670: loss 3.9185, time 159.45ms, mfu 35.12%\n",
            "iter 4680: loss 3.8354, time 159.63ms, mfu 35.18%\n",
            "iter 4690: loss 3.9594, time 159.25ms, mfu 35.25%\n",
            "iter 4700: loss 4.0554, time 159.95ms, mfu 35.29%\n",
            "iter 4710: loss 4.0366, time 158.89ms, mfu 35.35%\n",
            "iter 4720: loss 3.9872, time 158.92ms, mfu 35.41%\n",
            "iter 4730: loss 3.9264, time 159.06ms, mfu 35.45%\n",
            "iter 4740: loss 4.0527, time 160.01ms, mfu 35.48%\n",
            "iter 4750: loss 3.9679, time 160.21ms, mfu 35.49%\n",
            "iter 4760: loss 3.7833, time 160.08ms, mfu 35.50%\n",
            "iter 4770: loss 3.8896, time 159.70ms, mfu 35.53%\n",
            "iter 4780: loss 3.8738, time 159.33ms, mfu 35.56%\n",
            "iter 4790: loss 4.0484, time 159.42ms, mfu 35.58%\n",
            "step 4800: train loss 3.9311, val loss 3.9230\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4800: loss 3.9739, time 24690.95ms, mfu 32.04%\n",
            "iter 4810: loss 3.9787, time 159.04ms, mfu 32.43%\n",
            "iter 4820: loss 3.9255, time 159.28ms, mfu 32.77%\n",
            "iter 4830: loss 3.9357, time 159.12ms, mfu 33.08%\n",
            "iter 4840: loss 3.9176, time 159.97ms, mfu 33.34%\n",
            "iter 4850: loss 3.9307, time 160.34ms, mfu 33.56%\n",
            "iter 4860: loss 3.8082, time 160.09ms, mfu 33.77%\n",
            "iter 4870: loss 3.8487, time 160.14ms, mfu 33.96%\n",
            "iter 4880: loss 3.8728, time 159.78ms, mfu 34.13%\n",
            "iter 4890: loss 3.6999, time 159.10ms, mfu 34.30%\n",
            "iter 4900: loss 3.9299, time 160.63ms, mfu 34.43%\n",
            "iter 4910: loss 3.9479, time 160.86ms, mfu 34.53%\n",
            "iter 4920: loss 3.8750, time 159.82ms, mfu 34.65%\n",
            "iter 4930: loss 3.8432, time 159.26ms, mfu 34.77%\n",
            "iter 4940: loss 3.8818, time 159.77ms, mfu 34.86%\n",
            "iter 4950: loss 4.0244, time 160.26ms, mfu 34.93%\n",
            "iter 4960: loss 3.8790, time 160.56ms, mfu 35.00%\n",
            "iter 4970: loss 3.9291, time 159.13ms, mfu 35.08%\n",
            "iter 4980: loss 3.9724, time 159.05ms, mfu 35.16%\n",
            "iter 4990: loss 3.8798, time 159.06ms, mfu 35.23%\n",
            "iter 5000: loss 3.9229, time 159.51ms, mfu 35.29%\n",
            "iter 5010: loss 3.8713, time 159.91ms, mfu 35.33%\n",
            "iter 5020: loss 3.9643, time 160.11ms, mfu 35.36%\n",
            "iter 5030: loss 3.9335, time 160.29ms, mfu 35.38%\n",
            "iter 5040: loss 3.9876, time 158.98ms, mfu 35.43%\n",
            "iter 5050: loss 3.8073, time 159.18ms, mfu 35.47%\n",
            "iter 5060: loss 3.9631, time 159.28ms, mfu 35.51%\n",
            "iter 5070: loss 3.9320, time 159.63ms, mfu 35.53%\n",
            "iter 5080: loss 3.9202, time 160.77ms, mfu 35.53%\n",
            "iter 5090: loss 3.7677, time 160.24ms, mfu 35.54%\n",
            "step 5100: train loss 3.9194, val loss 3.9239\n",
            "iter 5100: loss 3.8092, time 22445.86ms, mfu 32.01%\n",
            "iter 5110: loss 4.0158, time 160.36ms, mfu 32.37%\n",
            "iter 5120: loss 3.8499, time 160.34ms, mfu 32.69%\n",
            "iter 5130: loss 3.9939, time 160.50ms, mfu 32.97%\n",
            "iter 5140: loss 3.7940, time 160.11ms, mfu 33.24%\n",
            "iter 5150: loss 3.8070, time 159.65ms, mfu 33.49%\n",
            "iter 5160: loss 4.0468, time 159.02ms, mfu 33.73%\n",
            "iter 5170: loss 4.0410, time 159.23ms, mfu 33.94%\n",
            "iter 5180: loss 3.8272, time 159.81ms, mfu 34.12%\n",
            "iter 5190: loss 4.0600, time 160.56ms, mfu 34.26%\n",
            "iter 5200: loss 3.9162, time 160.05ms, mfu 34.40%\n",
            "iter 5210: loss 3.8018, time 159.16ms, mfu 34.54%\n",
            "iter 5220: loss 3.8402, time 159.35ms, mfu 34.67%\n",
            "iter 5230: loss 3.8129, time 159.94ms, mfu 34.77%\n",
            "iter 5240: loss 4.0292, time 160.52ms, mfu 34.85%\n",
            "iter 5250: loss 3.8824, time 160.36ms, mfu 34.92%\n",
            "iter 5260: loss 3.9708, time 159.37ms, mfu 35.01%\n",
            "iter 5270: loss 3.9233, time 158.76ms, mfu 35.10%\n",
            "iter 5280: loss 4.0101, time 159.56ms, mfu 35.17%\n",
            "iter 5290: loss 3.9144, time 160.23ms, mfu 35.21%\n",
            "iter 5300: loss 3.8607, time 160.29ms, mfu 35.25%\n",
            "iter 5310: loss 3.8766, time 159.12ms, mfu 35.31%\n",
            "iter 5320: loss 3.9422, time 159.48ms, mfu 35.36%\n",
            "iter 5330: loss 3.8777, time 159.91ms, mfu 35.39%\n",
            "iter 5340: loss 3.8855, time 160.29ms, mfu 35.41%\n",
            "iter 5350: loss 3.8359, time 160.09ms, mfu 35.43%\n",
            "iter 5360: loss 3.8982, time 159.05ms, mfu 35.48%\n",
            "iter 5370: loss 3.7931, time 159.51ms, mfu 35.51%\n",
            "iter 5380: loss 3.9386, time 160.06ms, mfu 35.52%\n",
            "iter 5390: loss 3.8581, time 160.92ms, mfu 35.52%\n",
            "step 5400: train loss 3.9229, val loss 3.9190\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5400: loss 3.9424, time 24695.83ms, mfu 31.99%\n",
            "iter 5410: loss 3.9401, time 158.25ms, mfu 32.39%\n",
            "iter 5420: loss 3.9992, time 159.49ms, mfu 32.73%\n",
            "iter 5430: loss 4.0043, time 159.09ms, mfu 33.05%\n",
            "iter 5440: loss 3.9625, time 159.54ms, mfu 33.32%\n",
            "iter 5450: loss 3.8837, time 160.19ms, mfu 33.55%\n",
            "iter 5460: loss 3.9909, time 160.32ms, mfu 33.75%\n",
            "iter 5470: loss 3.9187, time 159.82ms, mfu 33.95%\n",
            "iter 5480: loss 4.0512, time 158.88ms, mfu 34.14%\n",
            "iter 5490: loss 3.9572, time 159.58ms, mfu 34.30%\n",
            "iter 5500: loss 3.9269, time 160.31ms, mfu 34.43%\n",
            "iter 5510: loss 3.9005, time 160.40ms, mfu 34.55%\n",
            "iter 5520: loss 3.8317, time 159.97ms, mfu 34.66%\n",
            "iter 5530: loss 3.8649, time 159.13ms, mfu 34.78%\n",
            "iter 5540: loss 3.7661, time 159.38ms, mfu 34.88%\n",
            "iter 5550: loss 3.7963, time 159.70ms, mfu 34.97%\n",
            "iter 5560: loss 3.9709, time 160.25ms, mfu 35.03%\n",
            "iter 5570: loss 3.8540, time 160.45ms, mfu 35.08%\n",
            "iter 5580: loss 3.8937, time 159.88ms, mfu 35.14%\n",
            "iter 5590: loss 3.9445, time 158.90ms, mfu 35.22%\n",
            "iter 5600: loss 3.9327, time 159.41ms, mfu 35.28%\n",
            "iter 5610: loss 3.7907, time 159.80ms, mfu 35.32%\n",
            "iter 5620: loss 4.0203, time 160.42ms, mfu 35.35%\n",
            "iter 5630: loss 3.8378, time 160.13ms, mfu 35.37%\n",
            "iter 5640: loss 3.9715, time 159.49ms, mfu 35.41%\n",
            "iter 5650: loss 3.9681, time 158.79ms, mfu 35.47%\n",
            "iter 5660: loss 3.9312, time 159.58ms, mfu 35.50%\n",
            "iter 5670: loss 3.9935, time 160.11ms, mfu 35.51%\n",
            "iter 5680: loss 3.9124, time 160.54ms, mfu 35.51%\n",
            "iter 5690: loss 3.7669, time 160.27ms, mfu 35.52%\n",
            "step 5700: train loss 3.9170, val loss 3.9056\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5700: loss 4.1087, time 25106.95ms, mfu 31.99%\n",
            "iter 5710: loss 3.9288, time 158.98ms, mfu 32.38%\n",
            "iter 5720: loss 3.8362, time 158.87ms, mfu 32.74%\n",
            "iter 5730: loss 3.8528, time 159.18ms, mfu 33.05%\n",
            "iter 5740: loss 3.9282, time 159.74ms, mfu 33.31%\n",
            "iter 5750: loss 4.0434, time 160.52ms, mfu 33.54%\n",
            "iter 5760: loss 3.8848, time 159.81ms, mfu 33.75%\n",
            "iter 5770: loss 3.7854, time 159.00ms, mfu 33.97%\n",
            "iter 5780: loss 3.7772, time 159.38ms, mfu 34.15%\n",
            "iter 5790: loss 3.8471, time 159.52ms, mfu 34.31%\n",
            "iter 5800: loss 3.8191, time 160.38ms, mfu 34.44%\n",
            "iter 5810: loss 3.9046, time 160.71ms, mfu 34.55%\n",
            "iter 5820: loss 3.9287, time 160.02ms, mfu 34.66%\n",
            "iter 5830: loss 3.7858, time 159.12ms, mfu 34.78%\n",
            "iter 5840: loss 3.8994, time 159.30ms, mfu 34.88%\n",
            "iter 5850: loss 4.0010, time 160.06ms, mfu 34.96%\n",
            "iter 5860: loss 3.9704, time 160.49ms, mfu 35.02%\n",
            "iter 5870: loss 3.9001, time 160.67ms, mfu 35.07%\n",
            "iter 5880: loss 3.9337, time 160.08ms, mfu 35.12%\n",
            "iter 5890: loss 3.9137, time 159.22ms, mfu 35.20%\n",
            "iter 5900: loss 3.9888, time 159.32ms, mfu 35.26%\n",
            "iter 5910: loss 3.9107, time 159.67ms, mfu 35.31%\n",
            "iter 5920: loss 3.9281, time 159.81ms, mfu 35.35%\n",
            "iter 5930: loss 3.7935, time 160.27ms, mfu 35.37%\n",
            "iter 5940: loss 3.7802, time 160.13ms, mfu 35.40%\n",
            "iter 5950: loss 3.9929, time 159.83ms, mfu 35.43%\n",
            "iter 5960: loss 3.9938, time 160.48ms, mfu 35.44%\n",
            "iter 5970: loss 3.9572, time 159.81ms, mfu 35.47%\n",
            "iter 5980: loss 3.9787, time 160.17ms, mfu 35.48%\n",
            "iter 5990: loss 3.9716, time 160.52ms, mfu 35.49%\n",
            "step 6000: train loss 3.9134, val loss 3.9096\n",
            "iter 6000: loss 3.7766, time 22596.45ms, mfu 31.97%\n",
            "iter 6010: loss 4.0079, time 160.41ms, mfu 32.33%\n",
            "iter 6020: loss 3.9009, time 159.69ms, mfu 32.67%\n",
            "iter 6030: loss 3.8590, time 159.17ms, mfu 32.98%\n",
            "iter 6040: loss 3.9545, time 159.64ms, mfu 33.26%\n",
            "iter 6050: loss 3.8498, time 160.50ms, mfu 33.49%\n",
            "iter 6060: loss 4.0238, time 160.34ms, mfu 33.70%\n",
            "iter 6070: loss 3.9724, time 159.58ms, mfu 33.90%\n",
            "iter 6080: loss 3.8996, time 158.87ms, mfu 34.11%\n",
            "iter 6090: loss 3.8584, time 159.22ms, mfu 34.28%\n",
            "iter 6100: loss 3.8341, time 159.58ms, mfu 34.43%\n",
            "iter 6110: loss 3.8486, time 160.36ms, mfu 34.54%\n",
            "iter 6120: loss 3.8572, time 160.40ms, mfu 34.64%\n",
            "iter 6130: loss 3.9000, time 159.79ms, mfu 34.75%\n",
            "iter 6140: loss 3.9419, time 158.85ms, mfu 34.87%\n",
            "iter 6150: loss 3.8104, time 159.90ms, mfu 34.95%\n",
            "iter 6160: loss 3.8615, time 160.55ms, mfu 35.01%\n",
            "iter 6170: loss 3.8538, time 160.50ms, mfu 35.06%\n",
            "iter 6180: loss 3.7685, time 160.26ms, mfu 35.12%\n",
            "iter 6190: loss 3.8083, time 159.00ms, mfu 35.19%\n",
            "iter 6200: loss 3.8835, time 159.52ms, mfu 35.25%\n",
            "iter 6210: loss 3.9872, time 159.14ms, mfu 35.31%\n",
            "iter 6220: loss 3.9956, time 159.07ms, mfu 35.37%\n",
            "iter 6230: loss 4.0744, time 159.19ms, mfu 35.42%\n",
            "iter 6240: loss 3.8975, time 159.07ms, mfu 35.46%\n",
            "iter 6250: loss 3.8256, time 159.00ms, mfu 35.50%\n",
            "iter 6260: loss 3.8743, time 159.03ms, mfu 35.54%\n",
            "iter 6270: loss 3.9991, time 159.31ms, mfu 35.57%\n",
            "iter 6280: loss 3.9585, time 158.91ms, mfu 35.60%\n",
            "iter 6290: loss 3.9945, time 158.87ms, mfu 35.63%\n",
            "step 6300: train loss 3.9121, val loss 3.8984\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 6300: loss 3.8227, time 24494.04ms, mfu 32.09%\n",
            "iter 6310: loss 3.9204, time 160.30ms, mfu 32.44%\n",
            "iter 6320: loss 3.8302, time 160.42ms, mfu 32.76%\n",
            "iter 6330: loss 3.9231, time 159.84ms, mfu 33.05%\n",
            "iter 6340: loss 3.9601, time 159.14ms, mfu 33.33%\n",
            "iter 6350: loss 3.8543, time 159.28ms, mfu 33.58%\n",
            "iter 6360: loss 3.8527, time 159.44ms, mfu 33.80%\n",
            "iter 6370: loss 3.7713, time 159.99ms, mfu 33.99%\n",
            "iter 6380: loss 3.9297, time 160.04ms, mfu 34.15%\n",
            "iter 6390: loss 3.8252, time 160.45ms, mfu 34.29%\n",
            "iter 6400: loss 3.9284, time 159.66ms, mfu 34.44%\n",
            "iter 6410: loss 3.9265, time 158.89ms, mfu 34.59%\n",
            "iter 6420: loss 3.9638, time 159.10ms, mfu 34.71%\n",
            "iter 6430: loss 3.9195, time 159.95ms, mfu 34.81%\n",
            "iter 6440: loss 4.0116, time 160.51ms, mfu 34.88%\n",
            "iter 6450: loss 3.8964, time 160.38ms, mfu 34.95%\n",
            "iter 6460: loss 3.9157, time 160.05ms, mfu 35.02%\n",
            "iter 6470: loss 3.9869, time 160.12ms, mfu 35.08%\n",
            "iter 6480: loss 4.0070, time 159.06ms, mfu 35.16%\n",
            "iter 6490: loss 3.8945, time 159.55ms, mfu 35.22%\n",
            "iter 6500: loss 3.9277, time 159.61ms, mfu 35.28%\n",
            "iter 6510: loss 3.9510, time 159.41ms, mfu 35.33%\n",
            "iter 6520: loss 3.9892, time 159.27ms, mfu 35.38%\n",
            "iter 6530: loss 3.9054, time 159.30ms, mfu 35.42%\n",
            "iter 6540: loss 3.9041, time 158.97ms, mfu 35.47%\n",
            "iter 6550: loss 3.9300, time 159.08ms, mfu 35.51%\n",
            "iter 6560: loss 3.7998, time 159.21ms, mfu 35.54%\n",
            "iter 6570: loss 3.9120, time 159.96ms, mfu 35.55%\n",
            "iter 6580: loss 4.0012, time 160.88ms, mfu 35.55%\n",
            "iter 6590: loss 3.9299, time 160.63ms, mfu 35.54%\n",
            "step 6600: train loss 3.9063, val loss 3.9066\n",
            "iter 6600: loss 4.0412, time 22290.50ms, mfu 32.01%\n",
            "iter 6610: loss 3.9239, time 159.25ms, mfu 32.40%\n",
            "iter 6620: loss 3.8267, time 160.22ms, mfu 32.72%\n",
            "iter 6630: loss 3.9331, time 160.12ms, mfu 33.01%\n",
            "iter 6640: loss 3.8133, time 159.84ms, mfu 33.28%\n",
            "iter 6650: loss 4.0710, time 158.99ms, mfu 33.54%\n",
            "iter 6660: loss 3.8534, time 160.00ms, mfu 33.75%\n",
            "iter 6670: loss 3.9364, time 160.39ms, mfu 33.93%\n",
            "iter 6680: loss 3.9538, time 160.46ms, mfu 34.10%\n",
            "iter 6690: loss 3.9346, time 159.45ms, mfu 34.27%\n",
            "iter 6700: loss 3.8349, time 159.18ms, mfu 34.42%\n",
            "iter 6710: loss 3.9118, time 160.10ms, mfu 34.55%\n",
            "iter 6720: loss 3.8545, time 160.03ms, mfu 34.66%\n",
            "iter 6730: loss 3.8334, time 160.55ms, mfu 34.74%\n",
            "iter 6740: loss 4.0179, time 160.36ms, mfu 34.83%\n",
            "iter 6750: loss 3.9720, time 159.72ms, mfu 34.92%\n",
            "iter 6760: loss 3.8398, time 159.08ms, mfu 35.01%\n",
            "iter 6770: loss 3.9321, time 159.56ms, mfu 35.09%\n",
            "iter 6780: loss 3.9770, time 160.24ms, mfu 35.14%\n",
            "iter 6790: loss 4.0136, time 159.19ms, mfu 35.21%\n",
            "iter 6800: loss 3.7585, time 159.42ms, mfu 35.27%\n",
            "iter 6810: loss 3.9436, time 159.38ms, mfu 35.32%\n",
            "iter 6820: loss 3.9602, time 159.47ms, mfu 35.37%\n",
            "iter 6830: loss 3.8451, time 159.29ms, mfu 35.41%\n",
            "iter 6840: loss 3.8860, time 159.32ms, mfu 35.45%\n",
            "iter 6850: loss 3.9944, time 159.77ms, mfu 35.48%\n",
            "iter 6860: loss 3.8742, time 160.77ms, mfu 35.48%\n",
            "iter 6870: loss 3.9272, time 160.57ms, mfu 35.49%\n",
            "iter 6880: loss 3.8435, time 159.81ms, mfu 35.51%\n",
            "iter 6890: loss 3.9692, time 159.65ms, mfu 35.53%\n",
            "step 6900: train loss 3.9151, val loss 3.9032\n",
            "iter 6900: loss 3.9663, time 22331.08ms, mfu 32.00%\n",
            "iter 6910: loss 3.8438, time 159.71ms, mfu 32.38%\n",
            "iter 6920: loss 3.8325, time 159.52ms, mfu 32.72%\n",
            "iter 6930: loss 3.7234, time 159.36ms, mfu 33.02%\n",
            "iter 6940: loss 3.9087, time 160.20ms, mfu 33.28%\n",
            "iter 6950: loss 3.8149, time 160.24ms, mfu 33.52%\n",
            "iter 6960: loss 3.9191, time 160.33ms, mfu 33.72%\n",
            "iter 6970: loss 3.8546, time 159.86ms, mfu 33.92%\n",
            "iter 6980: loss 3.8480, time 158.96ms, mfu 34.12%\n",
            "iter 6990: loss 3.8836, time 159.51ms, mfu 34.28%\n",
            "iter 7000: loss 3.9366, time 160.37ms, mfu 34.41%\n",
            "iter 7010: loss 3.9208, time 160.01ms, mfu 34.54%\n",
            "iter 7020: loss 3.9178, time 159.75ms, mfu 34.66%\n",
            "iter 7030: loss 4.0303, time 158.76ms, mfu 34.78%\n",
            "iter 7040: loss 3.9273, time 159.36ms, mfu 34.89%\n",
            "iter 7050: loss 3.9921, time 159.58ms, mfu 34.97%\n",
            "iter 7060: loss 3.9267, time 159.94ms, mfu 35.04%\n",
            "iter 7070: loss 3.9194, time 160.16ms, mfu 35.10%\n",
            "iter 7080: loss 3.9140, time 159.11ms, mfu 35.18%\n",
            "iter 7090: loss 3.9499, time 160.05ms, mfu 35.22%\n",
            "iter 7100: loss 4.0401, time 160.04ms, mfu 35.27%\n",
            "iter 7110: loss 4.0060, time 159.74ms, mfu 35.31%\n",
            "iter 7120: loss 3.9443, time 160.59ms, mfu 35.33%\n",
            "iter 7130: loss 3.9032, time 160.74ms, mfu 35.35%\n",
            "iter 7140: loss 3.8875, time 159.74ms, mfu 35.39%\n",
            "iter 7150: loss 3.8816, time 159.17ms, mfu 35.43%\n",
            "iter 7160: loss 3.8137, time 158.89ms, mfu 35.48%\n",
            "iter 7170: loss 3.9039, time 159.65ms, mfu 35.51%\n",
            "iter 7180: loss 3.9048, time 158.92ms, mfu 35.55%\n",
            "iter 7190: loss 3.8987, time 159.95ms, mfu 35.56%\n",
            "step 7200: train loss 3.9082, val loss 3.9099\n",
            "iter 7200: loss 3.8461, time 22308.25ms, mfu 32.03%\n",
            "iter 7210: loss 3.9407, time 159.71ms, mfu 32.40%\n",
            "iter 7220: loss 3.7945, time 160.05ms, mfu 32.72%\n",
            "iter 7230: loss 3.8812, time 159.84ms, mfu 33.02%\n",
            "iter 7240: loss 3.8896, time 159.30ms, mfu 33.30%\n",
            "iter 7250: loss 3.9456, time 160.00ms, mfu 33.54%\n",
            "iter 7260: loss 3.9894, time 160.70ms, mfu 33.73%\n",
            "iter 7270: loss 4.0049, time 160.23ms, mfu 33.92%\n",
            "iter 7280: loss 3.9623, time 158.90ms, mfu 34.12%\n",
            "iter 7290: loss 3.8355, time 159.10ms, mfu 34.30%\n",
            "iter 7300: loss 3.9746, time 159.29ms, mfu 34.45%\n",
            "iter 7310: loss 3.8331, time 160.33ms, mfu 34.56%\n",
            "iter 7320: loss 3.8322, time 160.61ms, mfu 34.66%\n",
            "iter 7330: loss 4.0170, time 160.34ms, mfu 34.75%\n",
            "iter 7340: loss 3.8474, time 159.78ms, mfu 34.85%\n",
            "iter 7350: loss 4.0354, time 158.90ms, mfu 34.95%\n",
            "iter 7360: loss 3.9515, time 159.52ms, mfu 35.03%\n",
            "iter 7370: loss 3.8797, time 159.69ms, mfu 35.10%\n",
            "iter 7380: loss 4.0059, time 159.46ms, mfu 35.17%\n",
            "iter 7390: loss 3.9344, time 159.35ms, mfu 35.24%\n",
            "iter 7400: loss 3.9489, time 159.15ms, mfu 35.30%\n",
            "iter 7410: loss 3.6773, time 158.57ms, mfu 35.37%\n",
            "iter 7420: loss 3.9842, time 159.21ms, mfu 35.41%\n",
            "iter 7430: loss 3.8750, time 160.03ms, mfu 35.44%\n",
            "iter 7440: loss 3.9095, time 160.62ms, mfu 35.45%\n",
            "iter 7450: loss 3.8805, time 160.34ms, mfu 35.46%\n",
            "iter 7460: loss 3.8829, time 159.99ms, mfu 35.48%\n",
            "iter 7470: loss 3.9821, time 159.21ms, mfu 35.52%\n",
            "iter 7480: loss 4.0090, time 159.03ms, mfu 35.55%\n",
            "iter 7490: loss 3.8927, time 159.55ms, mfu 35.57%\n",
            "step 7500: train loss 3.9132, val loss 3.8959\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 7500: loss 3.9053, time 24330.28ms, mfu 32.04%\n",
            "iter 7510: loss 3.9957, time 160.27ms, mfu 32.40%\n",
            "iter 7520: loss 3.8357, time 159.84ms, mfu 32.73%\n",
            "iter 7530: loss 3.9051, time 160.39ms, mfu 33.01%\n",
            "iter 7540: loss 3.7924, time 159.05ms, mfu 33.30%\n",
            "iter 7550: loss 3.8682, time 159.14ms, mfu 33.55%\n",
            "iter 7560: loss 3.9195, time 159.56ms, mfu 33.77%\n",
            "iter 7570: loss 3.9063, time 159.89ms, mfu 33.97%\n",
            "iter 7580: loss 3.8889, time 160.34ms, mfu 34.13%\n",
            "iter 7590: loss 3.9689, time 160.20ms, mfu 34.28%\n",
            "iter 7600: loss 3.8549, time 159.66ms, mfu 34.42%\n",
            "iter 7610: loss 3.9512, time 159.27ms, mfu 34.56%\n",
            "iter 7620: loss 3.9632, time 159.35ms, mfu 34.69%\n",
            "iter 7630: loss 3.9311, time 159.10ms, mfu 34.80%\n",
            "iter 7640: loss 3.9394, time 159.24ms, mfu 34.91%\n",
            "iter 7650: loss 4.0299, time 159.70ms, mfu 34.99%\n",
            "iter 7660: loss 3.9044, time 159.56ms, mfu 35.07%\n",
            "iter 7670: loss 3.9061, time 159.13ms, mfu 35.15%\n",
            "iter 7680: loss 3.9879, time 159.49ms, mfu 35.21%\n",
            "iter 7690: loss 3.8889, time 159.61ms, mfu 35.26%\n",
            "iter 7700: loss 3.9994, time 160.17ms, mfu 35.30%\n",
            "iter 7710: loss 3.9697, time 159.81ms, mfu 35.34%\n",
            "iter 7720: loss 3.9033, time 160.32ms, mfu 35.36%\n",
            "iter 7730: loss 3.7946, time 160.04ms, mfu 35.39%\n",
            "iter 7740: loss 3.8430, time 159.59ms, mfu 35.43%\n",
            "iter 7750: loss 3.9155, time 158.92ms, mfu 35.48%\n",
            "iter 7760: loss 3.8606, time 159.26ms, mfu 35.51%\n",
            "iter 7770: loss 3.7631, time 159.49ms, mfu 35.54%\n",
            "iter 7780: loss 3.9669, time 159.93ms, mfu 35.55%\n",
            "iter 7790: loss 3.9607, time 160.31ms, mfu 35.56%\n",
            "step 7800: train loss 3.9091, val loss 3.8950\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 7800: loss 3.8774, time 24732.40ms, mfu 32.02%\n",
            "iter 7810: loss 3.8664, time 159.41ms, mfu 32.40%\n",
            "iter 7820: loss 3.8523, time 160.04ms, mfu 32.73%\n",
            "iter 7830: loss 3.9041, time 159.61ms, mfu 33.03%\n",
            "iter 7840: loss 3.8820, time 159.04ms, mfu 33.31%\n",
            "iter 7850: loss 3.8895, time 158.96ms, mfu 33.57%\n",
            "iter 7860: loss 3.9424, time 159.50ms, mfu 33.79%\n",
            "iter 7870: loss 3.9021, time 159.83ms, mfu 33.98%\n",
            "iter 7880: loss 3.8064, time 160.27ms, mfu 34.14%\n",
            "iter 7890: loss 3.9444, time 160.36ms, mfu 34.29%\n",
            "iter 7900: loss 3.7889, time 159.79ms, mfu 34.43%\n",
            "iter 7910: loss 3.9197, time 160.04ms, mfu 34.55%\n",
            "iter 7920: loss 3.8676, time 159.83ms, mfu 34.67%\n",
            "iter 7930: loss 3.9340, time 159.70ms, mfu 34.77%\n",
            "iter 7940: loss 3.9541, time 160.03ms, mfu 34.86%\n",
            "iter 7950: loss 3.9292, time 160.04ms, mfu 34.94%\n",
            "iter 7960: loss 3.9389, time 160.58ms, mfu 35.00%\n",
            "iter 7970: loss 3.8190, time 160.48ms, mfu 35.06%\n",
            "iter 7980: loss 3.9337, time 160.15ms, mfu 35.11%\n",
            "iter 7990: loss 3.8479, time 159.75ms, mfu 35.17%\n",
            "iter 8000: loss 3.8572, time 159.15ms, mfu 35.24%\n",
            "iter 8010: loss 3.8017, time 159.12ms, mfu 35.30%\n",
            "iter 8020: loss 3.8172, time 158.87ms, mfu 35.36%\n",
            "iter 8030: loss 3.9587, time 159.14ms, mfu 35.41%\n",
            "iter 8040: loss 4.0344, time 159.89ms, mfu 35.44%\n",
            "iter 8050: loss 3.9642, time 159.97ms, mfu 35.46%\n",
            "iter 8060: loss 3.9015, time 160.36ms, mfu 35.48%\n",
            "iter 8070: loss 3.8980, time 159.99ms, mfu 35.49%\n",
            "iter 8080: loss 3.9914, time 158.97ms, mfu 35.53%\n",
            "iter 8090: loss 3.9346, time 159.54ms, mfu 35.56%\n",
            "step 8100: train loss 3.8993, val loss 3.8966\n",
            "iter 8100: loss 3.8956, time 22483.29ms, mfu 32.03%\n",
            "iter 8110: loss 3.8519, time 160.03ms, mfu 32.39%\n",
            "iter 8120: loss 3.9580, time 159.68ms, mfu 32.72%\n",
            "iter 8130: loss 3.9163, time 158.95ms, mfu 33.04%\n",
            "iter 8140: loss 3.9142, time 159.46ms, mfu 33.32%\n",
            "iter 8150: loss 3.8545, time 160.21ms, mfu 33.55%\n",
            "iter 8160: loss 3.8126, time 160.53ms, mfu 33.75%\n",
            "iter 8170: loss 3.9242, time 160.46ms, mfu 33.93%\n",
            "iter 8180: loss 3.8642, time 160.59ms, mfu 34.09%\n",
            "iter 8190: loss 3.9037, time 160.46ms, mfu 34.23%\n",
            "iter 8200: loss 3.9440, time 160.23ms, mfu 34.37%\n",
            "iter 8210: loss 3.8269, time 160.51ms, mfu 34.49%\n",
            "iter 8220: loss 3.8568, time 160.33ms, mfu 34.60%\n",
            "iter 8230: loss 3.9345, time 160.28ms, mfu 34.70%\n",
            "iter 8240: loss 3.9904, time 160.40ms, mfu 34.79%\n",
            "iter 8250: loss 3.9550, time 159.78ms, mfu 34.88%\n",
            "iter 8260: loss 3.8250, time 159.13ms, mfu 34.98%\n",
            "iter 8270: loss 3.9640, time 159.23ms, mfu 35.06%\n",
            "iter 8280: loss 3.8653, time 159.22ms, mfu 35.14%\n",
            "iter 8290: loss 3.8758, time 159.30ms, mfu 35.21%\n",
            "iter 8300: loss 4.0542, time 159.49ms, mfu 35.27%\n",
            "iter 8310: loss 3.9509, time 160.10ms, mfu 35.30%\n",
            "iter 8320: loss 3.9440, time 160.38ms, mfu 35.33%\n",
            "iter 8330: loss 3.8595, time 160.30ms, mfu 35.36%\n",
            "iter 8340: loss 3.8751, time 159.62ms, mfu 35.40%\n",
            "iter 8350: loss 3.9005, time 158.95ms, mfu 35.45%\n",
            "iter 8360: loss 3.9857, time 159.49ms, mfu 35.48%\n",
            "iter 8370: loss 3.9774, time 159.84ms, mfu 35.50%\n",
            "iter 8380: loss 3.8287, time 160.44ms, mfu 35.51%\n",
            "iter 8390: loss 4.0036, time 159.75ms, mfu 35.53%\n",
            "step 8400: train loss 3.9047, val loss 3.8938\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 8400: loss 3.8230, time 24727.94ms, mfu 32.00%\n",
            "iter 8410: loss 4.0957, time 159.24ms, mfu 32.38%\n",
            "iter 8420: loss 4.0286, time 160.14ms, mfu 32.71%\n",
            "iter 8430: loss 3.9130, time 159.56ms, mfu 33.01%\n",
            "iter 8440: loss 3.8672, time 159.94ms, mfu 33.28%\n",
            "iter 8450: loss 3.8738, time 159.36ms, mfu 33.53%\n",
            "iter 8460: loss 3.8719, time 159.56ms, mfu 33.75%\n",
            "iter 8470: loss 3.8012, time 159.44ms, mfu 33.96%\n",
            "iter 8480: loss 3.7613, time 159.67ms, mfu 34.14%\n",
            "iter 8490: loss 3.8585, time 159.71ms, mfu 34.29%\n",
            "iter 8500: loss 3.9026, time 159.63ms, mfu 34.44%\n",
            "iter 8510: loss 3.8605, time 160.23ms, mfu 34.56%\n",
            "iter 8520: loss 3.9526, time 160.32ms, mfu 34.66%\n",
            "iter 8530: loss 4.0246, time 160.38ms, mfu 34.75%\n",
            "iter 8540: loss 3.9091, time 160.24ms, mfu 34.84%\n",
            "iter 8550: loss 4.0049, time 159.98ms, mfu 34.92%\n",
            "iter 8560: loss 3.9016, time 159.48ms, mfu 35.01%\n",
            "iter 8570: loss 3.8289, time 158.83ms, mfu 35.10%\n",
            "iter 8580: loss 4.0184, time 159.07ms, mfu 35.17%\n",
            "iter 8590: loss 3.9032, time 159.29ms, mfu 35.24%\n",
            "iter 8600: loss 3.8643, time 158.78ms, mfu 35.31%\n",
            "iter 8610: loss 3.8962, time 159.81ms, mfu 35.35%\n",
            "iter 8620: loss 4.0139, time 160.16ms, mfu 35.38%\n",
            "iter 8630: loss 3.8403, time 160.37ms, mfu 35.40%\n",
            "iter 8640: loss 3.9929, time 159.15ms, mfu 35.44%\n",
            "iter 8650: loss 3.9709, time 159.41ms, mfu 35.48%\n",
            "iter 8660: loss 3.8226, time 159.93ms, mfu 35.50%\n",
            "iter 8670: loss 3.9018, time 160.46ms, mfu 35.50%\n",
            "iter 8680: loss 3.8494, time 160.31ms, mfu 35.51%\n",
            "iter 8690: loss 3.9106, time 160.05ms, mfu 35.53%\n",
            "step 8700: train loss 3.9040, val loss 3.8973\n",
            "iter 8700: loss 3.8340, time 22472.61ms, mfu 32.00%\n",
            "iter 8710: loss 3.8361, time 159.77ms, mfu 32.37%\n",
            "iter 8720: loss 3.9104, time 159.27ms, mfu 32.72%\n",
            "iter 8730: loss 3.9529, time 159.13ms, mfu 33.03%\n",
            "iter 8740: loss 3.9074, time 159.50ms, mfu 33.30%\n",
            "iter 8750: loss 4.0017, time 159.74ms, mfu 33.55%\n",
            "iter 8760: loss 4.0218, time 159.33ms, mfu 33.77%\n",
            "iter 8770: loss 3.8628, time 159.22ms, mfu 33.98%\n",
            "iter 8780: loss 3.9146, time 158.98ms, mfu 34.17%\n",
            "iter 8790: loss 3.8745, time 159.70ms, mfu 34.33%\n",
            "iter 8800: loss 3.7854, time 159.25ms, mfu 34.48%\n",
            "iter 8810: loss 3.9160, time 159.76ms, mfu 34.60%\n",
            "iter 8820: loss 3.9517, time 159.94ms, mfu 34.71%\n",
            "iter 8830: loss 3.8984, time 159.20ms, mfu 34.82%\n",
            "iter 8840: loss 3.8071, time 160.02ms, mfu 34.90%\n",
            "iter 8850: loss 3.8020, time 160.42ms, mfu 34.97%\n",
            "iter 8860: loss 3.9491, time 160.26ms, mfu 35.03%\n",
            "iter 8870: loss 3.9344, time 159.69ms, mfu 35.10%\n",
            "iter 8880: loss 3.8824, time 159.12ms, mfu 35.18%\n",
            "iter 8890: loss 3.8487, time 159.34ms, mfu 35.24%\n",
            "iter 8900: loss 3.8331, time 160.04ms, mfu 35.28%\n",
            "iter 8910: loss 3.9477, time 160.67ms, mfu 35.31%\n",
            "iter 8920: loss 3.8930, time 160.06ms, mfu 35.34%\n",
            "iter 8930: loss 3.8126, time 159.23ms, mfu 35.39%\n",
            "iter 8940: loss 3.9229, time 159.94ms, mfu 35.42%\n",
            "iter 8950: loss 3.8846, time 160.76ms, mfu 35.43%\n",
            "iter 8960: loss 3.8745, time 160.45ms, mfu 35.44%\n",
            "iter 8970: loss 4.0195, time 159.59ms, mfu 35.47%\n",
            "iter 8980: loss 3.8582, time 159.40ms, mfu 35.50%\n",
            "iter 8990: loss 4.0601, time 160.10ms, mfu 35.52%\n",
            "step 9000: train loss 3.8952, val loss 3.8947\n",
            "iter 9000: loss 3.9875, time 22602.12ms, mfu 31.99%\n",
            "iter 9010: loss 3.9639, time 160.08ms, mfu 32.36%\n",
            "iter 9020: loss 3.8030, time 160.49ms, mfu 32.68%\n",
            "iter 9030: loss 3.7695, time 160.64ms, mfu 32.96%\n",
            "iter 9040: loss 3.8797, time 160.32ms, mfu 33.22%\n",
            "iter 9050: loss 4.0219, time 160.55ms, mfu 33.45%\n",
            "iter 9060: loss 3.9508, time 160.42ms, mfu 33.67%\n",
            "iter 9070: loss 3.8304, time 159.67ms, mfu 33.87%\n",
            "iter 9080: loss 3.8801, time 159.40ms, mfu 34.07%\n",
            "iter 9090: loss 3.8531, time 159.70ms, mfu 34.23%\n",
            "iter 9100: loss 3.9108, time 160.40ms, mfu 34.37%\n",
            "iter 9110: loss 3.9532, time 160.41ms, mfu 34.49%\n",
            "iter 9120: loss 3.8166, time 160.35ms, mfu 34.60%\n",
            "iter 9130: loss 3.8456, time 160.13ms, mfu 34.70%\n",
            "iter 9140: loss 3.8078, time 158.94ms, mfu 34.82%\n",
            "iter 9150: loss 3.9969, time 159.68ms, mfu 34.91%\n",
            "iter 9160: loss 3.9210, time 159.48ms, mfu 35.00%\n",
            "iter 9170: loss 3.9786, time 160.11ms, mfu 35.06%\n",
            "iter 9180: loss 3.8188, time 160.35ms, mfu 35.11%\n",
            "iter 9190: loss 3.9122, time 159.98ms, mfu 35.17%\n",
            "iter 9200: loss 3.9102, time 158.75ms, mfu 35.25%\n",
            "iter 9210: loss 3.8280, time 159.98ms, mfu 35.29%\n",
            "iter 9220: loss 3.8128, time 160.36ms, mfu 35.32%\n",
            "iter 9230: loss 3.9427, time 160.28ms, mfu 35.35%\n",
            "iter 9240: loss 3.8694, time 159.48ms, mfu 35.39%\n",
            "iter 9250: loss 3.8301, time 158.61ms, mfu 35.45%\n",
            "iter 9260: loss 3.9059, time 160.20ms, mfu 35.47%\n",
            "iter 9270: loss 3.9333, time 160.31ms, mfu 35.48%\n",
            "iter 9280: loss 3.8694, time 160.06ms, mfu 35.50%\n",
            "iter 9290: loss 3.9670, time 159.17ms, mfu 35.53%\n",
            "step 9300: train loss 3.9038, val loss 3.8914\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9300: loss 3.9345, time 24481.24ms, mfu 32.00%\n",
            "iter 9310: loss 3.9484, time 159.99ms, mfu 32.37%\n",
            "iter 9320: loss 4.0463, time 160.07ms, mfu 32.69%\n",
            "iter 9330: loss 3.9329, time 159.75ms, mfu 33.00%\n",
            "iter 9340: loss 3.9508, time 159.95ms, mfu 33.26%\n",
            "iter 9350: loss 3.7819, time 160.46ms, mfu 33.49%\n",
            "iter 9360: loss 3.9349, time 160.45ms, mfu 33.70%\n",
            "iter 9370: loss 3.9394, time 160.53ms, mfu 33.89%\n",
            "iter 9380: loss 3.9285, time 160.54ms, mfu 34.05%\n",
            "iter 9390: loss 3.7373, time 160.31ms, mfu 34.21%\n",
            "iter 9400: loss 3.9886, time 160.58ms, mfu 34.34%\n",
            "iter 9410: loss 3.9487, time 160.72ms, mfu 34.45%\n",
            "iter 9420: loss 3.9073, time 159.97ms, mfu 34.58%\n",
            "iter 9430: loss 4.1061, time 159.12ms, mfu 34.70%\n",
            "iter 9440: loss 3.9533, time 159.09ms, mfu 34.82%\n",
            "iter 9450: loss 3.9050, time 159.64ms, mfu 34.91%\n",
            "iter 9460: loss 3.8810, time 160.46ms, mfu 34.98%\n",
            "iter 9470: loss 3.7947, time 160.33ms, mfu 35.04%\n",
            "iter 9480: loss 3.9404, time 159.87ms, mfu 35.10%\n",
            "iter 9490: loss 3.7964, time 159.05ms, mfu 35.18%\n",
            "iter 9500: loss 3.9543, time 159.07ms, mfu 35.25%\n",
            "iter 9510: loss 3.9092, time 159.07ms, mfu 35.31%\n",
            "iter 9520: loss 3.8046, time 159.62ms, mfu 35.36%\n",
            "iter 9530: loss 4.0386, time 159.99ms, mfu 35.39%\n",
            "iter 9540: loss 3.8092, time 159.07ms, mfu 35.43%\n",
            "iter 9550: loss 3.9046, time 159.23ms, mfu 35.47%\n",
            "iter 9560: loss 3.9343, time 159.02ms, mfu 35.52%\n",
            "iter 9570: loss 3.9589, time 160.55ms, mfu 35.52%\n",
            "iter 9580: loss 3.9307, time 160.05ms, mfu 35.53%\n",
            "iter 9590: loss 3.9339, time 159.45ms, mfu 35.56%\n",
            "step 9600: train loss 3.8972, val loss 3.8901\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9600: loss 3.9776, time 24732.73ms, mfu 32.02%\n",
            "iter 9610: loss 3.8513, time 159.90ms, mfu 32.39%\n",
            "iter 9620: loss 3.8458, time 160.05ms, mfu 32.72%\n",
            "iter 9630: loss 3.8957, time 160.31ms, mfu 33.00%\n",
            "iter 9640: loss 3.8957, time 160.33ms, mfu 33.26%\n",
            "iter 9650: loss 3.8879, time 160.55ms, mfu 33.49%\n",
            "iter 9660: loss 4.0033, time 160.26ms, mfu 33.70%\n",
            "iter 9670: loss 3.8978, time 159.33ms, mfu 33.91%\n",
            "iter 9680: loss 3.8756, time 160.03ms, mfu 34.09%\n",
            "iter 9690: loss 3.8344, time 159.90ms, mfu 34.25%\n",
            "iter 9700: loss 3.8250, time 160.11ms, mfu 34.39%\n",
            "iter 9710: loss 3.9784, time 159.20ms, mfu 34.53%\n",
            "iter 9720: loss 3.8490, time 159.25ms, mfu 34.66%\n",
            "iter 9730: loss 4.0405, time 159.34ms, mfu 34.78%\n",
            "iter 9740: loss 3.8540, time 160.27ms, mfu 34.86%\n",
            "iter 9750: loss 3.8918, time 160.24ms, mfu 34.93%\n",
            "iter 9760: loss 3.8540, time 160.06ms, mfu 35.00%\n",
            "iter 9770: loss 3.9788, time 160.25ms, mfu 35.06%\n",
            "iter 9780: loss 3.8921, time 160.13ms, mfu 35.12%\n",
            "iter 9790: loss 3.8682, time 159.26ms, mfu 35.19%\n",
            "iter 9800: loss 3.8996, time 158.92ms, mfu 35.26%\n",
            "iter 9810: loss 3.8841, time 158.89ms, mfu 35.33%\n",
            "iter 9820: loss 3.8789, time 159.23ms, mfu 35.38%\n",
            "iter 9830: loss 3.8280, time 160.13ms, mfu 35.40%\n",
            "iter 9840: loss 3.8669, time 160.47ms, mfu 35.42%\n",
            "iter 9850: loss 4.0621, time 159.93ms, mfu 35.45%\n",
            "iter 9860: loss 3.9403, time 159.40ms, mfu 35.48%\n",
            "iter 9870: loss 3.9313, time 159.11ms, mfu 35.52%\n",
            "iter 9880: loss 3.7525, time 159.42ms, mfu 35.55%\n",
            "iter 9890: loss 3.9098, time 160.03ms, mfu 35.56%\n",
            "step 9900: train loss 3.8913, val loss 3.8977\n",
            "iter 9900: loss 3.8847, time 22518.98ms, mfu 32.03%\n",
            "iter 9910: loss 3.9343, time 159.34ms, mfu 32.40%\n",
            "iter 9920: loss 3.9695, time 159.84ms, mfu 32.73%\n",
            "iter 9930: loss 3.7876, time 159.87ms, mfu 33.03%\n",
            "iter 9940: loss 3.7985, time 160.38ms, mfu 33.28%\n",
            "iter 9950: loss 3.8511, time 160.40ms, mfu 33.51%\n",
            "iter 9960: loss 3.9532, time 160.36ms, mfu 33.72%\n",
            "iter 9970: loss 3.9783, time 159.84ms, mfu 33.92%\n",
            "iter 9980: loss 3.9834, time 159.45ms, mfu 34.10%\n",
            "iter 9990: loss 3.9436, time 159.01ms, mfu 34.28%\n",
            "iter 10000: loss 3.8185, time 159.52ms, mfu 34.43%\n"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyKrsSISRE8r"
      },
      "outputs": [],
      "source": [
        "# start = \"How to join a tbank-research?\"\n",
        "# start = \"How to join a tbank-research?\"\n",
        "start = \"It's snow.\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdvmXlpURE8r",
        "outputId": "832380d2-df49-4035-ffcd-4002f7830a6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-9f0241acb2d4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_dE7EgKRHhh",
        "outputId": "a7e0c9e6-0239-41d5-a71e-2c8790b15e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's snow.\n",
            "\n",
            "One of the last few months we've been so lucky to have it. We're very lucky to have it on a ship that's still an island, so I had to live there long enough.\n",
            "\n",
            "But for the sake of life I'm not sure it will happen. I know I'm going to visit my husband, and he's a great friend, but I'll have to wait for him to leave.\n",
            "\n",
            "I've been so lucky to have it on a ship that we all went to visit our ship. I've met my father for the last few days. He's the husband of a man who is an active member of the ship and has been there since my age. I'm not sure what I'm going to tell.\n",
            "\n",
            "This is the second mission I've met. That is what the sea has been. The last, part of the sea has been there in the past year, but it's actually one of the most important missions to come.\n",
            "\n",
            "For a long time I've just met my father and raised my heart. He never was there.\n",
            "\n",
            "In the last few months, for a long time, the captain has told me what he has done. He has been here since the morning of the day when we have had it.\n",
            "\n",
            "He has been there since his age, but since then he has now moved to the sea.\n",
            "\n",
            "I know this is how he brings out his personal life. I know of the importance of time and effort. I can't blame you for the time and effort.\n",
            "\n",
            "I know I've lost control of me. I know who I am. I've lost control of myself. I know that I lost control of myself.\n",
            "\n",
            "So I know that I had my entire life. I've finally lost control of myself. I've lost control of myself. I've been there, and I've lost control of myself. Because I've lost control of myself.\n",
            "\n",
            "I know that my own life has been there since my age, but it is a little bit of an effort to do that.\n",
            "\n",
            "I know that I have a lost control of myself and that I can get my life. I am on that side of this. I will always be able to recover to keep my life alive. I am my own.\n",
            "\n",
            "I don't know what I'm supposed to be. I'm the only one. I have lost control of myself. I have\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"We have a lot of snow that is at my disposal. I like to go to the end of this week, and I'm very excited to be able to bring it back to the final day of the week,\" said Jeff Scott, a former city council member for the Mayor's office.\n",
            "\n",
            "\"It was a major, really friendly day for us, but we all knew it was a great day for us and it was really good to get in and be able to get a better feeling of the game,\" Scott said.\n",
            "\n",
            "The mayor's office has more than 60,000 signatures.\n",
            "\n",
            "In a report back in mid-March, the city council has also been holding a meeting of the council's council.\n",
            "\n",
            "\"We've been meeting people who brought this community to the next stage,\" said Scott.<|endoftext|>This week, we're getting more heated about: the future of smartphones, the iPhone, and the next big update. The new update will include how smart it will be if you want it and how it will be for you.\n",
            "\n",
            "These are good news for you, but I think that is some of the early news and the most critical of Windows users. (Just be warned.)\n",
            "\n",
            "When you're planning to start the update, let's start by with your friends.\n",
            "\n",
            "By signing the new update, you will need to do it properly.\n",
            "\n",
            "The update will send you some feedback and suggestions.\n",
            "\n",
            "And if you want to share a new update, let me know in the comments below.\n",
            "\n",
            "That said, there are the issues with your phone when you're planning to update.\n",
            "\n",
            "We do have a few updates to it and we will not be going any further into our discussions.\n",
            "\n",
            "When we have a new update that we are going to update on the latest update and we will be updating that and then update it to the game.\n",
            "\n",
            "If you want to add a new update to our new update, we'll support the new update as soon as we are out.\n",
            "\n",
            "If you want to take the update, we'll have some fresh comments from the launchers.\n",
            "\n",
            "I'm going to update that at the end of the day, we'll update it after launch.\n",
            "\n",
            "We'll be adding updates in the next update, so let's discuss with your group.\n",
            "\n",
            "A few changes will be added, so we've added several new fixes in the plans.\n",
            "\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"It's the same thing with the snow. It's snow, snow. It's the same thing with snow. It's the same thing, you're the one with the same thing with snow.\"\n",
            "\n",
            "The snow has gone up to the snow and snow.\n",
            "\n",
            "The snow, ice and snow appear to be around.\n",
            "\n",
            "After snow appears to be a hot spot.\n",
            "\n",
            "The snow can't handle any more rain.\n",
            "\n",
            "\"It's bad if you don't, it's a good thing and it's a good thing.\"\n",
            "\n",
            "The snow has gone up to the snow so it can't handle any more rain.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "It has been up to the snow and snow and snow as snow has been up to the snow.\n",
            "\n",
            "The snow is still in the snow and snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow, so that snow can't handle any more rain.\n",
            "\n",
            "As I've mentioned above, snow has gone up to the snow.\n",
            "\n",
            "They've gone up to the snow.\n",
            "\n",
            "The snow is not an ice-free snow, so that snow is a good thing.\n",
            "\n",
            "The snow is still in the snow and snow can't handle any more rain.\n",
            "\n",
            "The snow has gone up to the snow and snow has gone up to the snow, so the snow will be up to the snow.\n",
            "\n",
            "It's the snow that snow has gone up to the snow.\n",
            "\n",
            "There are no snow with snow.\n",
            "\n",
            "The snow is in the snow and snow has gone up to the snow.\n",
            "\n",
            "The snow is still in the snow and snow has gone up to the snow.\n",
            "\n",
            "The snow is still in the snow and snow has gone up to the snow.\n",
            "\n",
            "There are no snow but snow is still in the snow.\n",
            "\n",
            "I'm sorry this was actually caused.\n",
            "\n",
            "The snow and snow is still in the snow and snow has gone up to the snow and snow has gone down to the snow.\n",
            "\n",
            "It has gone up to the snow.\n",
            "\n",
            "A snow with snow on and snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "His camera video, which had the first camera video camera camera in the last few days, has already been featured in this video.\n",
            "\n",
            "He also has an all-time favourite photographer.\n",
            "\n",
            "It's an interesting picture.\n",
            "\n",
            "(Chris Clark, a photographer behind the 3D) is with his crew.\n",
            "\n",
            "(L.L.L.L.L.L.L.L.L.L.L.)L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.V.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L\n",
            "---------------\n",
            "It's snow. Now, the first day for me was the most amazing morning of night.\n",
            "\n",
            "\"It was a big night. It was a really intense night. It was terrible and we kept eating it. I got used to it and it was amazing,\" he said. \"That's the day of the day.\" He had tried to beat and began to get his house back up.\n",
            "\n",
            "\"I'm very proud of it so much.\"\n",
            "\n",
            "\"It's our own first day, but it's pretty amazing. It was amazing and I wasn't really really nervous. It was a lot of warm and great. It was really incredible. That's really incredible. It was fantastic and it was incredible. I was really proud of it so much. It was amazing. It was amazing. I was really excited to have this week's. I really enjoyed it and I wanted to show off the other guys it was amazing. It was wonderful. It was amazing. I was really excited to have this week.\"\n",
            "\n",
            "\"We called it a day of the day of the week. It's really amazing. It was really great and amazing. It was amazing and it was amazing. It was amazing and I really enjoyed it. It was amazing. It really was amazing. It was amazing. It was great. It was amazing. It was awesome. It was amazing. We were really excited to have this week's. It was amazing. It was great. It was amazing. It was amazing. It was incredible. It was amazing. It was amazing. It was amazing and it was amazing. It was amazing. It was amazing. It was amazing. It was amazing. It was amazing. It was incredible. It was amazing. It was amazing. It's amazing. It was amazing. It was amazing. It was amazing. It was amazing. It's amazing. I thought it was amazing. It was amazing. It was great. It was incredible. It was amazing. It was amazing. It was great. It was incredible. It was amazing. It was unbelievable. It was amazing. It was awesome. It was amazing. It was amazing. It was amazing. It was amazing. This was amazing. It was amazing. It was amazing. It was amazing! It was amazing. It was amazing. It was amazing. It was amazing and it was amazing. It was fantastic. It was amazing. It was unbelievable. It was amazing and beautiful. It was\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wdHC7WHR6If"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PHCo48LR_f5"
      },
      "outputs": [],
      "source": [
        "start = \"How to join a tbank-research?\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNkCDeDwR_f6",
        "outputId": "f6553e3a-4634-4e4c-dac5-90b81d3b5ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to join a tbank-research?\n",
            "\n",
            "Yes, that is. It is. It is. It is. It is. Since the beginning of the late 20th century the world is a far more distant future. But what we go is a great and we do it every day. We have a great time. We are a far more distant future. And we have a great time. We can’t wait for the next few years.\n",
            "\n",
            "To talk about the future of the world is for a moment. And then a moment of uncertainty in the present, we know that there will be no sooner than tomorrow. We have an idea of why we are now all we need to know. We have a vision for the future. We have to be clear that we have a vision for the future. We have to be clear that we have a vision for the future. We have a vision for potential future. We have a vision for the future. We have an idea of the future. We have a vision for the future. We have a vision for the future. We have a vision for us. We have a vision for the future. We have a vision for the future. We have an vision for the future. We have nothing but vision for other worlds. We have a vision for our future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. Our vision means a vision for the future. We have a vision for the future. Our vision is in the future. We have a vision for the future. We will be open to future. We are ready, we have a vision for the future. We have a vision for the future. There will be no sooner than tomorrow. We have a vision for the future. Our vision for the future. We have an idea for the future. We have a vision for future. We have a vision for the future. We want to be clear. We have a vision for future. We have a vision for future. We need a vision for the future. We need a vision for future. We need a vision for 2018. We have a vision for future. We need a vision for future. We need a vision for the future. We need a vision for future. We need\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "If you had a good time, and you were on a bumble a bit more than a couple of days away, the process of getting to know the data was going to be interesting. The most interesting part about the process is a complete mess. The first step of the process was to get a good idea of what a tbank-research solution that I call “bumble” to go to:\n",
            "\n",
            "What if a tbank-research solution that we call “bumble” on a tbank-research solution, that we’ll get to know the exact same thing. And next step is to get a good idea of how the whole process is going to work. And then there’s a solution that’s likely to go through the process.\n",
            "\n",
            "OK, we’ve got a big deal on Tbank-research, which is extremely small. Once you’ve got a good idea of how to manage it, it would be interesting, and we’re going to have to get our goals going and it’s pretty good. But you know what is going on, and when you get a good idea of what you’re going to do, that information is going to look very important.\n",
            "\n",
            "In the meantime, we’ve got a really big deal on the tbank-research solution. But this is happening in the nongovernmental market. We’re going to have to take advantage of the data, and then it’s going to be with the tbank-research solution.\n",
            "\n",
            "That’s it.\n",
            "\n",
            "Q: Do any of that?\n",
            "\n",
            "Well, I think we’re going to see a lot of the data, and we’re going to come up with a few different data. So the data that’s going to be related to the Tbank-research solution where you’re going to need a lot of work. And then there’s going to be kind of a step up to make sure that we’re making really informed decisions based on the data, because we’re going to have the data that’s going to put into the data, and that’s obviously a step up to make sure we’re dealing with the data itself. And then it’s going to be very clear if we can make sure that\n",
            "---------------\n",
            "How to join a tbank-research? You've got a good idea!\n",
            "\n",
            "If you're a local student, what are your thoughts on what's in your mind?<|endoftext|>A number of New Englanders are doing a little differently.\n",
            "\n",
            "The national average of 3.2 million respondents is 7 to 8 percent, the same as Americans, who are planning to increase their productivity or reduce economic growth to about 50 percent.\n",
            "\n",
            "The survey is a part of the public figure that appears to be more confident at first than those in the economy.\n",
            "\n",
            "Here's what the poll looked like. You can guess what's up to you, just look at the poll itself.\n",
            "\n",
            "For instance, the number of people in the U.S. has more than doubled to a factor in the job growth period.\n",
            "\n",
            "Roughly half of the population in the U.S. is still the largest group in the country, and the number in the US has increased to about 1.1 percent in the same period.\n",
            "\n",
            "The report comes as the first quarter of the survey released on Monday that tracked how the government had managed to grow 35 percent and the number in the U.S.\n",
            "\n",
            "A survey of 7.1 million people in the U.S. was conducted in the U.S. and 10.6 million with the highest population in the U.S.\n",
            "\n",
            "The survey is a part of the survey conducted on September 19 – so far, it's clear that the survey is up close to the charts.<|endoftext|>This article is from the archive of all the wonderful facts about the best things we do.\n",
            "\n",
            "It was written at a British government radio show last night on the Internet right after it was broadcast.\n",
            "\n",
            "This morning, our friend was at the top of the news conference with him, and we have no idea what was going to happen. We’re in a different world, and we have no idea what was going on.\n",
            "\n",
            "We can’t help with the fact that the people who knew that they knew the worst things they have to watch, and the people who knew that was in the air. If they had told us that they were going to have a bigger world than they were, they didn’t know what was going on, and we still need to take a picture like that.\n",
            "\n",
            "The main thing they have heard is that we have a huge audience, and that’s where they were, and that means\n",
            "---------------\n",
            "How to join a tbank-research? Our goal is to provide a more efficient and secure, reliable, and sustainable environment, to help build roads and provide education and education. Our goal depends on the way opportunities work. Our goal is to deploy jobs and make those communities more productive.\n",
            "\n",
            "In this interview we often discuss the current situation. We also discuss the specific issues of training in schools, public education, and the need to be careful in an environment where students can learn and provide access to education directly from their schools. The school programs are a key component of the new project. We also talk about the proposed funding method and how it would be implemented to support children whose primary goals are to improve their literacy. We also discuss new aspects of how schools would evolve, and what should be more important in the classroom.\n",
            "\n",
            "We are also discussing how funding a school/school program can be used to be completed. We want to explore important issues. We also discuss how the funding process can be used in schools. Our goal is to increase the number of students who are not eligible for the schools. We are working to make sure that teachers can participate in the program as a way to understand the needs of students.\n",
            "\n",
            "One important aspect of this new project is the decision-making process. We can make sure that teachers can receive funding from the faculty in order to prepare them for the new project. We would also discuss ways to ensure that we have a sustainable economy and also to help families grow families.\n",
            "\n",
            "We have also asked students to join the project. We are working to develop a policy approach to helping students learn and will help them learn and help them learn.\n",
            "\n",
            "Our goal is to make sure that we serve teachers who want to do what they want to help them learn and to help them learn and to help. Our goal is to make sure that their teachers and teachers are safe to start a project. And we are also working to make sure that we have a sustainable economy that is not sustainable. Our goal is to create jobs and make sure that we build it better than it would be.\n",
            "\n",
            "We also want to help members learn and do their homework as a way to help them learn. We want to help the kids learn and to help them learn and to help them learn and to help them learn and to help them learn and to help them learn and to help them learn and to help them learn.\n",
            "\n",
            "We want to make sure that we have a sustainable economy and need to build a\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "Anastro-sherson@sherson.com<|endoftext|>Kurnehana-based social networking giant\n",
            "\n",
            "The group's first open-source startup developer, which brings together an interactive, interactive and web-based platform.\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "I believe that all modern analytics is a good idea to create a web-based platform. But I believe that analytics is the best way to quickly build that platform. It's a little bit more complex.\n",
            "\n",
            "\"It's a great tool to be able to start developing this platform,\" she said.\n",
            "\n",
            "Kurnehana-based web-based companies are continuing a variety of ways to reach a wide range of platform designs.\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "In Kurnehana-based platform, Kurnehana-based social networking giant, has created a series of fast-growing companies that have run the web-based platform, including Kurnehana-based social networking giant JLA.\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based software company TK\n",
            "\n",
            "Kurnehana-based web-based software company COO\n",
            "\n",
            "Kurnehana-based social networking giant Yoga-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based mobile platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r1yj6Kygjwd"
      },
      "source": [
        "## GPT: Reflex-Attention (3)\n",
        "**1-6 layer: 2head: SA, 2head: CA(h of prev layer), 2head: CA(h of prev2x layer)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVWP8FGVgjwe"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "class ReflexAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # fullSA for all 6heads:\n",
        "        # splitting embeds for 6heads\n",
        "\n",
        "        #i_hidden_state:\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        if self.model_type == 'simple' or (self.model_type == 'reflex' and hidden_states.__len__() < 2):\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v,\n",
        "                                                                 attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        else:\n",
        "            # reflex_attention 6heads from\n",
        "            k_prevx, v_prevx = hidden_states[1]\n",
        "            k_prevxx, v_prevxx = hidden_states[0]\n",
        "\n",
        "            #SA_i=SA(k_i, v_i, q_i) for 2head\n",
        "            q_i, k_i, v_i = q[:, :2, :, :], k[:, :2, :, :], v[:, :2, :, :]\n",
        "            SA_i = torch.nn.functional.scaled_dot_product_attention(q_i, k_i, v_i,\n",
        "                                                                    attn_mask=None,\n",
        "                                                                    dropout_p=self.dropout if self.training else 0,\n",
        "                                                                    is_causal=True)\n",
        "            #SA_i=SA(k_i, v_i, q_i) for 2head\n",
        "\n",
        "            q_i_for_prevx_head, k_prevx_head, v_prevx_head = q[:, 2:4, :, :], k_prevx[:, 2:4, :, :], v_prevx[:, 2:4, :, :]\n",
        "            CA_ix = torch.nn.functional.scaled_dot_product_attention(q_i_for_prevx_head, k_prevx_head, v_prevx_head,\n",
        "                                                                                attn_mask=None,\n",
        "                                                                                dropout_p=self.dropout if self.training else 0,\n",
        "                                                                                is_causal=True)\n",
        "            # 2 head for previous previous layer\n",
        "\n",
        "            q_i_for_prevxx_head, k_prevxx_head, v_prevxx_head = q[:, 4:, :, :], k_prevxx[:, 4:, :, :], v_prevxx[:, 4:, :, :]\n",
        "            CA_ixx = torch.nn.functional.scaled_dot_product_attention(q_i_for_prevxx_head, k_prevxx_head, v_prevxx_head,\n",
        "                                                                                attn_mask=None,\n",
        "                                                                                dropout_p=self.dropout if self.training else 0,\n",
        "                                                                                is_causal=True)\n",
        "            # concatenate: Attn_i= Cat[SA_i, CA_i_i-1, CA_i_i-2]\n",
        "            y = torch.cat([SA_i, CA_ix, CA_ixx], dim=1)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y, (k, v)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = ReflexAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        attn_kv = self.attn(self.ln_1(x), hidden_states)\n",
        "        x = x + attn_kv[0] #y for 0\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_kv[1] #hidden_states_i for k,v for 1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "\n",
        "        hidden_states = []\n",
        "\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x, hidden_state = block(x, hidden_states)\n",
        "\n",
        "            # level1\n",
        "            if self.config.model_type == 'reflex':\n",
        "                if i >= 2:\n",
        "                  hidden_states.pop(0)\n",
        "                hidden_states.append(hidden_state)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "YRSfaZvOgjwf",
        "outputId": "dd7747f7-5de8-4cee-ceb5-2328a78fd4e7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ],
            "text/plain": [
              "__main__.GPT"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcmipRlngjwf"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'reflex' # or 'simple'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_eGnG2ygjwf",
        "outputId": "04da75ec-23ac-4d66-aafb-76675c95558f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubnPJyAvgjwg"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'reflexAttnGPT={config}1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Psh2hw7Ggjwg"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/reglex_attn_GPT'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ],
      "metadata": {
        "id": "Q9qLKTE1iL7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGuquyP_gjwg",
        "outputId": "00b169da-2805-4c1f-ff7b-a1f0b267cb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 32,768\n"
          ]
        }
      ],
      "source": [
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDzgPplogjwg"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSd_sgJCgjwh",
        "outputId": "fe0dd48f-ceaf-4463-c522-fa18837ba068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33TBI2wDgjwh",
        "outputId": "8406f2da-a992-41b9-dd08-b43b805a648c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 13, with 9,984 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-37-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CTUUmyFggjwh",
        "outputId": "e4077e28-4c4e-4ae7-d521-2f4b843cd61c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:f25ngbln) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">reflexAttnGPT=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/f25ngbln' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/f25ngbln</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241124_214029-f25ngbln/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:f25ngbln). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241124_214140-oxit79vl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/oxit79vl' target=\"_blank\">reflexAttnGPT=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/oxit79vl' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/oxit79vl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 163 \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.133000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 110 \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.572000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 17 \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.667000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 35 \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:49.895000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-28-ff320e365d7f> line 94 \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1124 21:41:50.095000 190 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.9687, val loss 10.9696\n",
            "iter 0: loss 10.9723, time 27964.86ms, mfu -100.00%\n",
            "iter 10: loss 9.7752, time 158.71ms, mfu 35.95%\n",
            "iter 20: loss 9.2403, time 159.47ms, mfu 35.94%\n",
            "iter 30: loss 8.7068, time 160.25ms, mfu 35.90%\n",
            "iter 40: loss 8.0201, time 159.29ms, mfu 35.89%\n",
            "iter 50: loss 7.3922, time 158.90ms, mfu 35.90%\n",
            "iter 60: loss 7.2296, time 158.03ms, mfu 35.92%\n",
            "iter 70: loss 7.0705, time 158.37ms, mfu 35.93%\n",
            "iter 80: loss 7.1468, time 158.76ms, mfu 35.93%\n",
            "iter 90: loss 6.7766, time 159.08ms, mfu 35.92%\n",
            "iter 100: loss 6.7940, time 159.37ms, mfu 35.91%\n",
            "iter 110: loss 6.6914, time 158.85ms, mfu 35.91%\n",
            "iter 120: loss 6.5497, time 159.08ms, mfu 35.91%\n",
            "iter 130: loss 6.3929, time 158.87ms, mfu 35.91%\n",
            "iter 140: loss 6.3806, time 158.94ms, mfu 35.91%\n",
            "iter 150: loss 6.3009, time 159.19ms, mfu 35.90%\n",
            "iter 160: loss 6.4253, time 159.20ms, mfu 35.90%\n",
            "iter 170: loss 6.3922, time 158.84ms, mfu 35.90%\n",
            "iter 180: loss 6.1838, time 158.98ms, mfu 35.90%\n",
            "iter 190: loss 6.2093, time 159.48ms, mfu 35.89%\n",
            "iter 200: loss 6.2313, time 159.39ms, mfu 35.88%\n",
            "iter 210: loss 6.2009, time 159.64ms, mfu 35.86%\n",
            "iter 220: loss 6.0941, time 159.83ms, mfu 35.85%\n",
            "iter 230: loss 6.2819, time 160.34ms, mfu 35.82%\n",
            "iter 240: loss 6.0491, time 160.08ms, mfu 35.80%\n",
            "iter 250: loss 6.0417, time 159.84ms, mfu 35.79%\n",
            "iter 260: loss 6.0717, time 158.97ms, mfu 35.80%\n",
            "iter 270: loss 6.1343, time 158.90ms, mfu 35.81%\n",
            "iter 280: loss 6.1254, time 159.62ms, mfu 35.81%\n",
            "iter 290: loss 6.0618, time 159.06ms, mfu 35.81%\n",
            "step 300: train loss 6.0098, val loss 5.9581\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 300: loss 5.8438, time 24044.12ms, mfu 32.26%\n",
            "iter 310: loss 5.9054, time 160.43ms, mfu 32.59%\n",
            "iter 320: loss 5.9453, time 159.92ms, mfu 32.90%\n",
            "iter 330: loss 5.9252, time 160.00ms, mfu 33.17%\n",
            "iter 340: loss 6.0186, time 160.45ms, mfu 33.41%\n",
            "iter 350: loss 5.9117, time 160.07ms, mfu 33.63%\n",
            "iter 360: loss 5.7801, time 160.34ms, mfu 33.83%\n",
            "iter 370: loss 5.8110, time 160.70ms, mfu 34.00%\n",
            "iter 380: loss 5.8372, time 160.10ms, mfu 34.16%\n",
            "iter 390: loss 5.7918, time 159.52ms, mfu 34.32%\n",
            "iter 400: loss 5.7597, time 158.92ms, mfu 34.48%\n",
            "iter 410: loss 5.7818, time 159.37ms, mfu 34.61%\n",
            "iter 420: loss 5.7871, time 159.01ms, mfu 34.74%\n",
            "iter 430: loss 5.7067, time 159.30ms, mfu 34.85%\n",
            "iter 440: loss 5.7979, time 159.90ms, mfu 34.93%\n",
            "iter 450: loss 5.8350, time 160.33ms, mfu 35.00%\n",
            "iter 460: loss 5.7481, time 160.30ms, mfu 35.06%\n",
            "iter 470: loss 5.7544, time 159.60ms, mfu 35.13%\n",
            "iter 480: loss 5.6953, time 158.82ms, mfu 35.21%\n",
            "iter 490: loss 5.7643, time 159.25ms, mfu 35.27%\n",
            "iter 500: loss 5.6469, time 159.71ms, mfu 35.31%\n",
            "iter 510: loss 5.6120, time 160.52ms, mfu 35.34%\n",
            "iter 520: loss 5.5958, time 160.53ms, mfu 35.36%\n",
            "iter 530: loss 5.6727, time 159.45ms, mfu 35.40%\n",
            "iter 540: loss 5.6501, time 158.84ms, mfu 35.45%\n",
            "iter 550: loss 5.6794, time 159.30ms, mfu 35.49%\n",
            "iter 560: loss 5.6105, time 159.92ms, mfu 35.51%\n",
            "iter 570: loss 5.5731, time 160.27ms, mfu 35.52%\n",
            "iter 580: loss 5.6480, time 159.83ms, mfu 35.54%\n",
            "iter 590: loss 5.5176, time 158.94ms, mfu 35.57%\n",
            "step 600: train loss 5.5201, val loss 5.4791\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 600: loss 5.3081, time 24927.16ms, mfu 32.04%\n",
            "iter 610: loss 5.5490, time 159.38ms, mfu 32.41%\n",
            "iter 620: loss 5.2449, time 159.71ms, mfu 32.75%\n",
            "iter 630: loss 5.4314, time 159.41ms, mfu 33.05%\n",
            "iter 640: loss 5.4711, time 159.93ms, mfu 33.31%\n",
            "iter 650: loss 5.3135, time 160.12ms, mfu 33.55%\n",
            "iter 660: loss 5.4598, time 159.74ms, mfu 33.76%\n",
            "iter 670: loss 5.3846, time 160.42ms, mfu 33.94%\n",
            "iter 680: loss 5.3663, time 160.51ms, mfu 34.10%\n",
            "iter 690: loss 5.4706, time 159.87ms, mfu 34.26%\n",
            "iter 700: loss 5.2440, time 159.26ms, mfu 34.42%\n",
            "iter 710: loss 5.4993, time 159.32ms, mfu 34.56%\n",
            "iter 720: loss 5.2520, time 160.12ms, mfu 34.67%\n",
            "iter 730: loss 5.3302, time 160.55ms, mfu 34.75%\n",
            "iter 740: loss 5.2834, time 160.46ms, mfu 34.83%\n",
            "iter 750: loss 5.4660, time 159.40ms, mfu 34.93%\n",
            "iter 760: loss 5.0932, time 158.79ms, mfu 35.03%\n",
            "iter 770: loss 5.2244, time 159.15ms, mfu 35.11%\n",
            "iter 780: loss 5.1930, time 159.67ms, mfu 35.18%\n",
            "iter 790: loss 5.2482, time 161.02ms, mfu 35.20%\n",
            "iter 800: loss 5.1178, time 160.16ms, mfu 35.24%\n",
            "iter 810: loss 5.1857, time 159.13ms, mfu 35.31%\n",
            "iter 820: loss 5.1791, time 159.39ms, mfu 35.35%\n",
            "iter 830: loss 5.2377, time 159.81ms, mfu 35.39%\n",
            "iter 840: loss 5.1999, time 160.09ms, mfu 35.41%\n",
            "iter 850: loss 5.2142, time 160.50ms, mfu 35.43%\n",
            "iter 860: loss 5.1885, time 160.11ms, mfu 35.45%\n",
            "iter 870: loss 5.1122, time 158.95ms, mfu 35.49%\n",
            "iter 880: loss 5.0547, time 159.50ms, mfu 35.52%\n",
            "iter 890: loss 5.0582, time 159.61ms, mfu 35.54%\n",
            "step 900: train loss 5.1193, val loss 5.0778\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 900: loss 5.1538, time 24482.67ms, mfu 32.01%\n",
            "iter 910: loss 5.0937, time 158.68ms, mfu 32.41%\n",
            "iter 920: loss 5.0240, time 158.96ms, mfu 32.76%\n",
            "iter 930: loss 5.1107, time 158.72ms, mfu 33.08%\n",
            "iter 940: loss 5.2438, time 159.19ms, mfu 33.35%\n",
            "iter 950: loss 5.3007, time 159.35ms, mfu 33.60%\n",
            "iter 960: loss 5.0333, time 159.73ms, mfu 33.81%\n",
            "iter 970: loss 4.9773, time 160.14ms, mfu 33.99%\n",
            "iter 980: loss 4.9855, time 160.06ms, mfu 34.16%\n",
            "iter 990: loss 5.0335, time 160.00ms, mfu 34.31%\n",
            "iter 1000: loss 4.9374, time 158.87ms, mfu 34.47%\n",
            "iter 1010: loss 4.8034, time 160.14ms, mfu 34.59%\n",
            "iter 1020: loss 4.8237, time 160.19ms, mfu 34.69%\n",
            "iter 1030: loss 4.8585, time 160.30ms, mfu 34.78%\n",
            "iter 1040: loss 4.9504, time 160.34ms, mfu 34.86%\n",
            "iter 1050: loss 4.9508, time 159.79ms, mfu 34.95%\n",
            "iter 1060: loss 4.8588, time 159.72ms, mfu 35.02%\n",
            "iter 1070: loss 4.8920, time 160.09ms, mfu 35.09%\n",
            "iter 1080: loss 4.8257, time 160.72ms, mfu 35.13%\n",
            "iter 1090: loss 4.8724, time 160.43ms, mfu 35.17%\n",
            "iter 1100: loss 4.8356, time 159.48ms, mfu 35.23%\n",
            "iter 1110: loss 4.9028, time 158.96ms, mfu 35.30%\n",
            "iter 1120: loss 4.8590, time 159.21ms, mfu 35.35%\n",
            "iter 1130: loss 4.8465, time 160.09ms, mfu 35.38%\n",
            "iter 1140: loss 4.7866, time 160.42ms, mfu 35.40%\n",
            "iter 1150: loss 4.9028, time 159.71ms, mfu 35.43%\n",
            "iter 1160: loss 4.8385, time 159.40ms, mfu 35.47%\n",
            "iter 1170: loss 4.8666, time 158.89ms, mfu 35.51%\n",
            "iter 1180: loss 4.7932, time 159.38ms, mfu 35.54%\n",
            "iter 1190: loss 4.7774, time 159.91ms, mfu 35.56%\n",
            "step 1200: train loss 4.7741, val loss 4.7461\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1200: loss 4.7595, time 24647.31ms, mfu 32.02%\n",
            "iter 1210: loss 4.7241, time 159.82ms, mfu 32.39%\n",
            "iter 1220: loss 4.8469, time 159.64ms, mfu 32.73%\n",
            "iter 1230: loss 4.8212, time 160.32ms, mfu 33.01%\n",
            "iter 1240: loss 4.7148, time 159.84ms, mfu 33.28%\n",
            "iter 1250: loss 4.7800, time 159.10ms, mfu 33.54%\n",
            "iter 1260: loss 4.6979, time 159.42ms, mfu 33.76%\n",
            "iter 1270: loss 4.7242, time 160.24ms, mfu 33.95%\n",
            "iter 1280: loss 4.8569, time 160.51ms, mfu 34.11%\n",
            "iter 1290: loss 4.6738, time 160.33ms, mfu 34.26%\n",
            "iter 1300: loss 4.7328, time 159.57ms, mfu 34.41%\n",
            "iter 1310: loss 4.6631, time 159.44ms, mfu 34.55%\n",
            "iter 1320: loss 4.6817, time 159.96ms, mfu 34.66%\n",
            "iter 1330: loss 4.6336, time 160.38ms, mfu 34.75%\n",
            "iter 1340: loss 4.7153, time 160.50ms, mfu 34.83%\n",
            "iter 1350: loss 4.7314, time 160.20ms, mfu 34.91%\n",
            "iter 1360: loss 4.6471, time 159.03ms, mfu 35.01%\n",
            "iter 1370: loss 4.6626, time 159.21ms, mfu 35.09%\n",
            "iter 1380: loss 4.5594, time 159.61ms, mfu 35.16%\n",
            "iter 1390: loss 4.6328, time 160.06ms, mfu 35.20%\n",
            "iter 1400: loss 4.6690, time 160.24ms, mfu 35.24%\n",
            "iter 1410: loss 4.5082, time 160.27ms, mfu 35.28%\n",
            "iter 1420: loss 4.5821, time 159.40ms, mfu 35.33%\n",
            "iter 1430: loss 4.5430, time 159.23ms, mfu 35.38%\n",
            "iter 1440: loss 4.6622, time 159.03ms, mfu 35.43%\n",
            "iter 1450: loss 4.5930, time 160.09ms, mfu 35.45%\n",
            "iter 1460: loss 4.5764, time 160.38ms, mfu 35.47%\n",
            "iter 1470: loss 4.5010, time 160.49ms, mfu 35.47%\n",
            "iter 1480: loss 4.5572, time 160.81ms, mfu 35.47%\n",
            "iter 1490: loss 4.5655, time 160.19ms, mfu 35.49%\n",
            "step 1500: train loss 4.5717, val loss 4.5465\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1500: loss 4.5962, time 24503.82ms, mfu 31.96%\n",
            "iter 1510: loss 4.4909, time 159.20ms, mfu 32.35%\n",
            "iter 1520: loss 4.5522, time 158.96ms, mfu 32.71%\n",
            "iter 1530: loss 4.6539, time 160.03ms, mfu 33.00%\n",
            "iter 1540: loss 4.5507, time 160.34ms, mfu 33.26%\n",
            "iter 1550: loss 4.4875, time 160.14ms, mfu 33.50%\n",
            "iter 1560: loss 4.4704, time 158.98ms, mfu 33.74%\n",
            "iter 1570: loss 4.5489, time 159.27ms, mfu 33.94%\n",
            "iter 1580: loss 4.4167, time 159.38ms, mfu 34.13%\n",
            "iter 1590: loss 4.4819, time 159.89ms, mfu 34.29%\n",
            "iter 1600: loss 4.6455, time 160.17ms, mfu 34.42%\n",
            "iter 1610: loss 4.4688, time 159.97ms, mfu 34.54%\n",
            "iter 1620: loss 4.5188, time 159.44ms, mfu 34.67%\n",
            "iter 1630: loss 4.5777, time 159.63ms, mfu 34.78%\n",
            "iter 1640: loss 4.4513, time 160.56ms, mfu 34.85%\n",
            "iter 1650: loss 4.4967, time 160.42ms, mfu 34.92%\n",
            "iter 1660: loss 4.5514, time 159.97ms, mfu 35.00%\n",
            "iter 1670: loss 4.4845, time 159.19ms, mfu 35.08%\n",
            "iter 1680: loss 4.3266, time 159.08ms, mfu 35.16%\n",
            "iter 1690: loss 4.4460, time 158.88ms, mfu 35.24%\n",
            "iter 1700: loss 4.4394, time 159.44ms, mfu 35.29%\n",
            "iter 1710: loss 4.3751, time 159.89ms, mfu 35.33%\n",
            "iter 1720: loss 4.4365, time 160.26ms, mfu 35.36%\n",
            "iter 1730: loss 4.5347, time 160.22ms, mfu 35.38%\n",
            "iter 1740: loss 4.4473, time 160.26ms, mfu 35.41%\n",
            "iter 1750: loss 4.5203, time 159.97ms, mfu 35.43%\n",
            "iter 1760: loss 4.4531, time 160.32ms, mfu 35.45%\n",
            "iter 1770: loss 4.5901, time 160.30ms, mfu 35.46%\n",
            "iter 1780: loss 4.4221, time 161.13ms, mfu 35.46%\n",
            "iter 1790: loss 4.4979, time 160.39ms, mfu 35.47%\n",
            "step 1800: train loss 4.4358, val loss 4.4243\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1800: loss 4.5921, time 24651.85ms, mfu 31.95%\n",
            "iter 1810: loss 4.5015, time 160.61ms, mfu 32.30%\n",
            "iter 1820: loss 4.4728, time 159.38ms, mfu 32.65%\n",
            "iter 1830: loss 4.4489, time 159.00ms, mfu 32.98%\n",
            "iter 1840: loss 4.3906, time 159.01ms, mfu 33.27%\n",
            "iter 1850: loss 4.5018, time 159.32ms, mfu 33.52%\n",
            "iter 1860: loss 4.4511, time 159.94ms, mfu 33.74%\n",
            "iter 1870: loss 4.4327, time 160.11ms, mfu 33.93%\n",
            "iter 1880: loss 4.4269, time 160.41ms, mfu 34.09%\n",
            "iter 1890: loss 4.4014, time 159.64ms, mfu 34.26%\n",
            "iter 1900: loss 4.4040, time 158.97ms, mfu 34.42%\n",
            "iter 1910: loss 4.6024, time 159.54ms, mfu 34.55%\n",
            "iter 1920: loss 4.4107, time 160.07ms, mfu 34.66%\n",
            "iter 1930: loss 4.4247, time 160.38ms, mfu 34.76%\n",
            "iter 1940: loss 4.1894, time 160.55ms, mfu 34.83%\n",
            "iter 1950: loss 4.5191, time 159.72ms, mfu 34.92%\n",
            "iter 1960: loss 4.4915, time 158.99ms, mfu 35.02%\n",
            "iter 1970: loss 4.3943, time 159.09ms, mfu 35.10%\n",
            "iter 1980: loss 4.4074, time 159.75ms, mfu 35.17%\n",
            "iter 1990: loss 4.3803, time 159.63ms, mfu 35.22%\n",
            "iter 2000: loss 4.4684, time 160.29ms, mfu 35.26%\n",
            "iter 2010: loss 4.4285, time 160.14ms, mfu 35.30%\n",
            "iter 2020: loss 4.2808, time 160.38ms, mfu 35.33%\n",
            "iter 2030: loss 4.2704, time 160.11ms, mfu 35.36%\n",
            "iter 2040: loss 4.3013, time 161.79ms, mfu 35.35%\n",
            "iter 2050: loss 4.4072, time 161.59ms, mfu 35.34%\n",
            "iter 2060: loss 4.2040, time 160.04ms, mfu 35.38%\n",
            "iter 2070: loss 4.4591, time 160.48ms, mfu 35.39%\n",
            "iter 2080: loss 4.3609, time 160.40ms, mfu 35.41%\n",
            "iter 2090: loss 4.4022, time 158.97ms, mfu 35.46%\n",
            "step 2100: train loss 4.3525, val loss 4.3411\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2100: loss 4.3763, time 24271.25ms, mfu 31.94%\n",
            "iter 2110: loss 4.3727, time 158.65ms, mfu 32.34%\n",
            "iter 2120: loss 4.2367, time 159.42ms, mfu 32.69%\n",
            "iter 2130: loss 4.4296, time 159.87ms, mfu 32.99%\n",
            "iter 2140: loss 4.4671, time 160.31ms, mfu 33.25%\n",
            "iter 2150: loss 4.3579, time 160.06ms, mfu 33.49%\n",
            "iter 2160: loss 4.4261, time 159.39ms, mfu 33.72%\n",
            "iter 2170: loss 4.4320, time 159.36ms, mfu 33.93%\n",
            "iter 2180: loss 4.3139, time 158.79ms, mfu 34.13%\n",
            "iter 2190: loss 4.2813, time 159.21ms, mfu 34.30%\n",
            "iter 2200: loss 4.2657, time 159.15ms, mfu 34.45%\n",
            "iter 2210: loss 4.3506, time 159.96ms, mfu 34.58%\n",
            "iter 2220: loss 4.2336, time 160.41ms, mfu 34.67%\n",
            "iter 2230: loss 4.2920, time 160.19ms, mfu 34.77%\n",
            "iter 2240: loss 4.2945, time 160.18ms, mfu 34.85%\n",
            "iter 2250: loss 4.3022, time 160.00ms, mfu 34.94%\n",
            "iter 2260: loss 4.3042, time 159.38ms, mfu 35.02%\n",
            "iter 2270: loss 4.4005, time 159.10ms, mfu 35.11%\n",
            "iter 2280: loss 4.2786, time 159.63ms, mfu 35.17%\n",
            "iter 2290: loss 4.3604, time 159.20ms, mfu 35.24%\n",
            "iter 2300: loss 4.1306, time 159.20ms, mfu 35.30%\n",
            "iter 2310: loss 4.3683, time 159.90ms, mfu 35.34%\n",
            "iter 2320: loss 4.2766, time 159.88ms, mfu 35.37%\n",
            "iter 2330: loss 4.2657, time 159.94ms, mfu 35.40%\n",
            "iter 2340: loss 4.2788, time 159.50ms, mfu 35.44%\n",
            "iter 2350: loss 4.3771, time 159.16ms, mfu 35.48%\n",
            "iter 2360: loss 4.3171, time 158.95ms, mfu 35.52%\n",
            "iter 2370: loss 4.3063, time 159.93ms, mfu 35.54%\n",
            "iter 2380: loss 4.3162, time 160.95ms, mfu 35.53%\n",
            "iter 2390: loss 4.1931, time 159.77ms, mfu 35.55%\n",
            "step 2400: train loss 4.2600, val loss 4.2384\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2400: loss 4.1830, time 24263.17ms, mfu 32.02%\n",
            "iter 2410: loss 4.3101, time 159.38ms, mfu 32.39%\n",
            "iter 2420: loss 4.3128, time 159.56ms, mfu 32.73%\n",
            "iter 2430: loss 4.2362, time 160.37ms, mfu 33.02%\n",
            "iter 2440: loss 4.2671, time 160.42ms, mfu 33.27%\n",
            "iter 2450: loss 4.2846, time 159.65ms, mfu 33.52%\n",
            "iter 2460: loss 4.2492, time 158.74ms, mfu 33.76%\n",
            "iter 2470: loss 4.2132, time 159.16ms, mfu 33.97%\n",
            "iter 2480: loss 4.2943, time 160.07ms, mfu 34.14%\n",
            "iter 2490: loss 4.1454, time 160.25ms, mfu 34.28%\n",
            "iter 2500: loss 4.1782, time 160.42ms, mfu 34.41%\n",
            "iter 2510: loss 4.1902, time 160.41ms, mfu 34.53%\n",
            "iter 2520: loss 4.2000, time 159.64ms, mfu 34.65%\n",
            "iter 2530: loss 4.2479, time 159.59ms, mfu 34.76%\n",
            "iter 2540: loss 4.1834, time 159.52ms, mfu 34.86%\n",
            "iter 2550: loss 4.3335, time 159.87ms, mfu 34.94%\n",
            "iter 2560: loss 4.2790, time 159.54ms, mfu 35.03%\n",
            "iter 2570: loss 4.1597, time 159.60ms, mfu 35.10%\n",
            "iter 2580: loss 4.2097, time 159.23ms, mfu 35.17%\n",
            "iter 2590: loss 4.2517, time 158.92ms, mfu 35.25%\n",
            "iter 2600: loss 4.0654, time 158.82ms, mfu 35.31%\n",
            "iter 2610: loss 4.3465, time 158.91ms, mfu 35.37%\n",
            "iter 2620: loss 4.2303, time 159.10ms, mfu 35.42%\n",
            "iter 2630: loss 4.2768, time 159.41ms, mfu 35.46%\n",
            "iter 2640: loss 4.1892, time 159.69ms, mfu 35.49%\n",
            "iter 2650: loss 4.2536, time 159.86ms, mfu 35.51%\n",
            "iter 2660: loss 4.1215, time 158.98ms, mfu 35.55%\n",
            "iter 2670: loss 4.1683, time 159.68ms, mfu 35.56%\n",
            "iter 2680: loss 4.1534, time 159.98ms, mfu 35.57%\n",
            "iter 2690: loss 4.3042, time 160.09ms, mfu 35.58%\n",
            "step 2700: train loss 4.1859, val loss 4.1749\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2700: loss 4.2978, time 24221.33ms, mfu 32.05%\n",
            "iter 2710: loss 4.1140, time 159.05ms, mfu 32.43%\n",
            "iter 2720: loss 4.1902, time 159.21ms, mfu 32.77%\n",
            "iter 2730: loss 4.1569, time 159.64ms, mfu 33.07%\n",
            "iter 2740: loss 4.1343, time 160.19ms, mfu 33.32%\n",
            "iter 2750: loss 4.1884, time 160.36ms, mfu 33.55%\n",
            "iter 2760: loss 4.1648, time 159.65ms, mfu 33.77%\n",
            "iter 2770: loss 4.1947, time 158.93ms, mfu 33.98%\n",
            "iter 2780: loss 4.1559, time 158.86ms, mfu 34.17%\n",
            "iter 2790: loss 4.2039, time 159.22ms, mfu 34.34%\n",
            "iter 2800: loss 4.1165, time 159.71ms, mfu 34.48%\n",
            "iter 2810: loss 4.2237, time 159.85ms, mfu 34.60%\n",
            "iter 2820: loss 4.1093, time 160.14ms, mfu 34.70%\n",
            "iter 2830: loss 4.1422, time 159.78ms, mfu 34.80%\n",
            "iter 2840: loss 4.1414, time 159.83ms, mfu 34.89%\n",
            "iter 2850: loss 4.2237, time 159.70ms, mfu 34.98%\n",
            "iter 2860: loss 4.1363, time 159.47ms, mfu 35.06%\n",
            "iter 2870: loss 4.0745, time 159.42ms, mfu 35.13%\n",
            "iter 2880: loss 4.2832, time 159.73ms, mfu 35.19%\n",
            "iter 2890: loss 4.0758, time 160.61ms, mfu 35.22%\n",
            "iter 2900: loss 4.2519, time 160.23ms, mfu 35.26%\n",
            "iter 2910: loss 4.1905, time 160.31ms, mfu 35.30%\n",
            "iter 2920: loss 4.1511, time 159.64ms, mfu 35.34%\n",
            "iter 2930: loss 4.1363, time 159.00ms, mfu 35.39%\n",
            "iter 2940: loss 4.1549, time 158.98ms, mfu 35.44%\n",
            "iter 2950: loss 4.1977, time 159.23ms, mfu 35.48%\n",
            "iter 2960: loss 4.1918, time 159.45ms, mfu 35.51%\n",
            "iter 2970: loss 4.1293, time 160.11ms, mfu 35.53%\n",
            "iter 2980: loss 4.0745, time 160.32ms, mfu 35.53%\n",
            "iter 2990: loss 4.2115, time 160.36ms, mfu 35.54%\n",
            "step 3000: train loss 4.1196, val loss 4.1176\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3000: loss 4.2012, time 24385.05ms, mfu 32.01%\n",
            "iter 3010: loss 4.2226, time 159.52ms, mfu 32.38%\n",
            "iter 3020: loss 4.1124, time 159.54ms, mfu 32.72%\n",
            "iter 3030: loss 3.9776, time 159.79ms, mfu 33.02%\n",
            "iter 3040: loss 4.2078, time 160.13ms, mfu 33.28%\n",
            "iter 3050: loss 4.0873, time 160.38ms, mfu 33.51%\n",
            "iter 3060: loss 3.9918, time 160.04ms, mfu 33.73%\n",
            "iter 3070: loss 4.1066, time 159.01ms, mfu 33.94%\n",
            "iter 3080: loss 4.1030, time 159.12ms, mfu 34.13%\n",
            "iter 3090: loss 4.1271, time 159.48ms, mfu 34.30%\n",
            "iter 3100: loss 4.0851, time 159.27ms, mfu 34.45%\n",
            "iter 3110: loss 4.0203, time 159.45ms, mfu 34.58%\n",
            "iter 3120: loss 4.1835, time 159.26ms, mfu 34.71%\n",
            "iter 3130: loss 3.9814, time 159.61ms, mfu 34.81%\n",
            "iter 3140: loss 4.1184, time 159.54ms, mfu 34.91%\n",
            "iter 3150: loss 4.0795, time 159.56ms, mfu 34.99%\n",
            "iter 3160: loss 4.1413, time 160.02ms, mfu 35.06%\n",
            "iter 3170: loss 4.1642, time 160.71ms, mfu 35.10%\n",
            "iter 3180: loss 4.1760, time 160.57ms, mfu 35.15%\n",
            "iter 3190: loss 4.0747, time 160.00ms, mfu 35.20%\n",
            "iter 3200: loss 3.9918, time 159.92ms, mfu 35.25%\n",
            "iter 3210: loss 4.1052, time 159.91ms, mfu 35.29%\n",
            "iter 3220: loss 4.0239, time 159.44ms, mfu 35.34%\n",
            "iter 3230: loss 4.1206, time 159.06ms, mfu 35.39%\n",
            "iter 3240: loss 4.0267, time 158.96ms, mfu 35.44%\n",
            "iter 3250: loss 4.0879, time 159.21ms, mfu 35.48%\n",
            "iter 3260: loss 4.1077, time 159.72ms, mfu 35.51%\n",
            "iter 3270: loss 4.0257, time 160.50ms, mfu 35.51%\n",
            "iter 3280: loss 4.1386, time 160.44ms, mfu 35.52%\n",
            "iter 3290: loss 3.9895, time 158.92ms, mfu 35.56%\n",
            "step 3300: train loss 4.0715, val loss 4.0568\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3300: loss 4.1646, time 24527.07ms, mfu 32.02%\n",
            "iter 3310: loss 4.1237, time 158.78ms, mfu 32.41%\n",
            "iter 3320: loss 4.0189, time 158.90ms, mfu 32.76%\n",
            "iter 3330: loss 4.1248, time 159.14ms, mfu 33.07%\n",
            "iter 3340: loss 4.1734, time 159.67ms, mfu 33.34%\n",
            "iter 3350: loss 4.1069, time 160.24ms, mfu 33.57%\n",
            "iter 3360: loss 4.0436, time 160.38ms, mfu 33.77%\n",
            "iter 3370: loss 3.9608, time 160.27ms, mfu 33.95%\n",
            "iter 3380: loss 4.1163, time 159.98ms, mfu 34.12%\n",
            "iter 3390: loss 4.0708, time 160.27ms, mfu 34.27%\n",
            "iter 3400: loss 3.9636, time 159.53ms, mfu 34.42%\n",
            "iter 3410: loss 3.9044, time 159.60ms, mfu 34.55%\n",
            "iter 3420: loss 4.1278, time 159.64ms, mfu 34.67%\n",
            "iter 3430: loss 4.0172, time 160.13ms, mfu 34.77%\n",
            "iter 3440: loss 3.9730, time 160.85ms, mfu 34.84%\n",
            "iter 3450: loss 4.0498, time 161.04ms, mfu 34.90%\n",
            "iter 3460: loss 4.1678, time 160.58ms, mfu 34.96%\n",
            "iter 3470: loss 4.0967, time 160.27ms, mfu 35.03%\n",
            "iter 3480: loss 3.9040, time 159.51ms, mfu 35.10%\n",
            "iter 3490: loss 4.0710, time 158.87ms, mfu 35.18%\n",
            "iter 3500: loss 4.0376, time 159.04ms, mfu 35.25%\n",
            "iter 3510: loss 4.1043, time 159.57ms, mfu 35.30%\n",
            "iter 3520: loss 4.0251, time 159.84ms, mfu 35.34%\n",
            "iter 3530: loss 4.0624, time 160.17ms, mfu 35.37%\n",
            "iter 3540: loss 4.0474, time 160.17ms, mfu 35.40%\n",
            "iter 3550: loss 4.0966, time 159.17ms, mfu 35.44%\n",
            "iter 3560: loss 4.0784, time 159.48ms, mfu 35.47%\n",
            "iter 3570: loss 4.1040, time 160.28ms, mfu 35.49%\n",
            "iter 3580: loss 4.0461, time 160.81ms, mfu 35.49%\n",
            "iter 3590: loss 3.9631, time 160.35ms, mfu 35.50%\n",
            "step 3600: train loss 4.0167, val loss 4.0132\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3600: loss 3.8842, time 24686.98ms, mfu 31.97%\n",
            "iter 3610: loss 4.1114, time 159.15ms, mfu 32.36%\n",
            "iter 3620: loss 4.1163, time 159.78ms, mfu 32.69%\n",
            "iter 3630: loss 3.8803, time 160.07ms, mfu 32.99%\n",
            "iter 3640: loss 4.0484, time 160.22ms, mfu 33.25%\n",
            "iter 3650: loss 4.0251, time 160.01ms, mfu 33.49%\n",
            "iter 3660: loss 3.9536, time 159.95ms, mfu 33.71%\n",
            "iter 3670: loss 4.0572, time 159.58ms, mfu 33.91%\n",
            "iter 3680: loss 3.9221, time 160.05ms, mfu 34.09%\n",
            "iter 3690: loss 3.8849, time 160.09ms, mfu 34.24%\n",
            "iter 3700: loss 4.1286, time 160.81ms, mfu 34.37%\n",
            "iter 3710: loss 4.0519, time 160.54ms, mfu 34.48%\n",
            "iter 3720: loss 3.8510, time 160.50ms, mfu 34.59%\n",
            "iter 3730: loss 3.9594, time 159.37ms, mfu 34.71%\n",
            "iter 3740: loss 4.0016, time 159.08ms, mfu 34.83%\n",
            "iter 3750: loss 4.0915, time 159.18ms, mfu 34.93%\n",
            "iter 3760: loss 4.0047, time 159.01ms, mfu 35.03%\n",
            "iter 3770: loss 4.0189, time 159.44ms, mfu 35.10%\n",
            "iter 3780: loss 3.9460, time 160.27ms, mfu 35.15%\n",
            "iter 3790: loss 4.0219, time 160.26ms, mfu 35.20%\n",
            "iter 3800: loss 4.0017, time 160.62ms, mfu 35.23%\n",
            "iter 3810: loss 4.1075, time 159.87ms, mfu 35.28%\n",
            "iter 3820: loss 3.9369, time 159.51ms, mfu 35.33%\n",
            "iter 3830: loss 3.9810, time 159.40ms, mfu 35.37%\n",
            "iter 3840: loss 4.0502, time 160.18ms, mfu 35.40%\n",
            "iter 3850: loss 3.9680, time 160.56ms, mfu 35.41%\n",
            "iter 3860: loss 4.1248, time 159.92ms, mfu 35.44%\n",
            "iter 3870: loss 3.9085, time 159.37ms, mfu 35.47%\n",
            "iter 3880: loss 4.1068, time 159.31ms, mfu 35.51%\n",
            "iter 3890: loss 4.0046, time 161.37ms, mfu 35.49%\n",
            "step 3900: train loss 3.9874, val loss 3.9805\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3900: loss 3.8791, time 24286.07ms, mfu 31.97%\n",
            "iter 3910: loss 3.9342, time 159.57ms, mfu 32.35%\n",
            "iter 3920: loss 3.9260, time 159.19ms, mfu 32.70%\n",
            "iter 3930: loss 3.8285, time 159.33ms, mfu 33.01%\n",
            "iter 3940: loss 3.9107, time 158.90ms, mfu 33.30%\n",
            "iter 3950: loss 3.9430, time 158.98ms, mfu 33.56%\n",
            "iter 3960: loss 3.9911, time 158.94ms, mfu 33.79%\n",
            "iter 3970: loss 3.9404, time 159.12ms, mfu 34.00%\n",
            "iter 3980: loss 3.9421, time 159.41ms, mfu 34.18%\n",
            "iter 3990: loss 3.9819, time 159.94ms, mfu 34.33%\n",
            "iter 4000: loss 3.8600, time 160.17ms, mfu 34.46%\n",
            "iter 4010: loss 3.9515, time 161.21ms, mfu 34.55%\n",
            "iter 4020: loss 4.0068, time 160.52ms, mfu 34.65%\n",
            "iter 4030: loss 3.9411, time 160.73ms, mfu 34.74%\n",
            "iter 4040: loss 4.1237, time 160.57ms, mfu 34.82%\n",
            "iter 4050: loss 3.9957, time 160.48ms, mfu 34.89%\n",
            "iter 4060: loss 3.8761, time 160.04ms, mfu 34.97%\n",
            "iter 4070: loss 4.0161, time 159.58ms, mfu 35.04%\n",
            "iter 4080: loss 4.0651, time 159.20ms, mfu 35.12%\n",
            "iter 4090: loss 3.9518, time 159.85ms, mfu 35.18%\n",
            "iter 4100: loss 3.9513, time 160.70ms, mfu 35.21%\n",
            "iter 4110: loss 3.9611, time 160.07ms, mfu 35.26%\n",
            "iter 4120: loss 4.0270, time 159.05ms, mfu 35.32%\n",
            "iter 4130: loss 4.0139, time 159.18ms, mfu 35.37%\n",
            "iter 4140: loss 3.8781, time 160.05ms, mfu 35.40%\n",
            "iter 4150: loss 3.9291, time 160.49ms, mfu 35.42%\n",
            "iter 4160: loss 4.0156, time 160.41ms, mfu 35.43%\n",
            "iter 4170: loss 4.0385, time 159.41ms, mfu 35.47%\n",
            "iter 4180: loss 3.9566, time 159.10ms, mfu 35.51%\n",
            "iter 4190: loss 3.9435, time 159.91ms, mfu 35.52%\n",
            "step 4200: train loss 3.9584, val loss 3.9560\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4200: loss 4.0239, time 24708.31ms, mfu 31.99%\n",
            "iter 4210: loss 4.0483, time 159.03ms, mfu 32.38%\n",
            "iter 4220: loss 3.9340, time 159.22ms, mfu 32.73%\n",
            "iter 4230: loss 4.0826, time 158.81ms, mfu 33.05%\n",
            "iter 4240: loss 3.9734, time 159.31ms, mfu 33.33%\n",
            "iter 4250: loss 4.0020, time 158.81ms, mfu 33.59%\n",
            "iter 4260: loss 3.9160, time 158.77ms, mfu 33.82%\n",
            "iter 4270: loss 4.0299, time 159.07ms, mfu 34.03%\n",
            "iter 4280: loss 3.8947, time 159.52ms, mfu 34.20%\n",
            "iter 4290: loss 4.0398, time 159.11ms, mfu 34.37%\n",
            "iter 4300: loss 3.9572, time 159.80ms, mfu 34.50%\n",
            "iter 4310: loss 3.7592, time 159.63ms, mfu 34.62%\n",
            "iter 4320: loss 3.9334, time 159.56ms, mfu 34.74%\n",
            "iter 4330: loss 3.8010, time 159.98ms, mfu 34.83%\n",
            "iter 4340: loss 3.9189, time 159.15ms, mfu 34.93%\n",
            "iter 4350: loss 3.9483, time 159.30ms, mfu 35.02%\n",
            "iter 4360: loss 3.9632, time 160.78ms, mfu 35.07%\n",
            "iter 4370: loss 4.0243, time 160.76ms, mfu 35.11%\n",
            "iter 4380: loss 3.8946, time 159.84ms, mfu 35.17%\n",
            "iter 4390: loss 3.8826, time 158.85ms, mfu 35.24%\n",
            "iter 4400: loss 3.9033, time 159.72ms, mfu 35.29%\n",
            "iter 4410: loss 3.9511, time 160.23ms, mfu 35.32%\n",
            "iter 4420: loss 3.9174, time 160.08ms, mfu 35.36%\n",
            "iter 4430: loss 3.7597, time 159.54ms, mfu 35.40%\n",
            "iter 4440: loss 3.9701, time 159.47ms, mfu 35.44%\n",
            "iter 4450: loss 3.9128, time 159.70ms, mfu 35.46%\n",
            "iter 4460: loss 3.8348, time 160.37ms, mfu 35.48%\n",
            "iter 4470: loss 3.9857, time 160.47ms, mfu 35.48%\n",
            "iter 4480: loss 3.9165, time 159.91ms, mfu 35.50%\n",
            "iter 4490: loss 3.9813, time 159.04ms, mfu 35.54%\n",
            "step 4500: train loss 3.9296, val loss 3.9384\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4500: loss 3.9525, time 24510.51ms, mfu 32.01%\n",
            "iter 4510: loss 3.8958, time 159.26ms, mfu 32.39%\n",
            "iter 4520: loss 3.9217, time 159.69ms, mfu 32.73%\n",
            "iter 4530: loss 3.9311, time 159.76ms, mfu 33.03%\n",
            "iter 4540: loss 3.8406, time 160.28ms, mfu 33.28%\n",
            "iter 4550: loss 3.8656, time 160.31ms, mfu 33.51%\n",
            "iter 4560: loss 3.8110, time 160.14ms, mfu 33.73%\n",
            "iter 4570: loss 3.9442, time 159.82ms, mfu 33.92%\n",
            "iter 4580: loss 4.0467, time 159.94ms, mfu 34.10%\n",
            "iter 4590: loss 3.9147, time 159.27ms, mfu 34.27%\n",
            "iter 4600: loss 3.8906, time 158.92ms, mfu 34.43%\n",
            "iter 4610: loss 3.8781, time 158.82ms, mfu 34.58%\n",
            "iter 4620: loss 3.8605, time 159.94ms, mfu 34.69%\n",
            "iter 4630: loss 4.0062, time 160.20ms, mfu 34.79%\n",
            "iter 4640: loss 3.8954, time 160.47ms, mfu 34.86%\n",
            "iter 4650: loss 3.9597, time 159.41ms, mfu 34.96%\n",
            "iter 4660: loss 3.9096, time 159.04ms, mfu 35.05%\n",
            "iter 4670: loss 3.9185, time 159.45ms, mfu 35.12%\n",
            "iter 4680: loss 3.8354, time 159.63ms, mfu 35.18%\n",
            "iter 4690: loss 3.9594, time 159.25ms, mfu 35.25%\n",
            "iter 4700: loss 4.0554, time 159.95ms, mfu 35.29%\n",
            "iter 4710: loss 4.0366, time 158.89ms, mfu 35.35%\n",
            "iter 4720: loss 3.9872, time 158.92ms, mfu 35.41%\n",
            "iter 4730: loss 3.9264, time 159.06ms, mfu 35.45%\n",
            "iter 4740: loss 4.0527, time 160.01ms, mfu 35.48%\n",
            "iter 4750: loss 3.9679, time 160.21ms, mfu 35.49%\n",
            "iter 4760: loss 3.7833, time 160.08ms, mfu 35.50%\n",
            "iter 4770: loss 3.8896, time 159.70ms, mfu 35.53%\n",
            "iter 4780: loss 3.8738, time 159.33ms, mfu 35.56%\n",
            "iter 4790: loss 4.0484, time 159.42ms, mfu 35.58%\n",
            "step 4800: train loss 3.9311, val loss 3.9230\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4800: loss 3.9739, time 24690.95ms, mfu 32.04%\n",
            "iter 4810: loss 3.9787, time 159.04ms, mfu 32.43%\n",
            "iter 4820: loss 3.9255, time 159.28ms, mfu 32.77%\n",
            "iter 4830: loss 3.9357, time 159.12ms, mfu 33.08%\n",
            "iter 4840: loss 3.9176, time 159.97ms, mfu 33.34%\n",
            "iter 4850: loss 3.9307, time 160.34ms, mfu 33.56%\n",
            "iter 4860: loss 3.8082, time 160.09ms, mfu 33.77%\n",
            "iter 4870: loss 3.8487, time 160.14ms, mfu 33.96%\n",
            "iter 4880: loss 3.8728, time 159.78ms, mfu 34.13%\n",
            "iter 4890: loss 3.6999, time 159.10ms, mfu 34.30%\n",
            "iter 4900: loss 3.9299, time 160.63ms, mfu 34.43%\n",
            "iter 4910: loss 3.9479, time 160.86ms, mfu 34.53%\n",
            "iter 4920: loss 3.8750, time 159.82ms, mfu 34.65%\n",
            "iter 4930: loss 3.8432, time 159.26ms, mfu 34.77%\n",
            "iter 4940: loss 3.8818, time 159.77ms, mfu 34.86%\n",
            "iter 4950: loss 4.0244, time 160.26ms, mfu 34.93%\n",
            "iter 4960: loss 3.8790, time 160.56ms, mfu 35.00%\n",
            "iter 4970: loss 3.9291, time 159.13ms, mfu 35.08%\n",
            "iter 4980: loss 3.9724, time 159.05ms, mfu 35.16%\n",
            "iter 4990: loss 3.8798, time 159.06ms, mfu 35.23%\n",
            "iter 5000: loss 3.9229, time 159.51ms, mfu 35.29%\n",
            "iter 5010: loss 3.8713, time 159.91ms, mfu 35.33%\n",
            "iter 5020: loss 3.9643, time 160.11ms, mfu 35.36%\n",
            "iter 5030: loss 3.9335, time 160.29ms, mfu 35.38%\n",
            "iter 5040: loss 3.9876, time 158.98ms, mfu 35.43%\n",
            "iter 5050: loss 3.8073, time 159.18ms, mfu 35.47%\n",
            "iter 5060: loss 3.9631, time 159.28ms, mfu 35.51%\n",
            "iter 5070: loss 3.9320, time 159.63ms, mfu 35.53%\n",
            "iter 5080: loss 3.9202, time 160.77ms, mfu 35.53%\n",
            "iter 5090: loss 3.7677, time 160.24ms, mfu 35.54%\n",
            "step 5100: train loss 3.9194, val loss 3.9239\n",
            "iter 5100: loss 3.8092, time 22445.86ms, mfu 32.01%\n",
            "iter 5110: loss 4.0158, time 160.36ms, mfu 32.37%\n",
            "iter 5120: loss 3.8499, time 160.34ms, mfu 32.69%\n",
            "iter 5130: loss 3.9939, time 160.50ms, mfu 32.97%\n",
            "iter 5140: loss 3.7940, time 160.11ms, mfu 33.24%\n",
            "iter 5150: loss 3.8070, time 159.65ms, mfu 33.49%\n",
            "iter 5160: loss 4.0468, time 159.02ms, mfu 33.73%\n",
            "iter 5170: loss 4.0410, time 159.23ms, mfu 33.94%\n",
            "iter 5180: loss 3.8272, time 159.81ms, mfu 34.12%\n",
            "iter 5190: loss 4.0600, time 160.56ms, mfu 34.26%\n",
            "iter 5200: loss 3.9162, time 160.05ms, mfu 34.40%\n",
            "iter 5210: loss 3.8018, time 159.16ms, mfu 34.54%\n",
            "iter 5220: loss 3.8402, time 159.35ms, mfu 34.67%\n",
            "iter 5230: loss 3.8129, time 159.94ms, mfu 34.77%\n",
            "iter 5240: loss 4.0292, time 160.52ms, mfu 34.85%\n",
            "iter 5250: loss 3.8824, time 160.36ms, mfu 34.92%\n",
            "iter 5260: loss 3.9708, time 159.37ms, mfu 35.01%\n",
            "iter 5270: loss 3.9233, time 158.76ms, mfu 35.10%\n",
            "iter 5280: loss 4.0101, time 159.56ms, mfu 35.17%\n",
            "iter 5290: loss 3.9144, time 160.23ms, mfu 35.21%\n",
            "iter 5300: loss 3.8607, time 160.29ms, mfu 35.25%\n",
            "iter 5310: loss 3.8766, time 159.12ms, mfu 35.31%\n",
            "iter 5320: loss 3.9422, time 159.48ms, mfu 35.36%\n",
            "iter 5330: loss 3.8777, time 159.91ms, mfu 35.39%\n",
            "iter 5340: loss 3.8855, time 160.29ms, mfu 35.41%\n",
            "iter 5350: loss 3.8359, time 160.09ms, mfu 35.43%\n",
            "iter 5360: loss 3.8982, time 159.05ms, mfu 35.48%\n",
            "iter 5370: loss 3.7931, time 159.51ms, mfu 35.51%\n",
            "iter 5380: loss 3.9386, time 160.06ms, mfu 35.52%\n",
            "iter 5390: loss 3.8581, time 160.92ms, mfu 35.52%\n",
            "step 5400: train loss 3.9229, val loss 3.9190\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5400: loss 3.9424, time 24695.83ms, mfu 31.99%\n",
            "iter 5410: loss 3.9401, time 158.25ms, mfu 32.39%\n",
            "iter 5420: loss 3.9992, time 159.49ms, mfu 32.73%\n",
            "iter 5430: loss 4.0043, time 159.09ms, mfu 33.05%\n",
            "iter 5440: loss 3.9625, time 159.54ms, mfu 33.32%\n",
            "iter 5450: loss 3.8837, time 160.19ms, mfu 33.55%\n",
            "iter 5460: loss 3.9909, time 160.32ms, mfu 33.75%\n",
            "iter 5470: loss 3.9187, time 159.82ms, mfu 33.95%\n",
            "iter 5480: loss 4.0512, time 158.88ms, mfu 34.14%\n",
            "iter 5490: loss 3.9572, time 159.58ms, mfu 34.30%\n",
            "iter 5500: loss 3.9269, time 160.31ms, mfu 34.43%\n",
            "iter 5510: loss 3.9005, time 160.40ms, mfu 34.55%\n",
            "iter 5520: loss 3.8317, time 159.97ms, mfu 34.66%\n",
            "iter 5530: loss 3.8649, time 159.13ms, mfu 34.78%\n",
            "iter 5540: loss 3.7661, time 159.38ms, mfu 34.88%\n",
            "iter 5550: loss 3.7963, time 159.70ms, mfu 34.97%\n",
            "iter 5560: loss 3.9709, time 160.25ms, mfu 35.03%\n",
            "iter 5570: loss 3.8540, time 160.45ms, mfu 35.08%\n",
            "iter 5580: loss 3.8937, time 159.88ms, mfu 35.14%\n",
            "iter 5590: loss 3.9445, time 158.90ms, mfu 35.22%\n",
            "iter 5600: loss 3.9327, time 159.41ms, mfu 35.28%\n",
            "iter 5610: loss 3.7907, time 159.80ms, mfu 35.32%\n",
            "iter 5620: loss 4.0203, time 160.42ms, mfu 35.35%\n",
            "iter 5630: loss 3.8378, time 160.13ms, mfu 35.37%\n",
            "iter 5640: loss 3.9715, time 159.49ms, mfu 35.41%\n",
            "iter 5650: loss 3.9681, time 158.79ms, mfu 35.47%\n",
            "iter 5660: loss 3.9312, time 159.58ms, mfu 35.50%\n",
            "iter 5670: loss 3.9935, time 160.11ms, mfu 35.51%\n",
            "iter 5680: loss 3.9124, time 160.54ms, mfu 35.51%\n",
            "iter 5690: loss 3.7669, time 160.27ms, mfu 35.52%\n",
            "step 5700: train loss 3.9170, val loss 3.9056\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5700: loss 4.1087, time 25106.95ms, mfu 31.99%\n",
            "iter 5710: loss 3.9288, time 158.98ms, mfu 32.38%\n",
            "iter 5720: loss 3.8362, time 158.87ms, mfu 32.74%\n",
            "iter 5730: loss 3.8528, time 159.18ms, mfu 33.05%\n",
            "iter 5740: loss 3.9282, time 159.74ms, mfu 33.31%\n",
            "iter 5750: loss 4.0434, time 160.52ms, mfu 33.54%\n",
            "iter 5760: loss 3.8848, time 159.81ms, mfu 33.75%\n",
            "iter 5770: loss 3.7854, time 159.00ms, mfu 33.97%\n",
            "iter 5780: loss 3.7772, time 159.38ms, mfu 34.15%\n",
            "iter 5790: loss 3.8471, time 159.52ms, mfu 34.31%\n",
            "iter 5800: loss 3.8191, time 160.38ms, mfu 34.44%\n",
            "iter 5810: loss 3.9046, time 160.71ms, mfu 34.55%\n",
            "iter 5820: loss 3.9287, time 160.02ms, mfu 34.66%\n",
            "iter 5830: loss 3.7858, time 159.12ms, mfu 34.78%\n",
            "iter 5840: loss 3.8994, time 159.30ms, mfu 34.88%\n",
            "iter 5850: loss 4.0010, time 160.06ms, mfu 34.96%\n",
            "iter 5860: loss 3.9704, time 160.49ms, mfu 35.02%\n",
            "iter 5870: loss 3.9001, time 160.67ms, mfu 35.07%\n",
            "iter 5880: loss 3.9337, time 160.08ms, mfu 35.12%\n",
            "iter 5890: loss 3.9137, time 159.22ms, mfu 35.20%\n",
            "iter 5900: loss 3.9888, time 159.32ms, mfu 35.26%\n",
            "iter 5910: loss 3.9107, time 159.67ms, mfu 35.31%\n",
            "iter 5920: loss 3.9281, time 159.81ms, mfu 35.35%\n",
            "iter 5930: loss 3.7935, time 160.27ms, mfu 35.37%\n",
            "iter 5940: loss 3.7802, time 160.13ms, mfu 35.40%\n",
            "iter 5950: loss 3.9929, time 159.83ms, mfu 35.43%\n",
            "iter 5960: loss 3.9938, time 160.48ms, mfu 35.44%\n",
            "iter 5970: loss 3.9572, time 159.81ms, mfu 35.47%\n",
            "iter 5980: loss 3.9787, time 160.17ms, mfu 35.48%\n",
            "iter 5990: loss 3.9716, time 160.52ms, mfu 35.49%\n",
            "step 6000: train loss 3.9134, val loss 3.9096\n",
            "iter 6000: loss 3.7766, time 22596.45ms, mfu 31.97%\n",
            "iter 6010: loss 4.0079, time 160.41ms, mfu 32.33%\n",
            "iter 6020: loss 3.9009, time 159.69ms, mfu 32.67%\n",
            "iter 6030: loss 3.8590, time 159.17ms, mfu 32.98%\n",
            "iter 6040: loss 3.9545, time 159.64ms, mfu 33.26%\n",
            "iter 6050: loss 3.8498, time 160.50ms, mfu 33.49%\n",
            "iter 6060: loss 4.0238, time 160.34ms, mfu 33.70%\n",
            "iter 6070: loss 3.9724, time 159.58ms, mfu 33.90%\n",
            "iter 6080: loss 3.8996, time 158.87ms, mfu 34.11%\n",
            "iter 6090: loss 3.8584, time 159.22ms, mfu 34.28%\n",
            "iter 6100: loss 3.8341, time 159.58ms, mfu 34.43%\n",
            "iter 6110: loss 3.8486, time 160.36ms, mfu 34.54%\n",
            "iter 6120: loss 3.8572, time 160.40ms, mfu 34.64%\n",
            "iter 6130: loss 3.9000, time 159.79ms, mfu 34.75%\n",
            "iter 6140: loss 3.9419, time 158.85ms, mfu 34.87%\n",
            "iter 6150: loss 3.8104, time 159.90ms, mfu 34.95%\n",
            "iter 6160: loss 3.8615, time 160.55ms, mfu 35.01%\n",
            "iter 6170: loss 3.8538, time 160.50ms, mfu 35.06%\n",
            "iter 6180: loss 3.7685, time 160.26ms, mfu 35.12%\n",
            "iter 6190: loss 3.8083, time 159.00ms, mfu 35.19%\n",
            "iter 6200: loss 3.8835, time 159.52ms, mfu 35.25%\n",
            "iter 6210: loss 3.9872, time 159.14ms, mfu 35.31%\n",
            "iter 6220: loss 3.9956, time 159.07ms, mfu 35.37%\n",
            "iter 6230: loss 4.0744, time 159.19ms, mfu 35.42%\n",
            "iter 6240: loss 3.8975, time 159.07ms, mfu 35.46%\n",
            "iter 6250: loss 3.8256, time 159.00ms, mfu 35.50%\n",
            "iter 6260: loss 3.8743, time 159.03ms, mfu 35.54%\n",
            "iter 6270: loss 3.9991, time 159.31ms, mfu 35.57%\n",
            "iter 6280: loss 3.9585, time 158.91ms, mfu 35.60%\n",
            "iter 6290: loss 3.9945, time 158.87ms, mfu 35.63%\n",
            "step 6300: train loss 3.9121, val loss 3.8984\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 6300: loss 3.8227, time 24494.04ms, mfu 32.09%\n",
            "iter 6310: loss 3.9204, time 160.30ms, mfu 32.44%\n",
            "iter 6320: loss 3.8302, time 160.42ms, mfu 32.76%\n",
            "iter 6330: loss 3.9231, time 159.84ms, mfu 33.05%\n",
            "iter 6340: loss 3.9601, time 159.14ms, mfu 33.33%\n",
            "iter 6350: loss 3.8543, time 159.28ms, mfu 33.58%\n",
            "iter 6360: loss 3.8527, time 159.44ms, mfu 33.80%\n",
            "iter 6370: loss 3.7713, time 159.99ms, mfu 33.99%\n",
            "iter 6380: loss 3.9297, time 160.04ms, mfu 34.15%\n",
            "iter 6390: loss 3.8252, time 160.45ms, mfu 34.29%\n",
            "iter 6400: loss 3.9284, time 159.66ms, mfu 34.44%\n",
            "iter 6410: loss 3.9265, time 158.89ms, mfu 34.59%\n",
            "iter 6420: loss 3.9638, time 159.10ms, mfu 34.71%\n",
            "iter 6430: loss 3.9195, time 159.95ms, mfu 34.81%\n",
            "iter 6440: loss 4.0116, time 160.51ms, mfu 34.88%\n",
            "iter 6450: loss 3.8964, time 160.38ms, mfu 34.95%\n",
            "iter 6460: loss 3.9157, time 160.05ms, mfu 35.02%\n",
            "iter 6470: loss 3.9869, time 160.12ms, mfu 35.08%\n",
            "iter 6480: loss 4.0070, time 159.06ms, mfu 35.16%\n",
            "iter 6490: loss 3.8945, time 159.55ms, mfu 35.22%\n",
            "iter 6500: loss 3.9277, time 159.61ms, mfu 35.28%\n",
            "iter 6510: loss 3.9510, time 159.41ms, mfu 35.33%\n",
            "iter 6520: loss 3.9892, time 159.27ms, mfu 35.38%\n",
            "iter 6530: loss 3.9054, time 159.30ms, mfu 35.42%\n",
            "iter 6540: loss 3.9041, time 158.97ms, mfu 35.47%\n",
            "iter 6550: loss 3.9300, time 159.08ms, mfu 35.51%\n",
            "iter 6560: loss 3.7998, time 159.21ms, mfu 35.54%\n",
            "iter 6570: loss 3.9120, time 159.96ms, mfu 35.55%\n",
            "iter 6580: loss 4.0012, time 160.88ms, mfu 35.55%\n",
            "iter 6590: loss 3.9299, time 160.63ms, mfu 35.54%\n",
            "step 6600: train loss 3.9063, val loss 3.9066\n",
            "iter 6600: loss 4.0412, time 22290.50ms, mfu 32.01%\n",
            "iter 6610: loss 3.9239, time 159.25ms, mfu 32.40%\n",
            "iter 6620: loss 3.8267, time 160.22ms, mfu 32.72%\n",
            "iter 6630: loss 3.9331, time 160.12ms, mfu 33.01%\n",
            "iter 6640: loss 3.8133, time 159.84ms, mfu 33.28%\n",
            "iter 6650: loss 4.0710, time 158.99ms, mfu 33.54%\n",
            "iter 6660: loss 3.8534, time 160.00ms, mfu 33.75%\n",
            "iter 6670: loss 3.9364, time 160.39ms, mfu 33.93%\n",
            "iter 6680: loss 3.9538, time 160.46ms, mfu 34.10%\n",
            "iter 6690: loss 3.9346, time 159.45ms, mfu 34.27%\n",
            "iter 6700: loss 3.8349, time 159.18ms, mfu 34.42%\n",
            "iter 6710: loss 3.9118, time 160.10ms, mfu 34.55%\n",
            "iter 6720: loss 3.8545, time 160.03ms, mfu 34.66%\n",
            "iter 6730: loss 3.8334, time 160.55ms, mfu 34.74%\n",
            "iter 6740: loss 4.0179, time 160.36ms, mfu 34.83%\n",
            "iter 6750: loss 3.9720, time 159.72ms, mfu 34.92%\n",
            "iter 6760: loss 3.8398, time 159.08ms, mfu 35.01%\n",
            "iter 6770: loss 3.9321, time 159.56ms, mfu 35.09%\n",
            "iter 6780: loss 3.9770, time 160.24ms, mfu 35.14%\n",
            "iter 6790: loss 4.0136, time 159.19ms, mfu 35.21%\n",
            "iter 6800: loss 3.7585, time 159.42ms, mfu 35.27%\n",
            "iter 6810: loss 3.9436, time 159.38ms, mfu 35.32%\n",
            "iter 6820: loss 3.9602, time 159.47ms, mfu 35.37%\n",
            "iter 6830: loss 3.8451, time 159.29ms, mfu 35.41%\n",
            "iter 6840: loss 3.8860, time 159.32ms, mfu 35.45%\n",
            "iter 6850: loss 3.9944, time 159.77ms, mfu 35.48%\n",
            "iter 6860: loss 3.8742, time 160.77ms, mfu 35.48%\n",
            "iter 6870: loss 3.9272, time 160.57ms, mfu 35.49%\n",
            "iter 6880: loss 3.8435, time 159.81ms, mfu 35.51%\n",
            "iter 6890: loss 3.9692, time 159.65ms, mfu 35.53%\n",
            "step 6900: train loss 3.9151, val loss 3.9032\n",
            "iter 6900: loss 3.9663, time 22331.08ms, mfu 32.00%\n",
            "iter 6910: loss 3.8438, time 159.71ms, mfu 32.38%\n",
            "iter 6920: loss 3.8325, time 159.52ms, mfu 32.72%\n",
            "iter 6930: loss 3.7234, time 159.36ms, mfu 33.02%\n",
            "iter 6940: loss 3.9087, time 160.20ms, mfu 33.28%\n",
            "iter 6950: loss 3.8149, time 160.24ms, mfu 33.52%\n",
            "iter 6960: loss 3.9191, time 160.33ms, mfu 33.72%\n",
            "iter 6970: loss 3.8546, time 159.86ms, mfu 33.92%\n",
            "iter 6980: loss 3.8480, time 158.96ms, mfu 34.12%\n",
            "iter 6990: loss 3.8836, time 159.51ms, mfu 34.28%\n",
            "iter 7000: loss 3.9366, time 160.37ms, mfu 34.41%\n",
            "iter 7010: loss 3.9208, time 160.01ms, mfu 34.54%\n",
            "iter 7020: loss 3.9178, time 159.75ms, mfu 34.66%\n",
            "iter 7030: loss 4.0303, time 158.76ms, mfu 34.78%\n",
            "iter 7040: loss 3.9273, time 159.36ms, mfu 34.89%\n",
            "iter 7050: loss 3.9921, time 159.58ms, mfu 34.97%\n",
            "iter 7060: loss 3.9267, time 159.94ms, mfu 35.04%\n",
            "iter 7070: loss 3.9194, time 160.16ms, mfu 35.10%\n",
            "iter 7080: loss 3.9140, time 159.11ms, mfu 35.18%\n",
            "iter 7090: loss 3.9499, time 160.05ms, mfu 35.22%\n",
            "iter 7100: loss 4.0401, time 160.04ms, mfu 35.27%\n",
            "iter 7110: loss 4.0060, time 159.74ms, mfu 35.31%\n",
            "iter 7120: loss 3.9443, time 160.59ms, mfu 35.33%\n",
            "iter 7130: loss 3.9032, time 160.74ms, mfu 35.35%\n",
            "iter 7140: loss 3.8875, time 159.74ms, mfu 35.39%\n",
            "iter 7150: loss 3.8816, time 159.17ms, mfu 35.43%\n",
            "iter 7160: loss 3.8137, time 158.89ms, mfu 35.48%\n",
            "iter 7170: loss 3.9039, time 159.65ms, mfu 35.51%\n",
            "iter 7180: loss 3.9048, time 158.92ms, mfu 35.55%\n",
            "iter 7190: loss 3.8987, time 159.95ms, mfu 35.56%\n",
            "step 7200: train loss 3.9082, val loss 3.9099\n",
            "iter 7200: loss 3.8461, time 22308.25ms, mfu 32.03%\n",
            "iter 7210: loss 3.9407, time 159.71ms, mfu 32.40%\n",
            "iter 7220: loss 3.7945, time 160.05ms, mfu 32.72%\n",
            "iter 7230: loss 3.8812, time 159.84ms, mfu 33.02%\n",
            "iter 7240: loss 3.8896, time 159.30ms, mfu 33.30%\n",
            "iter 7250: loss 3.9456, time 160.00ms, mfu 33.54%\n",
            "iter 7260: loss 3.9894, time 160.70ms, mfu 33.73%\n",
            "iter 7270: loss 4.0049, time 160.23ms, mfu 33.92%\n",
            "iter 7280: loss 3.9623, time 158.90ms, mfu 34.12%\n",
            "iter 7290: loss 3.8355, time 159.10ms, mfu 34.30%\n",
            "iter 7300: loss 3.9746, time 159.29ms, mfu 34.45%\n",
            "iter 7310: loss 3.8331, time 160.33ms, mfu 34.56%\n",
            "iter 7320: loss 3.8322, time 160.61ms, mfu 34.66%\n",
            "iter 7330: loss 4.0170, time 160.34ms, mfu 34.75%\n",
            "iter 7340: loss 3.8474, time 159.78ms, mfu 34.85%\n",
            "iter 7350: loss 4.0354, time 158.90ms, mfu 34.95%\n",
            "iter 7360: loss 3.9515, time 159.52ms, mfu 35.03%\n",
            "iter 7370: loss 3.8797, time 159.69ms, mfu 35.10%\n",
            "iter 7380: loss 4.0059, time 159.46ms, mfu 35.17%\n",
            "iter 7390: loss 3.9344, time 159.35ms, mfu 35.24%\n",
            "iter 7400: loss 3.9489, time 159.15ms, mfu 35.30%\n",
            "iter 7410: loss 3.6773, time 158.57ms, mfu 35.37%\n",
            "iter 7420: loss 3.9842, time 159.21ms, mfu 35.41%\n",
            "iter 7430: loss 3.8750, time 160.03ms, mfu 35.44%\n",
            "iter 7440: loss 3.9095, time 160.62ms, mfu 35.45%\n",
            "iter 7450: loss 3.8805, time 160.34ms, mfu 35.46%\n",
            "iter 7460: loss 3.8829, time 159.99ms, mfu 35.48%\n",
            "iter 7470: loss 3.9821, time 159.21ms, mfu 35.52%\n",
            "iter 7480: loss 4.0090, time 159.03ms, mfu 35.55%\n",
            "iter 7490: loss 3.8927, time 159.55ms, mfu 35.57%\n",
            "step 7500: train loss 3.9132, val loss 3.8959\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 7500: loss 3.9053, time 24330.28ms, mfu 32.04%\n",
            "iter 7510: loss 3.9957, time 160.27ms, mfu 32.40%\n",
            "iter 7520: loss 3.8357, time 159.84ms, mfu 32.73%\n",
            "iter 7530: loss 3.9051, time 160.39ms, mfu 33.01%\n",
            "iter 7540: loss 3.7924, time 159.05ms, mfu 33.30%\n",
            "iter 7550: loss 3.8682, time 159.14ms, mfu 33.55%\n",
            "iter 7560: loss 3.9195, time 159.56ms, mfu 33.77%\n",
            "iter 7570: loss 3.9063, time 159.89ms, mfu 33.97%\n",
            "iter 7580: loss 3.8889, time 160.34ms, mfu 34.13%\n",
            "iter 7590: loss 3.9689, time 160.20ms, mfu 34.28%\n",
            "iter 7600: loss 3.8549, time 159.66ms, mfu 34.42%\n",
            "iter 7610: loss 3.9512, time 159.27ms, mfu 34.56%\n",
            "iter 7620: loss 3.9632, time 159.35ms, mfu 34.69%\n",
            "iter 7630: loss 3.9311, time 159.10ms, mfu 34.80%\n",
            "iter 7640: loss 3.9394, time 159.24ms, mfu 34.91%\n",
            "iter 7650: loss 4.0299, time 159.70ms, mfu 34.99%\n",
            "iter 7660: loss 3.9044, time 159.56ms, mfu 35.07%\n",
            "iter 7670: loss 3.9061, time 159.13ms, mfu 35.15%\n",
            "iter 7680: loss 3.9879, time 159.49ms, mfu 35.21%\n",
            "iter 7690: loss 3.8889, time 159.61ms, mfu 35.26%\n",
            "iter 7700: loss 3.9994, time 160.17ms, mfu 35.30%\n",
            "iter 7710: loss 3.9697, time 159.81ms, mfu 35.34%\n",
            "iter 7720: loss 3.9033, time 160.32ms, mfu 35.36%\n",
            "iter 7730: loss 3.7946, time 160.04ms, mfu 35.39%\n",
            "iter 7740: loss 3.8430, time 159.59ms, mfu 35.43%\n",
            "iter 7750: loss 3.9155, time 158.92ms, mfu 35.48%\n",
            "iter 7760: loss 3.8606, time 159.26ms, mfu 35.51%\n",
            "iter 7770: loss 3.7631, time 159.49ms, mfu 35.54%\n",
            "iter 7780: loss 3.9669, time 159.93ms, mfu 35.55%\n",
            "iter 7790: loss 3.9607, time 160.31ms, mfu 35.56%\n",
            "step 7800: train loss 3.9091, val loss 3.8950\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 7800: loss 3.8774, time 24732.40ms, mfu 32.02%\n",
            "iter 7810: loss 3.8664, time 159.41ms, mfu 32.40%\n",
            "iter 7820: loss 3.8523, time 160.04ms, mfu 32.73%\n",
            "iter 7830: loss 3.9041, time 159.61ms, mfu 33.03%\n",
            "iter 7840: loss 3.8820, time 159.04ms, mfu 33.31%\n",
            "iter 7850: loss 3.8895, time 158.96ms, mfu 33.57%\n",
            "iter 7860: loss 3.9424, time 159.50ms, mfu 33.79%\n",
            "iter 7870: loss 3.9021, time 159.83ms, mfu 33.98%\n",
            "iter 7880: loss 3.8064, time 160.27ms, mfu 34.14%\n",
            "iter 7890: loss 3.9444, time 160.36ms, mfu 34.29%\n",
            "iter 7900: loss 3.7889, time 159.79ms, mfu 34.43%\n",
            "iter 7910: loss 3.9197, time 160.04ms, mfu 34.55%\n",
            "iter 7920: loss 3.8676, time 159.83ms, mfu 34.67%\n",
            "iter 7930: loss 3.9340, time 159.70ms, mfu 34.77%\n",
            "iter 7940: loss 3.9541, time 160.03ms, mfu 34.86%\n",
            "iter 7950: loss 3.9292, time 160.04ms, mfu 34.94%\n",
            "iter 7960: loss 3.9389, time 160.58ms, mfu 35.00%\n",
            "iter 7970: loss 3.8190, time 160.48ms, mfu 35.06%\n",
            "iter 7980: loss 3.9337, time 160.15ms, mfu 35.11%\n",
            "iter 7990: loss 3.8479, time 159.75ms, mfu 35.17%\n",
            "iter 8000: loss 3.8572, time 159.15ms, mfu 35.24%\n",
            "iter 8010: loss 3.8017, time 159.12ms, mfu 35.30%\n",
            "iter 8020: loss 3.8172, time 158.87ms, mfu 35.36%\n",
            "iter 8030: loss 3.9587, time 159.14ms, mfu 35.41%\n",
            "iter 8040: loss 4.0344, time 159.89ms, mfu 35.44%\n",
            "iter 8050: loss 3.9642, time 159.97ms, mfu 35.46%\n",
            "iter 8060: loss 3.9015, time 160.36ms, mfu 35.48%\n",
            "iter 8070: loss 3.8980, time 159.99ms, mfu 35.49%\n",
            "iter 8080: loss 3.9914, time 158.97ms, mfu 35.53%\n",
            "iter 8090: loss 3.9346, time 159.54ms, mfu 35.56%\n",
            "step 8100: train loss 3.8993, val loss 3.8966\n",
            "iter 8100: loss 3.8956, time 22483.29ms, mfu 32.03%\n",
            "iter 8110: loss 3.8519, time 160.03ms, mfu 32.39%\n",
            "iter 8120: loss 3.9580, time 159.68ms, mfu 32.72%\n",
            "iter 8130: loss 3.9163, time 158.95ms, mfu 33.04%\n",
            "iter 8140: loss 3.9142, time 159.46ms, mfu 33.32%\n",
            "iter 8150: loss 3.8545, time 160.21ms, mfu 33.55%\n",
            "iter 8160: loss 3.8126, time 160.53ms, mfu 33.75%\n",
            "iter 8170: loss 3.9242, time 160.46ms, mfu 33.93%\n",
            "iter 8180: loss 3.8642, time 160.59ms, mfu 34.09%\n",
            "iter 8190: loss 3.9037, time 160.46ms, mfu 34.23%\n",
            "iter 8200: loss 3.9440, time 160.23ms, mfu 34.37%\n",
            "iter 8210: loss 3.8269, time 160.51ms, mfu 34.49%\n",
            "iter 8220: loss 3.8568, time 160.33ms, mfu 34.60%\n",
            "iter 8230: loss 3.9345, time 160.28ms, mfu 34.70%\n",
            "iter 8240: loss 3.9904, time 160.40ms, mfu 34.79%\n",
            "iter 8250: loss 3.9550, time 159.78ms, mfu 34.88%\n",
            "iter 8260: loss 3.8250, time 159.13ms, mfu 34.98%\n",
            "iter 8270: loss 3.9640, time 159.23ms, mfu 35.06%\n",
            "iter 8280: loss 3.8653, time 159.22ms, mfu 35.14%\n",
            "iter 8290: loss 3.8758, time 159.30ms, mfu 35.21%\n",
            "iter 8300: loss 4.0542, time 159.49ms, mfu 35.27%\n",
            "iter 8310: loss 3.9509, time 160.10ms, mfu 35.30%\n",
            "iter 8320: loss 3.9440, time 160.38ms, mfu 35.33%\n",
            "iter 8330: loss 3.8595, time 160.30ms, mfu 35.36%\n",
            "iter 8340: loss 3.8751, time 159.62ms, mfu 35.40%\n",
            "iter 8350: loss 3.9005, time 158.95ms, mfu 35.45%\n",
            "iter 8360: loss 3.9857, time 159.49ms, mfu 35.48%\n",
            "iter 8370: loss 3.9774, time 159.84ms, mfu 35.50%\n",
            "iter 8380: loss 3.8287, time 160.44ms, mfu 35.51%\n",
            "iter 8390: loss 4.0036, time 159.75ms, mfu 35.53%\n",
            "step 8400: train loss 3.9047, val loss 3.8938\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 8400: loss 3.8230, time 24727.94ms, mfu 32.00%\n",
            "iter 8410: loss 4.0957, time 159.24ms, mfu 32.38%\n",
            "iter 8420: loss 4.0286, time 160.14ms, mfu 32.71%\n",
            "iter 8430: loss 3.9130, time 159.56ms, mfu 33.01%\n",
            "iter 8440: loss 3.8672, time 159.94ms, mfu 33.28%\n",
            "iter 8450: loss 3.8738, time 159.36ms, mfu 33.53%\n",
            "iter 8460: loss 3.8719, time 159.56ms, mfu 33.75%\n",
            "iter 8470: loss 3.8012, time 159.44ms, mfu 33.96%\n",
            "iter 8480: loss 3.7613, time 159.67ms, mfu 34.14%\n",
            "iter 8490: loss 3.8585, time 159.71ms, mfu 34.29%\n",
            "iter 8500: loss 3.9026, time 159.63ms, mfu 34.44%\n",
            "iter 8510: loss 3.8605, time 160.23ms, mfu 34.56%\n",
            "iter 8520: loss 3.9526, time 160.32ms, mfu 34.66%\n",
            "iter 8530: loss 4.0246, time 160.38ms, mfu 34.75%\n",
            "iter 8540: loss 3.9091, time 160.24ms, mfu 34.84%\n",
            "iter 8550: loss 4.0049, time 159.98ms, mfu 34.92%\n",
            "iter 8560: loss 3.9016, time 159.48ms, mfu 35.01%\n",
            "iter 8570: loss 3.8289, time 158.83ms, mfu 35.10%\n",
            "iter 8580: loss 4.0184, time 159.07ms, mfu 35.17%\n",
            "iter 8590: loss 3.9032, time 159.29ms, mfu 35.24%\n",
            "iter 8600: loss 3.8643, time 158.78ms, mfu 35.31%\n",
            "iter 8610: loss 3.8962, time 159.81ms, mfu 35.35%\n",
            "iter 8620: loss 4.0139, time 160.16ms, mfu 35.38%\n",
            "iter 8630: loss 3.8403, time 160.37ms, mfu 35.40%\n",
            "iter 8640: loss 3.9929, time 159.15ms, mfu 35.44%\n",
            "iter 8650: loss 3.9709, time 159.41ms, mfu 35.48%\n",
            "iter 8660: loss 3.8226, time 159.93ms, mfu 35.50%\n",
            "iter 8670: loss 3.9018, time 160.46ms, mfu 35.50%\n",
            "iter 8680: loss 3.8494, time 160.31ms, mfu 35.51%\n",
            "iter 8690: loss 3.9106, time 160.05ms, mfu 35.53%\n",
            "step 8700: train loss 3.9040, val loss 3.8973\n",
            "iter 8700: loss 3.8340, time 22472.61ms, mfu 32.00%\n",
            "iter 8710: loss 3.8361, time 159.77ms, mfu 32.37%\n",
            "iter 8720: loss 3.9104, time 159.27ms, mfu 32.72%\n",
            "iter 8730: loss 3.9529, time 159.13ms, mfu 33.03%\n",
            "iter 8740: loss 3.9074, time 159.50ms, mfu 33.30%\n",
            "iter 8750: loss 4.0017, time 159.74ms, mfu 33.55%\n",
            "iter 8760: loss 4.0218, time 159.33ms, mfu 33.77%\n",
            "iter 8770: loss 3.8628, time 159.22ms, mfu 33.98%\n",
            "iter 8780: loss 3.9146, time 158.98ms, mfu 34.17%\n",
            "iter 8790: loss 3.8745, time 159.70ms, mfu 34.33%\n",
            "iter 8800: loss 3.7854, time 159.25ms, mfu 34.48%\n",
            "iter 8810: loss 3.9160, time 159.76ms, mfu 34.60%\n",
            "iter 8820: loss 3.9517, time 159.94ms, mfu 34.71%\n",
            "iter 8830: loss 3.8984, time 159.20ms, mfu 34.82%\n",
            "iter 8840: loss 3.8071, time 160.02ms, mfu 34.90%\n",
            "iter 8850: loss 3.8020, time 160.42ms, mfu 34.97%\n",
            "iter 8860: loss 3.9491, time 160.26ms, mfu 35.03%\n",
            "iter 8870: loss 3.9344, time 159.69ms, mfu 35.10%\n",
            "iter 8880: loss 3.8824, time 159.12ms, mfu 35.18%\n",
            "iter 8890: loss 3.8487, time 159.34ms, mfu 35.24%\n",
            "iter 8900: loss 3.8331, time 160.04ms, mfu 35.28%\n",
            "iter 8910: loss 3.9477, time 160.67ms, mfu 35.31%\n",
            "iter 8920: loss 3.8930, time 160.06ms, mfu 35.34%\n",
            "iter 8930: loss 3.8126, time 159.23ms, mfu 35.39%\n",
            "iter 8940: loss 3.9229, time 159.94ms, mfu 35.42%\n",
            "iter 8950: loss 3.8846, time 160.76ms, mfu 35.43%\n",
            "iter 8960: loss 3.8745, time 160.45ms, mfu 35.44%\n",
            "iter 8970: loss 4.0195, time 159.59ms, mfu 35.47%\n",
            "iter 8980: loss 3.8582, time 159.40ms, mfu 35.50%\n",
            "iter 8990: loss 4.0601, time 160.10ms, mfu 35.52%\n",
            "step 9000: train loss 3.8952, val loss 3.8947\n",
            "iter 9000: loss 3.9875, time 22602.12ms, mfu 31.99%\n",
            "iter 9010: loss 3.9639, time 160.08ms, mfu 32.36%\n",
            "iter 9020: loss 3.8030, time 160.49ms, mfu 32.68%\n",
            "iter 9030: loss 3.7695, time 160.64ms, mfu 32.96%\n",
            "iter 9040: loss 3.8797, time 160.32ms, mfu 33.22%\n",
            "iter 9050: loss 4.0219, time 160.55ms, mfu 33.45%\n",
            "iter 9060: loss 3.9508, time 160.42ms, mfu 33.67%\n",
            "iter 9070: loss 3.8304, time 159.67ms, mfu 33.87%\n",
            "iter 9080: loss 3.8801, time 159.40ms, mfu 34.07%\n",
            "iter 9090: loss 3.8531, time 159.70ms, mfu 34.23%\n",
            "iter 9100: loss 3.9108, time 160.40ms, mfu 34.37%\n",
            "iter 9110: loss 3.9532, time 160.41ms, mfu 34.49%\n",
            "iter 9120: loss 3.8166, time 160.35ms, mfu 34.60%\n",
            "iter 9130: loss 3.8456, time 160.13ms, mfu 34.70%\n",
            "iter 9140: loss 3.8078, time 158.94ms, mfu 34.82%\n",
            "iter 9150: loss 3.9969, time 159.68ms, mfu 34.91%\n",
            "iter 9160: loss 3.9210, time 159.48ms, mfu 35.00%\n",
            "iter 9170: loss 3.9786, time 160.11ms, mfu 35.06%\n",
            "iter 9180: loss 3.8188, time 160.35ms, mfu 35.11%\n",
            "iter 9190: loss 3.9122, time 159.98ms, mfu 35.17%\n",
            "iter 9200: loss 3.9102, time 158.75ms, mfu 35.25%\n",
            "iter 9210: loss 3.8280, time 159.98ms, mfu 35.29%\n",
            "iter 9220: loss 3.8128, time 160.36ms, mfu 35.32%\n",
            "iter 9230: loss 3.9427, time 160.28ms, mfu 35.35%\n",
            "iter 9240: loss 3.8694, time 159.48ms, mfu 35.39%\n",
            "iter 9250: loss 3.8301, time 158.61ms, mfu 35.45%\n",
            "iter 9260: loss 3.9059, time 160.20ms, mfu 35.47%\n",
            "iter 9270: loss 3.9333, time 160.31ms, mfu 35.48%\n",
            "iter 9280: loss 3.8694, time 160.06ms, mfu 35.50%\n",
            "iter 9290: loss 3.9670, time 159.17ms, mfu 35.53%\n",
            "step 9300: train loss 3.9038, val loss 3.8914\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9300: loss 3.9345, time 24481.24ms, mfu 32.00%\n",
            "iter 9310: loss 3.9484, time 159.99ms, mfu 32.37%\n",
            "iter 9320: loss 4.0463, time 160.07ms, mfu 32.69%\n",
            "iter 9330: loss 3.9329, time 159.75ms, mfu 33.00%\n",
            "iter 9340: loss 3.9508, time 159.95ms, mfu 33.26%\n",
            "iter 9350: loss 3.7819, time 160.46ms, mfu 33.49%\n",
            "iter 9360: loss 3.9349, time 160.45ms, mfu 33.70%\n",
            "iter 9370: loss 3.9394, time 160.53ms, mfu 33.89%\n",
            "iter 9380: loss 3.9285, time 160.54ms, mfu 34.05%\n",
            "iter 9390: loss 3.7373, time 160.31ms, mfu 34.21%\n",
            "iter 9400: loss 3.9886, time 160.58ms, mfu 34.34%\n",
            "iter 9410: loss 3.9487, time 160.72ms, mfu 34.45%\n",
            "iter 9420: loss 3.9073, time 159.97ms, mfu 34.58%\n",
            "iter 9430: loss 4.1061, time 159.12ms, mfu 34.70%\n",
            "iter 9440: loss 3.9533, time 159.09ms, mfu 34.82%\n",
            "iter 9450: loss 3.9050, time 159.64ms, mfu 34.91%\n",
            "iter 9460: loss 3.8810, time 160.46ms, mfu 34.98%\n",
            "iter 9470: loss 3.7947, time 160.33ms, mfu 35.04%\n",
            "iter 9480: loss 3.9404, time 159.87ms, mfu 35.10%\n",
            "iter 9490: loss 3.7964, time 159.05ms, mfu 35.18%\n",
            "iter 9500: loss 3.9543, time 159.07ms, mfu 35.25%\n",
            "iter 9510: loss 3.9092, time 159.07ms, mfu 35.31%\n",
            "iter 9520: loss 3.8046, time 159.62ms, mfu 35.36%\n",
            "iter 9530: loss 4.0386, time 159.99ms, mfu 35.39%\n",
            "iter 9540: loss 3.8092, time 159.07ms, mfu 35.43%\n",
            "iter 9550: loss 3.9046, time 159.23ms, mfu 35.47%\n",
            "iter 9560: loss 3.9343, time 159.02ms, mfu 35.52%\n",
            "iter 9570: loss 3.9589, time 160.55ms, mfu 35.52%\n",
            "iter 9580: loss 3.9307, time 160.05ms, mfu 35.53%\n",
            "iter 9590: loss 3.9339, time 159.45ms, mfu 35.56%\n",
            "step 9600: train loss 3.8972, val loss 3.8901\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9600: loss 3.9776, time 24732.73ms, mfu 32.02%\n",
            "iter 9610: loss 3.8513, time 159.90ms, mfu 32.39%\n",
            "iter 9620: loss 3.8458, time 160.05ms, mfu 32.72%\n",
            "iter 9630: loss 3.8957, time 160.31ms, mfu 33.00%\n",
            "iter 9640: loss 3.8957, time 160.33ms, mfu 33.26%\n",
            "iter 9650: loss 3.8879, time 160.55ms, mfu 33.49%\n",
            "iter 9660: loss 4.0033, time 160.26ms, mfu 33.70%\n",
            "iter 9670: loss 3.8978, time 159.33ms, mfu 33.91%\n",
            "iter 9680: loss 3.8756, time 160.03ms, mfu 34.09%\n",
            "iter 9690: loss 3.8344, time 159.90ms, mfu 34.25%\n",
            "iter 9700: loss 3.8250, time 160.11ms, mfu 34.39%\n",
            "iter 9710: loss 3.9784, time 159.20ms, mfu 34.53%\n",
            "iter 9720: loss 3.8490, time 159.25ms, mfu 34.66%\n",
            "iter 9730: loss 4.0405, time 159.34ms, mfu 34.78%\n",
            "iter 9740: loss 3.8540, time 160.27ms, mfu 34.86%\n",
            "iter 9750: loss 3.8918, time 160.24ms, mfu 34.93%\n",
            "iter 9760: loss 3.8540, time 160.06ms, mfu 35.00%\n",
            "iter 9770: loss 3.9788, time 160.25ms, mfu 35.06%\n",
            "iter 9780: loss 3.8921, time 160.13ms, mfu 35.12%\n",
            "iter 9790: loss 3.8682, time 159.26ms, mfu 35.19%\n",
            "iter 9800: loss 3.8996, time 158.92ms, mfu 35.26%\n",
            "iter 9810: loss 3.8841, time 158.89ms, mfu 35.33%\n",
            "iter 9820: loss 3.8789, time 159.23ms, mfu 35.38%\n",
            "iter 9830: loss 3.8280, time 160.13ms, mfu 35.40%\n",
            "iter 9840: loss 3.8669, time 160.47ms, mfu 35.42%\n",
            "iter 9850: loss 4.0621, time 159.93ms, mfu 35.45%\n",
            "iter 9860: loss 3.9403, time 159.40ms, mfu 35.48%\n",
            "iter 9870: loss 3.9313, time 159.11ms, mfu 35.52%\n",
            "iter 9880: loss 3.7525, time 159.42ms, mfu 35.55%\n",
            "iter 9890: loss 3.9098, time 160.03ms, mfu 35.56%\n",
            "step 9900: train loss 3.8913, val loss 3.8977\n",
            "iter 9900: loss 3.8847, time 22518.98ms, mfu 32.03%\n",
            "iter 9910: loss 3.9343, time 159.34ms, mfu 32.40%\n",
            "iter 9920: loss 3.9695, time 159.84ms, mfu 32.73%\n",
            "iter 9930: loss 3.7876, time 159.87ms, mfu 33.03%\n",
            "iter 9940: loss 3.7985, time 160.38ms, mfu 33.28%\n",
            "iter 9950: loss 3.8511, time 160.40ms, mfu 33.51%\n",
            "iter 9960: loss 3.9532, time 160.36ms, mfu 33.72%\n",
            "iter 9970: loss 3.9783, time 159.84ms, mfu 33.92%\n",
            "iter 9980: loss 3.9834, time 159.45ms, mfu 34.10%\n",
            "iter 9990: loss 3.9436, time 159.01ms, mfu 34.28%\n",
            "iter 10000: loss 3.8185, time 159.52ms, mfu 34.43%\n"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9KXf5Xygjwi"
      },
      "outputs": [],
      "source": [
        "# start = \"How to join a tbank-research?\"\n",
        "# start = \"How to join a tbank-research?\"\n",
        "start = \"It's snow.\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA6vDeMCgjwi",
        "outputId": "832380d2-df49-4035-ffcd-4002f7830a6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-9f0241acb2d4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1X5T2Aygjwi",
        "outputId": "a7e0c9e6-0239-41d5-a71e-2c8790b15e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's snow.\n",
            "\n",
            "One of the last few months we've been so lucky to have it. We're very lucky to have it on a ship that's still an island, so I had to live there long enough.\n",
            "\n",
            "But for the sake of life I'm not sure it will happen. I know I'm going to visit my husband, and he's a great friend, but I'll have to wait for him to leave.\n",
            "\n",
            "I've been so lucky to have it on a ship that we all went to visit our ship. I've met my father for the last few days. He's the husband of a man who is an active member of the ship and has been there since my age. I'm not sure what I'm going to tell.\n",
            "\n",
            "This is the second mission I've met. That is what the sea has been. The last, part of the sea has been there in the past year, but it's actually one of the most important missions to come.\n",
            "\n",
            "For a long time I've just met my father and raised my heart. He never was there.\n",
            "\n",
            "In the last few months, for a long time, the captain has told me what he has done. He has been here since the morning of the day when we have had it.\n",
            "\n",
            "He has been there since his age, but since then he has now moved to the sea.\n",
            "\n",
            "I know this is how he brings out his personal life. I know of the importance of time and effort. I can't blame you for the time and effort.\n",
            "\n",
            "I know I've lost control of me. I know who I am. I've lost control of myself. I know that I lost control of myself.\n",
            "\n",
            "So I know that I had my entire life. I've finally lost control of myself. I've lost control of myself. I've been there, and I've lost control of myself. Because I've lost control of myself.\n",
            "\n",
            "I know that my own life has been there since my age, but it is a little bit of an effort to do that.\n",
            "\n",
            "I know that I have a lost control of myself and that I can get my life. I am on that side of this. I will always be able to recover to keep my life alive. I am my own.\n",
            "\n",
            "I don't know what I'm supposed to be. I'm the only one. I have lost control of myself. I have\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"We have a lot of snow that is at my disposal. I like to go to the end of this week, and I'm very excited to be able to bring it back to the final day of the week,\" said Jeff Scott, a former city council member for the Mayor's office.\n",
            "\n",
            "\"It was a major, really friendly day for us, but we all knew it was a great day for us and it was really good to get in and be able to get a better feeling of the game,\" Scott said.\n",
            "\n",
            "The mayor's office has more than 60,000 signatures.\n",
            "\n",
            "In a report back in mid-March, the city council has also been holding a meeting of the council's council.\n",
            "\n",
            "\"We've been meeting people who brought this community to the next stage,\" said Scott.<|endoftext|>This week, we're getting more heated about: the future of smartphones, the iPhone, and the next big update. The new update will include how smart it will be if you want it and how it will be for you.\n",
            "\n",
            "These are good news for you, but I think that is some of the early news and the most critical of Windows users. (Just be warned.)\n",
            "\n",
            "When you're planning to start the update, let's start by with your friends.\n",
            "\n",
            "By signing the new update, you will need to do it properly.\n",
            "\n",
            "The update will send you some feedback and suggestions.\n",
            "\n",
            "And if you want to share a new update, let me know in the comments below.\n",
            "\n",
            "That said, there are the issues with your phone when you're planning to update.\n",
            "\n",
            "We do have a few updates to it and we will not be going any further into our discussions.\n",
            "\n",
            "When we have a new update that we are going to update on the latest update and we will be updating that and then update it to the game.\n",
            "\n",
            "If you want to add a new update to our new update, we'll support the new update as soon as we are out.\n",
            "\n",
            "If you want to take the update, we'll have some fresh comments from the launchers.\n",
            "\n",
            "I'm going to update that at the end of the day, we'll update it after launch.\n",
            "\n",
            "We'll be adding updates in the next update, so let's discuss with your group.\n",
            "\n",
            "A few changes will be added, so we've added several new fixes in the plans.\n",
            "\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"It's the same thing with the snow. It's snow, snow. It's the same thing with snow. It's the same thing, you're the one with the same thing with snow.\"\n",
            "\n",
            "The snow has gone up to the snow and snow.\n",
            "\n",
            "The snow, ice and snow appear to be around.\n",
            "\n",
            "After snow appears to be a hot spot.\n",
            "\n",
            "The snow can't handle any more rain.\n",
            "\n",
            "\"It's bad if you don't, it's a good thing and it's a good thing.\"\n",
            "\n",
            "The snow has gone up to the snow so it can't handle any more rain.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "It has been up to the snow and snow and snow as snow has been up to the snow.\n",
            "\n",
            "The snow is still in the snow and snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow, so that snow can't handle any more rain.\n",
            "\n",
            "As I've mentioned above, snow has gone up to the snow.\n",
            "\n",
            "They've gone up to the snow.\n",
            "\n",
            "The snow is not an ice-free snow, so that snow is a good thing.\n",
            "\n",
            "The snow is still in the snow and snow can't handle any more rain.\n",
            "\n",
            "The snow has gone up to the snow and snow has gone up to the snow, so the snow will be up to the snow.\n",
            "\n",
            "It's the snow that snow has gone up to the snow.\n",
            "\n",
            "There are no snow with snow.\n",
            "\n",
            "The snow is in the snow and snow has gone up to the snow.\n",
            "\n",
            "The snow is still in the snow and snow has gone up to the snow.\n",
            "\n",
            "The snow is still in the snow and snow has gone up to the snow.\n",
            "\n",
            "There are no snow but snow is still in the snow.\n",
            "\n",
            "I'm sorry this was actually caused.\n",
            "\n",
            "The snow and snow is still in the snow and snow has gone up to the snow and snow has gone down to the snow.\n",
            "\n",
            "It has gone up to the snow.\n",
            "\n",
            "A snow with snow on and snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The snow has gone up to the snow.\n",
            "\n",
            "The\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "His camera video, which had the first camera video camera camera in the last few days, has already been featured in this video.\n",
            "\n",
            "He also has an all-time favourite photographer.\n",
            "\n",
            "It's an interesting picture.\n",
            "\n",
            "(Chris Clark, a photographer behind the 3D) is with his crew.\n",
            "\n",
            "(L.L.L.L.L.L.L.L.L.L.L.)L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.V.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L.L\n",
            "---------------\n",
            "It's snow. Now, the first day for me was the most amazing morning of night.\n",
            "\n",
            "\"It was a big night. It was a really intense night. It was terrible and we kept eating it. I got used to it and it was amazing,\" he said. \"That's the day of the day.\" He had tried to beat and began to get his house back up.\n",
            "\n",
            "\"I'm very proud of it so much.\"\n",
            "\n",
            "\"It's our own first day, but it's pretty amazing. It was amazing and I wasn't really really nervous. It was a lot of warm and great. It was really incredible. That's really incredible. It was fantastic and it was incredible. I was really proud of it so much. It was amazing. It was amazing. I was really excited to have this week's. I really enjoyed it and I wanted to show off the other guys it was amazing. It was wonderful. It was amazing. I was really excited to have this week.\"\n",
            "\n",
            "\"We called it a day of the day of the week. It's really amazing. It was really great and amazing. It was amazing and it was amazing. It was amazing and I really enjoyed it. It was amazing. It really was amazing. It was amazing. It was great. It was amazing. It was awesome. It was amazing. We were really excited to have this week's. It was amazing. It was great. It was amazing. It was amazing. It was incredible. It was amazing. It was amazing. It was amazing and it was amazing. It was amazing. It was amazing. It was amazing. It was amazing. It was amazing. It was incredible. It was amazing. It was amazing. It's amazing. It was amazing. It was amazing. It was amazing. It was amazing. It's amazing. I thought it was amazing. It was amazing. It was great. It was incredible. It was amazing. It was amazing. It was great. It was incredible. It was amazing. It was unbelievable. It was amazing. It was awesome. It was amazing. It was amazing. It was amazing. It was amazing. This was amazing. It was amazing. It was amazing. It was amazing! It was amazing. It was amazing. It was amazing. It was amazing and it was amazing. It was fantastic. It was amazing. It was unbelievable. It was amazing and beautiful. It was\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qB152SRMgjwj"
      },
      "outputs": [],
      "source": [
        "start = \"How to join a tbank-research?\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkmxDtzjgjwj",
        "outputId": "f6553e3a-4634-4e4c-dac5-90b81d3b5ec5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to join a tbank-research?\n",
            "\n",
            "Yes, that is. It is. It is. It is. It is. Since the beginning of the late 20th century the world is a far more distant future. But what we go is a great and we do it every day. We have a great time. We are a far more distant future. And we have a great time. We can’t wait for the next few years.\n",
            "\n",
            "To talk about the future of the world is for a moment. And then a moment of uncertainty in the present, we know that there will be no sooner than tomorrow. We have an idea of why we are now all we need to know. We have a vision for the future. We have to be clear that we have a vision for the future. We have to be clear that we have a vision for the future. We have a vision for potential future. We have a vision for the future. We have an idea of the future. We have a vision for the future. We have a vision for the future. We have a vision for us. We have a vision for the future. We have a vision for the future. We have an vision for the future. We have nothing but vision for other worlds. We have a vision for our future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. We have a vision for the future. Our vision means a vision for the future. We have a vision for the future. Our vision is in the future. We have a vision for the future. We will be open to future. We are ready, we have a vision for the future. We have a vision for the future. There will be no sooner than tomorrow. We have a vision for the future. Our vision for the future. We have an idea for the future. We have a vision for future. We have a vision for the future. We want to be clear. We have a vision for future. We have a vision for future. We need a vision for the future. We need a vision for future. We need a vision for 2018. We have a vision for future. We need a vision for future. We need a vision for the future. We need a vision for future. We need\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "If you had a good time, and you were on a bumble a bit more than a couple of days away, the process of getting to know the data was going to be interesting. The most interesting part about the process is a complete mess. The first step of the process was to get a good idea of what a tbank-research solution that I call “bumble” to go to:\n",
            "\n",
            "What if a tbank-research solution that we call “bumble” on a tbank-research solution, that we’ll get to know the exact same thing. And next step is to get a good idea of how the whole process is going to work. And then there’s a solution that’s likely to go through the process.\n",
            "\n",
            "OK, we’ve got a big deal on Tbank-research, which is extremely small. Once you’ve got a good idea of how to manage it, it would be interesting, and we’re going to have to get our goals going and it’s pretty good. But you know what is going on, and when you get a good idea of what you’re going to do, that information is going to look very important.\n",
            "\n",
            "In the meantime, we’ve got a really big deal on the tbank-research solution. But this is happening in the nongovernmental market. We’re going to have to take advantage of the data, and then it’s going to be with the tbank-research solution.\n",
            "\n",
            "That’s it.\n",
            "\n",
            "Q: Do any of that?\n",
            "\n",
            "Well, I think we’re going to see a lot of the data, and we’re going to come up with a few different data. So the data that’s going to be related to the Tbank-research solution where you’re going to need a lot of work. And then there’s going to be kind of a step up to make sure that we’re making really informed decisions based on the data, because we’re going to have the data that’s going to put into the data, and that’s obviously a step up to make sure we’re dealing with the data itself. And then it’s going to be very clear if we can make sure that\n",
            "---------------\n",
            "How to join a tbank-research? You've got a good idea!\n",
            "\n",
            "If you're a local student, what are your thoughts on what's in your mind?<|endoftext|>A number of New Englanders are doing a little differently.\n",
            "\n",
            "The national average of 3.2 million respondents is 7 to 8 percent, the same as Americans, who are planning to increase their productivity or reduce economic growth to about 50 percent.\n",
            "\n",
            "The survey is a part of the public figure that appears to be more confident at first than those in the economy.\n",
            "\n",
            "Here's what the poll looked like. You can guess what's up to you, just look at the poll itself.\n",
            "\n",
            "For instance, the number of people in the U.S. has more than doubled to a factor in the job growth period.\n",
            "\n",
            "Roughly half of the population in the U.S. is still the largest group in the country, and the number in the US has increased to about 1.1 percent in the same period.\n",
            "\n",
            "The report comes as the first quarter of the survey released on Monday that tracked how the government had managed to grow 35 percent and the number in the U.S.\n",
            "\n",
            "A survey of 7.1 million people in the U.S. was conducted in the U.S. and 10.6 million with the highest population in the U.S.\n",
            "\n",
            "The survey is a part of the survey conducted on September 19 – so far, it's clear that the survey is up close to the charts.<|endoftext|>This article is from the archive of all the wonderful facts about the best things we do.\n",
            "\n",
            "It was written at a British government radio show last night on the Internet right after it was broadcast.\n",
            "\n",
            "This morning, our friend was at the top of the news conference with him, and we have no idea what was going to happen. We’re in a different world, and we have no idea what was going on.\n",
            "\n",
            "We can’t help with the fact that the people who knew that they knew the worst things they have to watch, and the people who knew that was in the air. If they had told us that they were going to have a bigger world than they were, they didn’t know what was going on, and we still need to take a picture like that.\n",
            "\n",
            "The main thing they have heard is that we have a huge audience, and that’s where they were, and that means\n",
            "---------------\n",
            "How to join a tbank-research? Our goal is to provide a more efficient and secure, reliable, and sustainable environment, to help build roads and provide education and education. Our goal depends on the way opportunities work. Our goal is to deploy jobs and make those communities more productive.\n",
            "\n",
            "In this interview we often discuss the current situation. We also discuss the specific issues of training in schools, public education, and the need to be careful in an environment where students can learn and provide access to education directly from their schools. The school programs are a key component of the new project. We also talk about the proposed funding method and how it would be implemented to support children whose primary goals are to improve their literacy. We also discuss new aspects of how schools would evolve, and what should be more important in the classroom.\n",
            "\n",
            "We are also discussing how funding a school/school program can be used to be completed. We want to explore important issues. We also discuss how the funding process can be used in schools. Our goal is to increase the number of students who are not eligible for the schools. We are working to make sure that teachers can participate in the program as a way to understand the needs of students.\n",
            "\n",
            "One important aspect of this new project is the decision-making process. We can make sure that teachers can receive funding from the faculty in order to prepare them for the new project. We would also discuss ways to ensure that we have a sustainable economy and also to help families grow families.\n",
            "\n",
            "We have also asked students to join the project. We are working to develop a policy approach to helping students learn and will help them learn and help them learn.\n",
            "\n",
            "Our goal is to make sure that we serve teachers who want to do what they want to help them learn and to help them learn and to help. Our goal is to make sure that their teachers and teachers are safe to start a project. And we are also working to make sure that we have a sustainable economy that is not sustainable. Our goal is to create jobs and make sure that we build it better than it would be.\n",
            "\n",
            "We also want to help members learn and do their homework as a way to help them learn. We want to help the kids learn and to help them learn and to help them learn and to help them learn and to help them learn and to help them learn and to help them learn and to help them learn.\n",
            "\n",
            "We want to make sure that we have a sustainable economy and need to build a\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "Anastro-sherson@sherson.com<|endoftext|>Kurnehana-based social networking giant\n",
            "\n",
            "The group's first open-source startup developer, which brings together an interactive, interactive and web-based platform.\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "I believe that all modern analytics is a good idea to create a web-based platform. But I believe that analytics is the best way to quickly build that platform. It's a little bit more complex.\n",
            "\n",
            "\"It's a great tool to be able to start developing this platform,\" she said.\n",
            "\n",
            "Kurnehana-based web-based companies are continuing a variety of ways to reach a wide range of platform designs.\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "In Kurnehana-based platform, Kurnehana-based social networking giant, has created a series of fast-growing companies that have run the web-based platform, including Kurnehana-based social networking giant JLA.\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based software company TK\n",
            "\n",
            "Kurnehana-based web-based software company COO\n",
            "\n",
            "Kurnehana-based social networking giant Yoga-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based mobile platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based platform\n",
            "\n",
            "Kurnehana-based web-based\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsfrhQkpUCYN"
      },
      "source": [
        "## GPT: Reflex-Router-Attention (5)\n",
        "\n",
        "**2 layer: SA for 6 head**\n",
        "\n",
        "**3-6 layer: Reflex-Router-Attention for 6 head for all previous hidden states: fitting linear combination**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCUmirWwUCYO"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "\n",
        "class Router(nn.Module):\n",
        "  def __init__(self, n_head, i_layer):\n",
        "    super().__init__()\n",
        "    self.weights = nn.ParameterDict({str(i): nn.Parameter(torch.randn(i_layer)) for i in range(n_head)}) #.to(device) #cuda\n",
        "\n",
        "  def forward(self, k_vs, head_i):\n",
        "    return sum(weight * k_v for weight, k_v in zip(self.weights[str(head_i)], k_vs))\n",
        "\n",
        "\n",
        "class ReflexRouterAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.router = Router(config.n_head, i_layer + 1)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        #i_hidden_state:\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        #reflex!\n",
        "        if self.model_type == 'simple' or (self.model_type == 'reflex' and hidden_states.__len__() < 3):\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v,\n",
        "                                                                 attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        else:\n",
        "            attn = []\n",
        "            for i in range(self.n_head):\n",
        "              # for i's head\n",
        "              q_i, k_i, v_i = q[:, i,:, :], self.router([hidden_state[0][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i), self.router([hidden_state[1][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i)\n",
        "              attn.append(torch.nn.functional.scaled_dot_product_attention(q_i, k_i, v_i, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "              del q_i, k_i, v_i\n",
        "            y = torch.cat(attn, dim=1)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y, (k, v)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = ReflexRouterAttention(config, i_layer)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        attn_kv = self.attn(self.ln_1(x), hidden_states)\n",
        "        x = x + attn_kv[0] #y for 0\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_kv[1] #hidden_states_i for k,v for 1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config, _) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        hidden_states = []\n",
        "\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x, hidden_state = block(x, hidden_states)\n",
        "\n",
        "            # level2: for router\n",
        "            if self.config.model_type == 'reflex':\n",
        "                hidden_states.append(hidden_state)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "x2_URC8OUCYP",
        "outputId": "d4f6a958-1467-47ae-9075-0bfb361ae3aa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ],
            "text/plain": [
              "__main__.GPT"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Osc7TQbeUCYP"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'reflex' # or 'simple'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agk1BOteUCYP",
        "outputId": "89a2a9ff-8942-4c2d-a2ba-124350906d6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3hGyBDlUCYP"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'reflexAttnGPT_with_router={config}1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ni7f6kSUCYP"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/reglex_attn_GPT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKT_vVi3UCYQ",
        "outputId": "dddcd425-ac78-419c-cc4e-bc1a8eddd0a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 32,768\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx2Z9EGuUCYQ"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRptvPNbUCYQ",
        "outputId": "6a26cb2b-69d1-4c11-f618-047cd4d51554"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfLeyVpmUCYQ",
        "outputId": "fd5ab468-ed85-4b98-ba63-0053a6ffa2e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 49, with 10,110 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "66is5pkxUCYR",
        "outputId": "8be9cd16-f4fa-42c1-ff95-96b59344470e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m206spv\u001b[0m (\u001b[33m206spv-central-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241125_020351-0efgcb4k</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/0efgcb4k' target=\"_blank\">reflexAttnGPT_with_router=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/0efgcb4k' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/0efgcb4k</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-11-0f0ed152afa8> line 153 \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:10.090000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-11-0f0ed152afa8> line 100 \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.020000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-11-0f0ed152afa8> line 17 \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.100000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-11-0f0ed152afa8> line 45 \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.605000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-11-0f0ed152afa8> line 84 \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:11.972000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-11-0f0ed152afa8> line 26 \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 02:04:12.134000 6393 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 10.9932, val loss 10.9934\n",
            "iter 0: loss 10.9969, time 50475.59ms, mfu -100.00%\n",
            "iter 10: loss 9.9123, time 280.19ms, mfu 20.36%\n",
            "iter 20: loss 9.2266, time 280.14ms, mfu 20.36%\n",
            "iter 30: loss 8.6969, time 280.29ms, mfu 20.36%\n",
            "iter 40: loss 7.9985, time 279.70ms, mfu 20.37%\n",
            "iter 50: loss 7.4439, time 280.25ms, mfu 20.37%\n",
            "iter 60: loss 7.3035, time 281.11ms, mfu 20.36%\n",
            "iter 70: loss 7.1692, time 280.56ms, mfu 20.36%\n",
            "iter 80: loss 6.7882, time 279.97ms, mfu 20.36%\n",
            "iter 90: loss 6.8603, time 280.23ms, mfu 20.36%\n",
            "iter 100: loss 6.6895, time 280.95ms, mfu 20.36%\n",
            "iter 110: loss 6.5602, time 280.57ms, mfu 20.35%\n",
            "iter 120: loss 6.6441, time 280.26ms, mfu 20.35%\n",
            "iter 130: loss 6.3946, time 280.04ms, mfu 20.36%\n",
            "iter 140: loss 6.4340, time 281.01ms, mfu 20.35%\n",
            "iter 150: loss 6.4244, time 280.88ms, mfu 20.35%\n",
            "iter 160: loss 6.4920, time 279.73ms, mfu 20.35%\n",
            "iter 170: loss 6.3158, time 279.98ms, mfu 20.36%\n",
            "iter 180: loss 6.2929, time 281.51ms, mfu 20.35%\n",
            "iter 190: loss 6.3015, time 281.24ms, mfu 20.34%\n",
            "iter 200: loss 6.1616, time 279.82ms, mfu 20.35%\n",
            "iter 210: loss 6.1580, time 279.97ms, mfu 20.35%\n",
            "iter 220: loss 6.2040, time 281.17ms, mfu 20.34%\n",
            "iter 230: loss 6.1673, time 280.64ms, mfu 20.34%\n",
            "iter 240: loss 6.2252, time 280.02ms, mfu 20.35%\n",
            "iter 250: loss 6.1752, time 280.49ms, mfu 20.35%\n",
            "iter 260: loss 6.1559, time 281.21ms, mfu 20.34%\n",
            "iter 270: loss 5.9983, time 318.59ms, mfu 20.10%\n",
            "iter 280: loss 6.0872, time 281.17ms, mfu 20.12%\n",
            "iter 290: loss 5.9992, time 281.06ms, mfu 20.14%\n",
            "step 300: train loss 6.0393, val loss 5.9935\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 300: loss 6.1076, time 36797.03ms, mfu 18.14%\n",
            "iter 310: loss 6.1170, time 280.58ms, mfu 18.36%\n",
            "iter 320: loss 6.0684, time 281.58ms, mfu 18.55%\n",
            "iter 330: loss 6.3164, time 280.37ms, mfu 18.73%\n",
            "iter 340: loss 5.9447, time 279.79ms, mfu 18.89%\n",
            "iter 350: loss 6.0055, time 281.34ms, mfu 19.03%\n",
            "iter 360: loss 5.8723, time 280.24ms, mfu 19.17%\n",
            "iter 370: loss 5.9717, time 279.36ms, mfu 19.29%\n",
            "iter 380: loss 5.9727, time 281.31ms, mfu 19.39%\n",
            "iter 390: loss 6.0097, time 280.85ms, mfu 19.48%\n",
            "iter 400: loss 5.9378, time 280.89ms, mfu 19.57%\n",
            "iter 410: loss 6.0343, time 280.07ms, mfu 19.65%\n",
            "iter 420: loss 5.8558, time 280.10ms, mfu 19.72%\n",
            "iter 430: loss 5.7785, time 281.28ms, mfu 19.78%\n",
            "iter 440: loss 5.9131, time 280.95ms, mfu 19.83%\n",
            "iter 450: loss 5.8399, time 279.87ms, mfu 19.89%\n",
            "iter 460: loss 5.7060, time 280.61ms, mfu 19.93%\n",
            "iter 470: loss 5.9145, time 281.13ms, mfu 19.97%\n",
            "iter 480: loss 5.6704, time 280.57ms, mfu 20.00%\n",
            "iter 490: loss 5.8978, time 279.70ms, mfu 20.04%\n",
            "iter 500: loss 5.8154, time 281.08ms, mfu 20.07%\n",
            "iter 510: loss 5.7125, time 280.28ms, mfu 20.10%\n",
            "iter 520: loss 5.7543, time 279.97ms, mfu 20.13%\n",
            "iter 530: loss 5.5551, time 281.27ms, mfu 20.14%\n",
            "iter 540: loss 5.8258, time 281.06ms, mfu 20.16%\n",
            "iter 550: loss 5.7300, time 280.24ms, mfu 20.18%\n",
            "iter 560: loss 5.6277, time 280.16ms, mfu 20.20%\n",
            "iter 570: loss 5.6874, time 281.35ms, mfu 20.21%\n",
            "iter 580: loss 5.6967, time 281.90ms, mfu 20.21%\n",
            "iter 590: loss 5.5752, time 279.70ms, mfu 20.23%\n",
            "step 600: train loss 5.5452, val loss 5.5014\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 600: loss 5.4313, time 37659.35ms, mfu 18.22%\n",
            "iter 610: loss 5.4547, time 279.99ms, mfu 18.44%\n",
            "iter 620: loss 5.5861, time 280.69ms, mfu 18.63%\n",
            "iter 630: loss 5.2936, time 281.67ms, mfu 18.79%\n",
            "iter 640: loss 5.6656, time 280.18ms, mfu 18.95%\n",
            "iter 650: loss 5.4319, time 280.10ms, mfu 19.09%\n",
            "iter 660: loss 5.4455, time 281.12ms, mfu 19.21%\n",
            "iter 670: loss 5.4416, time 280.99ms, mfu 19.32%\n",
            "iter 680: loss 5.3347, time 280.24ms, mfu 19.42%\n",
            "iter 690: loss 5.4472, time 280.22ms, mfu 19.52%\n",
            "iter 700: loss 5.3871, time 281.30ms, mfu 19.59%\n",
            "iter 710: loss 5.5342, time 281.55ms, mfu 19.66%\n",
            "iter 720: loss 5.3418, time 280.80ms, mfu 19.73%\n",
            "iter 730: loss 5.4652, time 280.54ms, mfu 19.79%\n",
            "iter 740: loss 5.2890, time 280.50ms, mfu 19.84%\n",
            "iter 750: loss 5.3374, time 280.62ms, mfu 19.89%\n",
            "iter 760: loss 5.3479, time 279.92ms, mfu 19.94%\n",
            "iter 770: loss 5.3616, time 280.36ms, mfu 19.98%\n",
            "iter 780: loss 5.1008, time 281.24ms, mfu 20.01%\n",
            "iter 790: loss 5.2122, time 280.74ms, mfu 20.04%\n",
            "iter 800: loss 5.2790, time 280.03ms, mfu 20.08%\n",
            "iter 810: loss 5.2582, time 281.36ms, mfu 20.10%\n",
            "iter 820: loss 5.2408, time 280.94ms, mfu 20.12%\n",
            "iter 830: loss 5.2700, time 279.73ms, mfu 20.15%\n",
            "iter 840: loss 5.2353, time 280.55ms, mfu 20.17%\n",
            "iter 850: loss 5.2367, time 281.08ms, mfu 20.18%\n",
            "iter 860: loss 5.0694, time 280.56ms, mfu 20.20%\n",
            "iter 870: loss 5.1435, time 280.33ms, mfu 20.21%\n",
            "iter 880: loss 5.2382, time 280.43ms, mfu 20.22%\n",
            "iter 890: loss 5.1913, time 281.16ms, mfu 20.23%\n",
            "step 900: train loss 5.1625, val loss 5.1185\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 900: loss 5.1469, time 37367.62ms, mfu 18.22%\n",
            "iter 910: loss 5.0659, time 279.00ms, mfu 18.45%\n",
            "iter 920: loss 5.1033, time 281.24ms, mfu 18.63%\n",
            "iter 930: loss 5.2536, time 280.55ms, mfu 18.80%\n",
            "iter 940: loss 5.2152, time 279.85ms, mfu 18.96%\n",
            "iter 950: loss 5.0802, time 282.25ms, mfu 19.09%\n",
            "iter 960: loss 5.2284, time 280.65ms, mfu 19.21%\n",
            "iter 970: loss 5.0054, time 280.21ms, mfu 19.33%\n",
            "iter 980: loss 5.1821, time 280.04ms, mfu 19.43%\n",
            "iter 990: loss 5.0980, time 281.42ms, mfu 19.52%\n",
            "iter 1000: loss 5.1335, time 281.32ms, mfu 19.59%\n",
            "iter 1010: loss 4.8460, time 280.15ms, mfu 19.67%\n",
            "iter 1020: loss 5.0412, time 280.47ms, mfu 19.74%\n",
            "iter 1030: loss 5.1045, time 281.81ms, mfu 19.79%\n",
            "iter 1040: loss 5.0729, time 280.37ms, mfu 19.84%\n",
            "iter 1050: loss 5.0462, time 280.13ms, mfu 19.90%\n",
            "iter 1060: loss 5.0748, time 279.76ms, mfu 19.95%\n",
            "iter 1070: loss 5.1187, time 280.22ms, mfu 19.99%\n",
            "iter 1080: loss 5.0121, time 280.15ms, mfu 20.03%\n",
            "iter 1090: loss 4.9558, time 281.10ms, mfu 20.05%\n",
            "iter 1100: loss 4.9063, time 281.29ms, mfu 20.08%\n",
            "iter 1110: loss 5.1955, time 280.95ms, mfu 20.10%\n",
            "iter 1120: loss 5.0223, time 280.19ms, mfu 20.13%\n",
            "iter 1130: loss 4.9310, time 280.31ms, mfu 20.15%\n",
            "iter 1140: loss 4.7990, time 280.86ms, mfu 20.17%\n",
            "iter 1150: loss 5.0000, time 281.15ms, mfu 20.18%\n",
            "iter 1160: loss 4.9279, time 279.88ms, mfu 20.20%\n",
            "iter 1170: loss 5.1264, time 279.76ms, mfu 20.22%\n",
            "iter 1180: loss 4.8148, time 281.19ms, mfu 20.23%\n",
            "iter 1190: loss 4.8909, time 280.10ms, mfu 20.24%\n",
            "step 1200: train loss 4.9281, val loss 4.8927\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1200: loss 4.8028, time 37466.47ms, mfu 18.23%\n",
            "iter 1210: loss 4.9378, time 280.37ms, mfu 18.44%\n",
            "iter 1220: loss 4.9638, time 281.17ms, mfu 18.63%\n",
            "iter 1230: loss 4.8420, time 281.29ms, mfu 18.79%\n",
            "iter 1240: loss 4.8584, time 280.57ms, mfu 18.95%\n",
            "iter 1250: loss 4.9386, time 280.16ms, mfu 19.09%\n",
            "iter 1260: loss 4.8728, time 280.96ms, mfu 19.21%\n",
            "iter 1270: loss 4.8459, time 281.46ms, mfu 19.32%\n",
            "iter 1280: loss 4.8072, time 281.12ms, mfu 19.42%\n",
            "iter 1290: loss 4.8186, time 279.89ms, mfu 19.51%\n",
            "iter 1300: loss 4.9294, time 281.13ms, mfu 19.59%\n",
            "iter 1310: loss 4.7079, time 280.99ms, mfu 19.66%\n",
            "iter 1320: loss 4.8646, time 278.83ms, mfu 19.74%\n",
            "iter 1330: loss 4.7964, time 280.06ms, mfu 19.81%\n",
            "iter 1340: loss 4.7644, time 281.51ms, mfu 19.85%\n",
            "iter 1350: loss 4.7963, time 280.94ms, mfu 19.90%\n",
            "iter 1360: loss 4.7395, time 279.73ms, mfu 19.95%\n",
            "iter 1370: loss 4.7278, time 280.99ms, mfu 19.98%\n",
            "iter 1380: loss 4.8068, time 281.18ms, mfu 20.01%\n",
            "iter 1390: loss 4.7847, time 280.93ms, mfu 20.04%\n",
            "iter 1400: loss 4.8429, time 279.93ms, mfu 20.08%\n",
            "iter 1410: loss 4.7925, time 279.92ms, mfu 20.11%\n",
            "iter 1420: loss 4.5550, time 279.85ms, mfu 20.14%\n",
            "iter 1430: loss 4.6656, time 280.06ms, mfu 20.16%\n",
            "iter 1440: loss 4.7792, time 280.64ms, mfu 20.18%\n",
            "iter 1450: loss 4.6672, time 281.16ms, mfu 20.19%\n",
            "iter 1460: loss 4.7434, time 281.13ms, mfu 20.20%\n",
            "iter 1470: loss 4.6942, time 280.20ms, mfu 20.22%\n",
            "iter 1480: loss 4.7613, time 280.21ms, mfu 20.23%\n",
            "iter 1490: loss 4.7261, time 280.07ms, mfu 20.25%\n",
            "step 1500: train loss 4.6915, val loss 4.6642\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1500: loss 4.6858, time 36994.44ms, mfu 18.24%\n",
            "iter 1510: loss 4.6113, time 281.19ms, mfu 18.44%\n",
            "iter 1520: loss 4.7303, time 279.92ms, mfu 18.64%\n",
            "iter 1530: loss 4.6624, time 281.65ms, mfu 18.80%\n",
            "iter 1540: loss 4.7672, time 281.43ms, mfu 18.95%\n",
            "iter 1550: loss 4.6382, time 281.01ms, mfu 19.08%\n",
            "iter 1560: loss 4.5579, time 280.47ms, mfu 19.21%\n",
            "iter 1570: loss 4.6706, time 279.74ms, mfu 19.33%\n",
            "iter 1580: loss 4.4640, time 281.14ms, mfu 19.42%\n",
            "iter 1590: loss 4.6881, time 281.42ms, mfu 19.51%\n",
            "iter 1600: loss 4.7893, time 280.37ms, mfu 19.59%\n",
            "iter 1610: loss 4.5517, time 279.95ms, mfu 19.67%\n",
            "iter 1620: loss 4.6173, time 281.12ms, mfu 19.73%\n",
            "iter 1630: loss 4.6759, time 280.84ms, mfu 19.79%\n",
            "iter 1640: loss 4.5868, time 280.54ms, mfu 19.85%\n",
            "iter 1650: loss 4.5120, time 280.30ms, mfu 19.90%\n",
            "iter 1660: loss 4.5956, time 281.52ms, mfu 19.94%\n",
            "iter 1670: loss 4.6751, time 280.46ms, mfu 19.98%\n",
            "iter 1680: loss 4.4021, time 279.55ms, mfu 20.02%\n",
            "iter 1690: loss 4.7315, time 280.92ms, mfu 20.05%\n",
            "iter 1700: loss 4.5038, time 280.92ms, mfu 20.08%\n",
            "iter 1710: loss 4.6233, time 280.43ms, mfu 20.10%\n",
            "iter 1720: loss 4.6453, time 279.89ms, mfu 20.13%\n",
            "iter 1730: loss 4.6386, time 281.54ms, mfu 20.14%\n",
            "iter 1740: loss 4.5329, time 281.24ms, mfu 20.16%\n",
            "iter 1750: loss 4.6345, time 280.33ms, mfu 20.18%\n",
            "iter 1760: loss 4.5885, time 279.98ms, mfu 20.20%\n",
            "iter 1770: loss 4.5863, time 279.93ms, mfu 20.22%\n",
            "iter 1780: loss 4.4828, time 279.85ms, mfu 20.23%\n",
            "iter 1790: loss 4.4798, time 279.82ms, mfu 20.25%\n",
            "step 1800: train loss 4.5446, val loss 4.5333\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1800: loss 4.5348, time 37280.76ms, mfu 18.24%\n",
            "iter 1810: loss 4.5088, time 281.01ms, mfu 18.45%\n",
            "iter 1820: loss 4.4657, time 280.55ms, mfu 18.64%\n",
            "iter 1830: loss 4.6347, time 279.94ms, mfu 18.81%\n",
            "iter 1840: loss 4.5835, time 281.45ms, mfu 18.96%\n",
            "iter 1850: loss 4.5422, time 281.68ms, mfu 19.09%\n",
            "iter 1860: loss 4.5841, time 279.86ms, mfu 19.22%\n",
            "iter 1870: loss 4.6088, time 279.96ms, mfu 19.33%\n",
            "iter 1880: loss 4.5004, time 280.00ms, mfu 19.44%\n",
            "iter 1890: loss 4.5359, time 280.32ms, mfu 19.53%\n",
            "iter 1900: loss 4.5186, time 281.51ms, mfu 19.60%\n",
            "iter 1910: loss 4.5217, time 280.97ms, mfu 19.67%\n",
            "iter 1920: loss 4.4879, time 280.10ms, mfu 19.74%\n",
            "iter 1930: loss 4.3604, time 279.76ms, mfu 19.81%\n",
            "iter 1940: loss 4.4787, time 281.66ms, mfu 19.85%\n",
            "iter 1950: loss 4.5312, time 281.28ms, mfu 19.90%\n",
            "iter 1960: loss 4.5505, time 279.84ms, mfu 19.95%\n",
            "iter 1970: loss 4.5097, time 280.85ms, mfu 19.98%\n",
            "iter 1980: loss 4.6215, time 281.35ms, mfu 20.01%\n",
            "iter 1990: loss 4.5649, time 280.39ms, mfu 20.05%\n",
            "iter 2000: loss 4.5273, time 279.95ms, mfu 20.08%\n",
            "iter 2010: loss 4.4693, time 281.37ms, mfu 20.10%\n",
            "iter 2020: loss 4.5280, time 282.93ms, mfu 20.11%\n",
            "iter 2030: loss 4.3613, time 279.69ms, mfu 20.14%\n",
            "iter 2040: loss 4.4501, time 280.78ms, mfu 20.15%\n",
            "iter 2050: loss 4.5421, time 281.44ms, mfu 20.17%\n",
            "iter 2060: loss 4.4243, time 280.71ms, mfu 20.18%\n",
            "iter 2070: loss 4.5685, time 279.92ms, mfu 20.20%\n",
            "iter 2080: loss 4.4272, time 280.49ms, mfu 20.22%\n",
            "iter 2090: loss 4.4458, time 281.41ms, mfu 20.22%\n",
            "step 2100: train loss 4.4520, val loss 4.4389\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2100: loss 4.4291, time 37264.41ms, mfu 18.22%\n",
            "iter 2110: loss 4.3310, time 279.98ms, mfu 18.43%\n",
            "iter 2120: loss 4.4565, time 281.05ms, mfu 18.62%\n",
            "iter 2130: loss 4.4926, time 280.93ms, mfu 18.79%\n",
            "iter 2140: loss 4.3724, time 279.65ms, mfu 18.95%\n",
            "iter 2150: loss 4.5536, time 280.16ms, mfu 19.09%\n",
            "iter 2160: loss 4.2501, time 281.45ms, mfu 19.21%\n",
            "iter 2170: loss 4.4706, time 280.33ms, mfu 19.32%\n",
            "iter 2180: loss 4.4491, time 279.94ms, mfu 19.43%\n",
            "iter 2190: loss 4.3845, time 280.23ms, mfu 19.52%\n",
            "iter 2200: loss 4.5166, time 281.54ms, mfu 19.60%\n",
            "iter 2210: loss 4.3748, time 280.86ms, mfu 19.67%\n",
            "iter 2220: loss 4.4656, time 281.70ms, mfu 19.73%\n",
            "iter 2230: loss 4.4137, time 281.25ms, mfu 19.78%\n",
            "iter 2240: loss 4.3262, time 280.53ms, mfu 19.84%\n",
            "iter 2250: loss 4.3975, time 280.13ms, mfu 19.89%\n",
            "iter 2260: loss 4.3560, time 280.15ms, mfu 19.94%\n",
            "iter 2270: loss 4.3468, time 281.61ms, mfu 19.97%\n",
            "iter 2280: loss 4.3841, time 281.20ms, mfu 20.00%\n",
            "iter 2290: loss 4.5617, time 279.87ms, mfu 20.04%\n",
            "iter 2300: loss 4.4287, time 280.42ms, mfu 20.07%\n",
            "iter 2310: loss 4.4001, time 281.47ms, mfu 20.09%\n",
            "iter 2320: loss 4.4653, time 280.74ms, mfu 20.12%\n",
            "iter 2330: loss 4.3001, time 279.84ms, mfu 20.14%\n",
            "iter 2340: loss 4.3694, time 281.22ms, mfu 20.16%\n",
            "iter 2350: loss 4.3194, time 281.12ms, mfu 20.17%\n",
            "iter 2360: loss 4.3363, time 280.19ms, mfu 20.19%\n",
            "iter 2370: loss 4.3232, time 280.16ms, mfu 20.21%\n",
            "iter 2380: loss 4.3687, time 282.52ms, mfu 20.21%\n",
            "iter 2390: loss 4.4911, time 280.38ms, mfu 20.22%\n",
            "step 2400: train loss 4.3652, val loss 4.3414\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2400: loss 4.4090, time 36972.15ms, mfu 18.22%\n",
            "iter 2410: loss 4.3437, time 281.02ms, mfu 18.42%\n",
            "iter 2420: loss 4.2623, time 280.35ms, mfu 18.62%\n",
            "iter 2430: loss 4.2900, time 280.09ms, mfu 18.79%\n",
            "iter 2440: loss 4.3490, time 281.48ms, mfu 18.94%\n",
            "iter 2450: loss 4.4907, time 281.12ms, mfu 19.08%\n",
            "iter 2460: loss 4.3745, time 280.81ms, mfu 19.20%\n",
            "iter 2470: loss 4.2524, time 280.08ms, mfu 19.32%\n",
            "iter 2480: loss 4.3210, time 281.52ms, mfu 19.41%\n",
            "iter 2490: loss 4.4234, time 280.45ms, mfu 19.51%\n",
            "iter 2500: loss 4.2984, time 280.21ms, mfu 19.59%\n",
            "iter 2510: loss 4.3804, time 281.38ms, mfu 19.66%\n",
            "iter 2520: loss 4.4116, time 281.02ms, mfu 19.72%\n",
            "iter 2530: loss 4.2499, time 279.78ms, mfu 19.79%\n",
            "iter 2540: loss 4.2597, time 280.98ms, mfu 19.84%\n",
            "iter 2550: loss 4.2964, time 280.94ms, mfu 19.89%\n",
            "iter 2560: loss 4.3537, time 280.98ms, mfu 19.93%\n",
            "iter 2570: loss 4.2599, time 280.67ms, mfu 19.97%\n",
            "iter 2580: loss 4.3690, time 280.95ms, mfu 20.01%\n",
            "iter 2590: loss 4.3631, time 280.20ms, mfu 20.04%\n",
            "iter 2600: loss 4.4223, time 279.43ms, mfu 20.08%\n",
            "iter 2610: loss 4.2717, time 280.18ms, mfu 20.11%\n",
            "iter 2620: loss 4.3451, time 281.48ms, mfu 20.12%\n",
            "iter 2630: loss 4.3955, time 280.62ms, mfu 20.14%\n",
            "iter 2640: loss 4.2282, time 279.80ms, mfu 20.17%\n",
            "iter 2650: loss 4.2683, time 281.02ms, mfu 20.18%\n",
            "iter 2660: loss 4.3723, time 281.12ms, mfu 20.19%\n",
            "iter 2670: loss 4.4355, time 280.33ms, mfu 20.21%\n",
            "iter 2680: loss 4.3075, time 279.78ms, mfu 20.23%\n",
            "iter 2690: loss 4.2238, time 281.39ms, mfu 20.23%\n",
            "step 2700: train loss 4.2793, val loss 4.2717\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2700: loss 4.3245, time 37293.45ms, mfu 18.23%\n",
            "iter 2710: loss 4.2587, time 280.22ms, mfu 18.44%\n",
            "iter 2720: loss 4.3934, time 279.94ms, mfu 18.63%\n",
            "iter 2730: loss 4.3171, time 281.45ms, mfu 18.80%\n",
            "iter 2740: loss 4.2609, time 281.20ms, mfu 18.95%\n",
            "iter 2750: loss 4.2579, time 280.45ms, mfu 19.09%\n",
            "iter 2760: loss 4.3562, time 280.05ms, mfu 19.22%\n",
            "iter 2770: loss 4.2999, time 281.31ms, mfu 19.32%\n",
            "iter 2780: loss 4.3008, time 280.37ms, mfu 19.43%\n",
            "iter 2790: loss 4.2563, time 280.03ms, mfu 19.52%\n",
            "iter 2800: loss 4.2668, time 280.24ms, mfu 19.60%\n",
            "iter 2810: loss 4.2848, time 281.38ms, mfu 19.67%\n",
            "iter 2820: loss 4.2690, time 280.67ms, mfu 19.74%\n",
            "iter 2830: loss 4.2082, time 279.97ms, mfu 19.80%\n",
            "iter 2840: loss 4.2906, time 280.38ms, mfu 19.86%\n",
            "iter 2850: loss 4.2670, time 281.09ms, mfu 19.90%\n",
            "iter 2860: loss 4.2099, time 279.96ms, mfu 19.95%\n",
            "iter 2870: loss 4.1843, time 280.61ms, mfu 19.99%\n",
            "iter 2880: loss 4.1214, time 281.20ms, mfu 20.02%\n",
            "iter 2890: loss 4.1792, time 280.35ms, mfu 20.05%\n",
            "iter 2900: loss 4.2796, time 279.76ms, mfu 20.09%\n",
            "iter 2910: loss 4.3773, time 280.44ms, mfu 20.11%\n",
            "iter 2920: loss 4.2455, time 280.29ms, mfu 20.14%\n",
            "iter 2930: loss 4.3518, time 281.31ms, mfu 20.15%\n",
            "iter 2940: loss 4.2168, time 281.82ms, mfu 20.16%\n",
            "iter 2950: loss 4.2551, time 280.60ms, mfu 20.18%\n",
            "iter 2960: loss 4.2194, time 279.89ms, mfu 20.20%\n",
            "iter 2970: loss 4.1704, time 279.89ms, mfu 20.22%\n",
            "iter 2980: loss 4.1375, time 280.81ms, mfu 20.23%\n",
            "iter 2990: loss 4.2429, time 281.60ms, mfu 20.23%\n",
            "step 3000: train loss 4.2138, val loss 4.2082\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3000: loss 4.1757, time 37401.18ms, mfu 18.22%\n",
            "iter 3010: loss 4.2079, time 280.97ms, mfu 18.43%\n",
            "iter 3020: loss 4.1633, time 280.63ms, mfu 18.62%\n",
            "iter 3030: loss 4.3001, time 280.74ms, mfu 18.79%\n",
            "iter 3040: loss 4.1165, time 281.21ms, mfu 18.94%\n",
            "iter 3050: loss 4.1145, time 281.04ms, mfu 19.08%\n",
            "iter 3060: loss 4.1998, time 280.51ms, mfu 19.20%\n",
            "iter 3070: loss 4.2278, time 279.91ms, mfu 19.32%\n",
            "iter 3080: loss 4.1764, time 281.62ms, mfu 19.42%\n",
            "iter 3090: loss 4.2971, time 281.41ms, mfu 19.50%\n",
            "iter 3100: loss 4.2906, time 279.36ms, mfu 19.59%\n",
            "iter 3110: loss 4.1346, time 279.89ms, mfu 19.67%\n",
            "iter 3120: loss 4.1789, time 281.21ms, mfu 19.74%\n",
            "iter 3130: loss 4.2670, time 281.10ms, mfu 19.79%\n",
            "iter 3140: loss 4.2072, time 280.01ms, mfu 19.85%\n",
            "iter 3150: loss 4.0750, time 280.64ms, mfu 19.90%\n",
            "iter 3160: loss 4.2631, time 281.55ms, mfu 19.94%\n",
            "iter 3170: loss 4.2420, time 281.13ms, mfu 19.97%\n",
            "iter 3180: loss 4.1559, time 280.35ms, mfu 20.01%\n",
            "iter 3190: loss 4.1328, time 281.54ms, mfu 20.04%\n",
            "iter 3200: loss 4.1347, time 280.80ms, mfu 20.06%\n",
            "iter 3210: loss 4.2657, time 279.69ms, mfu 20.10%\n",
            "iter 3220: loss 4.2621, time 281.14ms, mfu 20.12%\n",
            "iter 3230: loss 4.1044, time 281.14ms, mfu 20.14%\n",
            "iter 3240: loss 4.0825, time 280.35ms, mfu 20.16%\n",
            "iter 3250: loss 4.1845, time 280.01ms, mfu 20.18%\n",
            "iter 3260: loss 4.1483, time 279.78ms, mfu 20.20%\n",
            "iter 3270: loss 4.0521, time 279.87ms, mfu 20.22%\n",
            "iter 3280: loss 4.1159, time 279.96ms, mfu 20.24%\n",
            "iter 3290: loss 4.2386, time 282.35ms, mfu 20.23%\n",
            "step 3300: train loss 4.1623, val loss 4.1479\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3300: loss 4.0845, time 37301.01ms, mfu 18.22%\n",
            "iter 3310: loss 4.0610, time 280.08ms, mfu 18.44%\n",
            "iter 3320: loss 4.1920, time 279.86ms, mfu 18.63%\n",
            "iter 3330: loss 4.1873, time 280.95ms, mfu 18.80%\n",
            "iter 3340: loss 4.1870, time 282.27ms, mfu 18.94%\n",
            "iter 3350: loss 4.1346, time 280.46ms, mfu 19.08%\n",
            "iter 3360: loss 4.1938, time 279.86ms, mfu 19.21%\n",
            "iter 3370: loss 4.1311, time 279.76ms, mfu 19.33%\n",
            "iter 3380: loss 4.1175, time 280.06ms, mfu 19.44%\n",
            "iter 3390: loss 4.1258, time 280.01ms, mfu 19.53%\n",
            "iter 3400: loss 4.1617, time 281.03ms, mfu 19.61%\n",
            "iter 3410: loss 4.1645, time 281.33ms, mfu 19.68%\n",
            "iter 3420: loss 4.2261, time 280.00ms, mfu 19.75%\n",
            "iter 3430: loss 4.1020, time 279.94ms, mfu 19.81%\n",
            "iter 3440: loss 4.0531, time 281.49ms, mfu 19.86%\n",
            "iter 3450: loss 4.0948, time 281.40ms, mfu 19.90%\n",
            "iter 3460: loss 4.1976, time 280.57ms, mfu 19.94%\n",
            "iter 3470: loss 4.2104, time 280.21ms, mfu 19.98%\n",
            "iter 3480: loss 4.1615, time 281.22ms, mfu 20.01%\n",
            "iter 3490: loss 4.1596, time 280.71ms, mfu 20.05%\n",
            "iter 3500: loss 4.1759, time 281.05ms, mfu 20.07%\n",
            "iter 3510: loss 4.0014, time 281.24ms, mfu 20.09%\n",
            "iter 3520: loss 4.0575, time 281.00ms, mfu 20.11%\n",
            "iter 3530: loss 4.1439, time 279.95ms, mfu 20.14%\n",
            "iter 3540: loss 4.1751, time 280.40ms, mfu 20.16%\n",
            "iter 3550: loss 4.1938, time 281.66ms, mfu 20.17%\n",
            "iter 3560: loss 4.2595, time 280.43ms, mfu 20.19%\n",
            "iter 3570: loss 4.0586, time 280.28ms, mfu 20.21%\n",
            "iter 3580: loss 4.0985, time 281.37ms, mfu 20.21%\n",
            "iter 3590: loss 4.1165, time 281.58ms, mfu 20.22%\n",
            "step 3600: train loss 4.1041, val loss 4.1003\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3600: loss 4.0845, time 37310.19ms, mfu 18.21%\n",
            "iter 3610: loss 4.0686, time 280.17ms, mfu 18.43%\n",
            "iter 3620: loss 4.0527, time 280.08ms, mfu 18.62%\n",
            "iter 3630: loss 3.9732, time 281.24ms, mfu 18.79%\n",
            "iter 3640: loss 4.0573, time 281.01ms, mfu 18.94%\n",
            "iter 3650: loss 4.1923, time 280.02ms, mfu 19.08%\n",
            "iter 3660: loss 4.0434, time 280.41ms, mfu 19.21%\n",
            "iter 3670: loss 4.1167, time 281.70ms, mfu 19.31%\n",
            "iter 3680: loss 4.1561, time 280.78ms, mfu 19.42%\n",
            "iter 3690: loss 4.0751, time 279.93ms, mfu 19.51%\n",
            "iter 3700: loss 4.0482, time 280.69ms, mfu 19.59%\n",
            "iter 3710: loss 4.1457, time 280.86ms, mfu 19.67%\n",
            "iter 3720: loss 4.1125, time 280.74ms, mfu 19.73%\n",
            "iter 3730: loss 4.0616, time 282.06ms, mfu 19.78%\n",
            "iter 3740: loss 4.0632, time 281.28ms, mfu 19.83%\n",
            "iter 3750: loss 4.0790, time 280.27ms, mfu 19.88%\n",
            "iter 3760: loss 3.9480, time 280.12ms, mfu 19.93%\n",
            "iter 3770: loss 4.1331, time 281.14ms, mfu 19.97%\n",
            "iter 3780: loss 4.1553, time 281.56ms, mfu 20.00%\n",
            "iter 3790: loss 4.1108, time 280.67ms, mfu 20.03%\n",
            "iter 3800: loss 4.0754, time 280.09ms, mfu 20.07%\n",
            "iter 3810: loss 4.1521, time 281.41ms, mfu 20.09%\n",
            "iter 3820: loss 4.0160, time 281.19ms, mfu 20.11%\n",
            "iter 3830: loss 4.1758, time 280.62ms, mfu 20.13%\n",
            "iter 3840: loss 4.1019, time 280.58ms, mfu 20.15%\n",
            "iter 3850: loss 4.0819, time 281.24ms, mfu 20.16%\n",
            "iter 3860: loss 4.0697, time 281.44ms, mfu 20.18%\n",
            "iter 3870: loss 3.9640, time 280.15ms, mfu 20.19%\n",
            "iter 3880: loss 4.1320, time 281.63ms, mfu 20.20%\n",
            "iter 3890: loss 4.0832, time 280.45ms, mfu 20.22%\n",
            "step 3900: train loss 4.0721, val loss 4.0665\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3900: loss 4.0833, time 37333.07ms, mfu 18.21%\n",
            "iter 3910: loss 4.1460, time 281.04ms, mfu 18.42%\n",
            "iter 3920: loss 4.0670, time 281.41ms, mfu 18.60%\n",
            "iter 3930: loss 4.0250, time 280.96ms, mfu 18.77%\n",
            "iter 3940: loss 3.9582, time 280.15ms, mfu 18.93%\n",
            "iter 3950: loss 4.0540, time 280.74ms, mfu 19.07%\n",
            "iter 3960: loss 4.1843, time 281.76ms, mfu 19.19%\n",
            "iter 3970: loss 4.1848, time 281.45ms, mfu 19.30%\n",
            "iter 3980: loss 4.0612, time 279.93ms, mfu 19.41%\n",
            "iter 3990: loss 3.9560, time 279.88ms, mfu 19.51%\n",
            "iter 4000: loss 4.1121, time 280.69ms, mfu 19.59%\n",
            "iter 4010: loss 4.0362, time 280.81ms, mfu 19.66%\n",
            "iter 4020: loss 4.0905, time 280.04ms, mfu 19.73%\n",
            "iter 4030: loss 4.0682, time 280.12ms, mfu 19.80%\n",
            "iter 4040: loss 3.9636, time 281.59ms, mfu 19.84%\n",
            "iter 4050: loss 4.0213, time 280.71ms, mfu 19.89%\n",
            "iter 4060: loss 4.0972, time 281.15ms, mfu 19.93%\n",
            "iter 4070: loss 4.0883, time 281.08ms, mfu 19.97%\n",
            "iter 4080: loss 4.0747, time 280.45ms, mfu 20.01%\n",
            "iter 4090: loss 4.0749, time 279.74ms, mfu 20.05%\n",
            "iter 4100: loss 4.0289, time 280.06ms, mfu 20.08%\n",
            "iter 4110: loss 4.1223, time 281.31ms, mfu 20.10%\n",
            "iter 4120: loss 4.1017, time 281.25ms, mfu 20.12%\n",
            "iter 4130: loss 3.9483, time 280.07ms, mfu 20.14%\n",
            "iter 4140: loss 4.0127, time 280.45ms, mfu 20.16%\n",
            "iter 4150: loss 4.1543, time 281.03ms, mfu 20.18%\n",
            "iter 4160: loss 4.1189, time 281.46ms, mfu 20.19%\n",
            "iter 4170: loss 4.1244, time 281.20ms, mfu 20.20%\n",
            "iter 4180: loss 4.1000, time 280.29ms, mfu 20.21%\n",
            "iter 4190: loss 4.2060, time 280.26ms, mfu 20.23%\n",
            "step 4200: train loss 4.0379, val loss 4.0365\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4200: loss 3.9924, time 37283.86ms, mfu 18.22%\n",
            "iter 4210: loss 4.1345, time 280.85ms, mfu 18.43%\n",
            "iter 4220: loss 4.0614, time 281.44ms, mfu 18.61%\n",
            "iter 4230: loss 4.0100, time 281.15ms, mfu 18.78%\n",
            "iter 4240: loss 3.9562, time 279.18ms, mfu 18.95%\n",
            "iter 4250: loss 4.0683, time 281.30ms, mfu 19.08%\n",
            "iter 4260: loss 3.9723, time 281.45ms, mfu 19.20%\n",
            "iter 4270: loss 3.8456, time 280.43ms, mfu 19.32%\n",
            "iter 4280: loss 4.0971, time 279.97ms, mfu 19.42%\n",
            "iter 4290: loss 4.0732, time 281.33ms, mfu 19.51%\n",
            "iter 4300: loss 4.0406, time 281.65ms, mfu 19.58%\n",
            "iter 4310: loss 3.9846, time 280.42ms, mfu 19.66%\n",
            "iter 4320: loss 4.1079, time 280.35ms, mfu 19.73%\n",
            "iter 4330: loss 3.9783, time 281.45ms, mfu 19.78%\n",
            "iter 4340: loss 3.9275, time 280.42ms, mfu 19.84%\n",
            "iter 4350: loss 4.1527, time 280.26ms, mfu 19.89%\n",
            "iter 4360: loss 3.9871, time 281.13ms, mfu 19.93%\n",
            "iter 4370: loss 3.8329, time 281.26ms, mfu 19.97%\n",
            "iter 4380: loss 3.9191, time 280.76ms, mfu 20.00%\n",
            "iter 4390: loss 4.0355, time 279.88ms, mfu 20.04%\n",
            "iter 4400: loss 4.0465, time 280.18ms, mfu 20.07%\n",
            "iter 4410: loss 4.0958, time 280.07ms, mfu 20.10%\n",
            "iter 4420: loss 4.0196, time 279.86ms, mfu 20.13%\n",
            "iter 4430: loss 4.0751, time 279.94ms, mfu 20.16%\n",
            "iter 4440: loss 4.0821, time 280.25ms, mfu 20.18%\n",
            "iter 4450: loss 4.1509, time 281.70ms, mfu 20.19%\n",
            "iter 4460: loss 4.0562, time 280.96ms, mfu 20.20%\n",
            "iter 4470: loss 3.8842, time 280.60ms, mfu 20.21%\n",
            "iter 4480: loss 3.9385, time 278.55ms, mfu 20.24%\n",
            "iter 4490: loss 4.0195, time 280.46ms, mfu 20.25%\n",
            "step 4500: train loss 4.0103, val loss 4.0170\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4500: loss 4.0430, time 37305.86ms, mfu 18.24%\n",
            "iter 4510: loss 4.0456, time 279.90ms, mfu 18.45%\n",
            "iter 4520: loss 4.1351, time 279.88ms, mfu 18.65%\n",
            "iter 4530: loss 3.9155, time 279.90ms, mfu 18.82%\n",
            "iter 4540: loss 4.0364, time 280.09ms, mfu 18.98%\n",
            "iter 4550: loss 4.1025, time 281.16ms, mfu 19.11%\n",
            "iter 4560: loss 4.1379, time 281.23ms, mfu 19.23%\n",
            "iter 4570: loss 3.9243, time 280.13ms, mfu 19.34%\n",
            "iter 4580: loss 3.9641, time 279.60ms, mfu 19.45%\n",
            "iter 4590: loss 3.9643, time 280.61ms, mfu 19.54%\n",
            "iter 4600: loss 3.8357, time 281.23ms, mfu 19.61%\n",
            "iter 4610: loss 3.9121, time 280.88ms, mfu 19.68%\n",
            "iter 4620: loss 4.0984, time 279.66ms, mfu 19.75%\n",
            "iter 4630: loss 4.0565, time 281.09ms, mfu 19.81%\n",
            "iter 4640: loss 3.8706, time 267.21ms, mfu 19.96%\n",
            "iter 4650: loss 4.1230, time 278.82ms, mfu 20.01%\n",
            "iter 4660: loss 3.9721, time 281.45ms, mfu 20.04%\n",
            "iter 4670: loss 3.9589, time 280.77ms, mfu 20.07%\n",
            "iter 4680: loss 3.9971, time 280.20ms, mfu 20.10%\n",
            "iter 4690: loss 4.0595, time 279.82ms, mfu 20.13%\n",
            "iter 4700: loss 4.0196, time 281.58ms, mfu 20.14%\n",
            "iter 4710: loss 4.1100, time 281.32ms, mfu 20.15%\n",
            "iter 4720: loss 4.0521, time 279.95ms, mfu 20.18%\n",
            "iter 4730: loss 3.9911, time 280.68ms, mfu 20.19%\n",
            "iter 4740: loss 3.9439, time 281.11ms, mfu 20.20%\n",
            "iter 4750: loss 4.1120, time 280.89ms, mfu 20.21%\n",
            "iter 4760: loss 3.9610, time 281.03ms, mfu 20.22%\n",
            "iter 4770: loss 3.9566, time 280.34ms, mfu 20.24%\n",
            "iter 4780: loss 3.9706, time 280.46ms, mfu 20.25%\n",
            "iter 4790: loss 3.8139, time 279.95ms, mfu 20.26%\n",
            "step 4800: train loss 4.0091, val loss 4.0011\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4800: loss 3.9620, time 37326.91ms, mfu 18.25%\n",
            "iter 4810: loss 4.0801, time 280.03ms, mfu 18.46%\n",
            "iter 4820: loss 4.0219, time 279.98ms, mfu 18.65%\n",
            "iter 4830: loss 4.0535, time 281.18ms, mfu 18.82%\n",
            "iter 4840: loss 3.9579, time 281.63ms, mfu 18.96%\n",
            "iter 4850: loss 4.0243, time 280.38ms, mfu 19.10%\n",
            "iter 4860: loss 3.8781, time 280.15ms, mfu 19.23%\n",
            "iter 4870: loss 4.0988, time 280.67ms, mfu 19.34%\n",
            "iter 4880: loss 3.9837, time 280.55ms, mfu 19.44%\n",
            "iter 4890: loss 4.0768, time 280.19ms, mfu 19.53%\n",
            "iter 4900: loss 4.0841, time 279.99ms, mfu 19.62%\n",
            "iter 4910: loss 3.9768, time 280.98ms, mfu 19.68%\n",
            "iter 4920: loss 4.1352, time 281.40ms, mfu 19.74%\n",
            "iter 4930: loss 3.9916, time 281.02ms, mfu 19.80%\n",
            "iter 4940: loss 4.0046, time 280.05ms, mfu 19.86%\n",
            "iter 4950: loss 4.0617, time 281.48ms, mfu 19.90%\n",
            "iter 4960: loss 3.8170, time 280.96ms, mfu 19.94%\n",
            "iter 4970: loss 3.9100, time 280.03ms, mfu 19.98%\n",
            "iter 4980: loss 3.9493, time 280.01ms, mfu 20.02%\n",
            "iter 4990: loss 3.9751, time 281.09ms, mfu 20.05%\n",
            "iter 5000: loss 4.0167, time 280.90ms, mfu 20.08%\n",
            "iter 5010: loss 4.0761, time 280.30ms, mfu 20.10%\n",
            "iter 5020: loss 3.9132, time 280.67ms, mfu 20.13%\n",
            "iter 5030: loss 4.0614, time 281.51ms, mfu 20.14%\n",
            "iter 5040: loss 4.0235, time 280.71ms, mfu 20.16%\n",
            "iter 5050: loss 4.0363, time 279.99ms, mfu 20.18%\n",
            "iter 5060: loss 4.0651, time 281.04ms, mfu 20.19%\n",
            "iter 5070: loss 3.9836, time 281.50ms, mfu 20.20%\n",
            "iter 5080: loss 4.0049, time 280.59ms, mfu 20.21%\n",
            "iter 5090: loss 3.9308, time 279.92ms, mfu 20.23%\n",
            "step 5100: train loss 3.9973, val loss 4.0019\n",
            "iter 5100: loss 4.0839, time 35280.54ms, mfu 18.22%\n",
            "iter 5110: loss 3.9399, time 280.43ms, mfu 18.44%\n",
            "iter 5120: loss 4.0925, time 280.03ms, mfu 18.63%\n",
            "iter 5130: loss 4.0631, time 281.39ms, mfu 18.80%\n",
            "iter 5140: loss 4.0133, time 280.84ms, mfu 18.95%\n",
            "iter 5150: loss 4.0317, time 279.99ms, mfu 19.09%\n",
            "iter 5160: loss 4.0087, time 281.62ms, mfu 19.21%\n",
            "iter 5170: loss 3.9079, time 280.66ms, mfu 19.32%\n",
            "iter 5180: loss 3.9400, time 280.16ms, mfu 19.42%\n",
            "iter 5190: loss 4.0128, time 281.30ms, mfu 19.51%\n",
            "iter 5200: loss 4.0937, time 281.43ms, mfu 19.59%\n",
            "iter 5210: loss 4.1110, time 279.59ms, mfu 19.67%\n",
            "iter 5220: loss 3.8898, time 280.19ms, mfu 19.74%\n",
            "iter 5230: loss 3.9917, time 280.14ms, mfu 19.80%\n",
            "iter 5240: loss 3.9513, time 280.23ms, mfu 19.86%\n",
            "iter 5250: loss 3.9746, time 280.48ms, mfu 19.91%\n",
            "iter 5260: loss 3.9821, time 281.06ms, mfu 19.95%\n",
            "iter 5270: loss 4.0315, time 281.72ms, mfu 19.98%\n",
            "iter 5280: loss 3.8909, time 280.55ms, mfu 20.01%\n",
            "iter 5290: loss 3.9429, time 280.42ms, mfu 20.05%\n",
            "iter 5300: loss 4.0164, time 282.77ms, mfu 20.06%\n",
            "iter 5310: loss 3.9957, time 281.30ms, mfu 20.08%\n",
            "iter 5320: loss 4.0603, time 281.38ms, mfu 20.10%\n",
            "iter 5330: loss 3.9485, time 279.72ms, mfu 20.13%\n",
            "iter 5340: loss 4.0266, time 281.52ms, mfu 20.15%\n",
            "iter 5350: loss 3.9973, time 281.37ms, mfu 20.16%\n",
            "iter 5360: loss 3.9728, time 280.74ms, mfu 20.18%\n",
            "iter 5370: loss 3.9049, time 280.07ms, mfu 20.19%\n",
            "iter 5380: loss 3.8544, time 281.56ms, mfu 20.20%\n",
            "iter 5390: loss 3.8868, time 280.59ms, mfu 20.22%\n",
            "step 5400: train loss 4.0003, val loss 3.9983\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5400: loss 4.0781, time 37336.30ms, mfu 18.21%\n",
            "iter 5410: loss 3.9612, time 280.23ms, mfu 18.42%\n",
            "iter 5420: loss 4.0184, time 281.59ms, mfu 18.61%\n",
            "iter 5430: loss 4.0024, time 280.63ms, mfu 18.78%\n",
            "iter 5440: loss 3.9611, time 279.88ms, mfu 18.94%\n",
            "iter 5450: loss 3.8749, time 281.20ms, mfu 19.08%\n",
            "iter 5460: loss 4.0219, time 281.21ms, mfu 19.20%\n",
            "iter 5470: loss 3.9409, time 280.29ms, mfu 19.31%\n",
            "iter 5480: loss 3.9646, time 279.87ms, mfu 19.42%\n",
            "iter 5490: loss 4.0237, time 280.08ms, mfu 19.52%\n",
            "iter 5500: loss 4.0615, time 281.60ms, mfu 19.59%\n",
            "iter 5510: loss 3.9449, time 280.53ms, mfu 19.67%\n",
            "iter 5520: loss 4.0499, time 280.56ms, mfu 19.73%\n",
            "iter 5530: loss 4.0136, time 280.34ms, mfu 19.80%\n",
            "iter 5540: loss 3.9287, time 281.74ms, mfu 19.84%\n",
            "iter 5550: loss 3.8370, time 281.16ms, mfu 19.89%\n",
            "iter 5560: loss 3.9726, time 280.97ms, mfu 19.93%\n",
            "iter 5570: loss 3.9268, time 280.55ms, mfu 19.97%\n",
            "iter 5580: loss 3.9612, time 280.97ms, mfu 20.00%\n",
            "iter 5590: loss 4.0390, time 280.94ms, mfu 20.03%\n",
            "iter 5600: loss 3.8737, time 280.47ms, mfu 20.06%\n",
            "iter 5610: loss 4.0337, time 280.22ms, mfu 20.09%\n",
            "iter 5620: loss 3.9343, time 280.03ms, mfu 20.12%\n",
            "iter 5630: loss 4.0368, time 279.87ms, mfu 20.15%\n",
            "iter 5640: loss 4.0716, time 281.23ms, mfu 20.16%\n",
            "iter 5650: loss 3.9848, time 280.52ms, mfu 20.18%\n",
            "iter 5660: loss 3.9232, time 280.76ms, mfu 20.20%\n",
            "iter 5670: loss 3.9380, time 279.84ms, mfu 20.21%\n",
            "iter 5680: loss 4.0916, time 280.79ms, mfu 20.23%\n",
            "iter 5690: loss 3.8826, time 281.25ms, mfu 20.23%\n",
            "step 5700: train loss 3.9948, val loss 3.9820\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5700: loss 4.0592, time 37322.45ms, mfu 18.22%\n",
            "iter 5710: loss 4.1830, time 281.20ms, mfu 18.43%\n",
            "iter 5720: loss 3.9950, time 281.33ms, mfu 18.62%\n",
            "iter 5730: loss 4.0329, time 281.25ms, mfu 18.78%\n",
            "iter 5740: loss 3.8660, time 278.99ms, mfu 18.95%\n",
            "iter 5750: loss 3.9020, time 279.98ms, mfu 19.09%\n",
            "iter 5760: loss 3.9769, time 280.95ms, mfu 19.21%\n",
            "iter 5770: loss 4.0432, time 281.81ms, mfu 19.32%\n",
            "iter 5780: loss 4.0225, time 281.19ms, mfu 19.42%\n",
            "iter 5790: loss 4.0637, time 280.89ms, mfu 19.51%\n",
            "iter 5800: loss 4.0101, time 279.64ms, mfu 19.60%\n",
            "iter 5810: loss 3.8707, time 281.15ms, mfu 19.66%\n",
            "iter 5820: loss 3.9909, time 281.48ms, mfu 19.73%\n",
            "iter 5830: loss 3.9513, time 280.34ms, mfu 19.79%\n",
            "iter 5840: loss 4.0292, time 279.70ms, mfu 19.85%\n",
            "iter 5850: loss 4.0029, time 281.11ms, mfu 19.89%\n",
            "iter 5860: loss 3.9631, time 281.13ms, mfu 19.93%\n",
            "iter 5870: loss 4.1183, time 279.62ms, mfu 19.98%\n",
            "iter 5880: loss 3.9769, time 281.03ms, mfu 20.01%\n",
            "iter 5890: loss 3.9998, time 281.10ms, mfu 20.04%\n",
            "iter 5900: loss 3.9303, time 280.35ms, mfu 20.07%\n",
            "iter 5910: loss 3.9203, time 280.34ms, mfu 20.10%\n",
            "iter 5920: loss 4.0319, time 280.39ms, mfu 20.13%\n",
            "iter 5930: loss 3.9764, time 280.84ms, mfu 20.15%\n",
            "iter 5940: loss 4.0541, time 280.30ms, mfu 20.17%\n",
            "iter 5950: loss 3.9946, time 279.97ms, mfu 20.19%\n",
            "iter 5960: loss 3.9789, time 279.98ms, mfu 20.21%\n",
            "iter 5970: loss 3.9014, time 281.29ms, mfu 20.21%\n",
            "iter 5980: loss 3.9626, time 281.63ms, mfu 20.22%\n",
            "iter 5990: loss 4.0278, time 280.30ms, mfu 20.23%\n",
            "step 6000: train loss 3.9902, val loss 3.9874\n",
            "iter 6000: loss 3.9711, time 35289.01ms, mfu 18.23%\n",
            "iter 6010: loss 3.9867, time 281.29ms, mfu 18.43%\n",
            "iter 6020: loss 4.0315, time 281.30ms, mfu 18.62%\n",
            "iter 6030: loss 3.9074, time 281.33ms, mfu 18.78%\n",
            "iter 6040: loss 4.0631, time 281.10ms, mfu 18.94%\n",
            "iter 6050: loss 3.9473, time 280.61ms, mfu 19.07%\n",
            "iter 6060: loss 4.1204, time 281.06ms, mfu 19.20%\n",
            "iter 6070: loss 3.9556, time 281.17ms, mfu 19.31%\n",
            "iter 6080: loss 3.8239, time 280.47ms, mfu 19.41%\n",
            "iter 6090: loss 3.9336, time 280.15ms, mfu 19.51%\n",
            "iter 6100: loss 3.7827, time 280.97ms, mfu 19.59%\n",
            "iter 6110: loss 4.0216, time 281.33ms, mfu 19.66%\n",
            "iter 6120: loss 4.0387, time 280.16ms, mfu 19.73%\n",
            "iter 6130: loss 4.0383, time 279.96ms, mfu 19.79%\n",
            "iter 6140: loss 4.1723, time 280.97ms, mfu 19.84%\n",
            "iter 6150: loss 4.0807, time 283.28ms, mfu 19.87%\n",
            "iter 6160: loss 3.9545, time 280.34ms, mfu 19.92%\n",
            "iter 6170: loss 3.9554, time 279.97ms, mfu 19.97%\n",
            "iter 6180: loss 4.1623, time 281.23ms, mfu 20.00%\n",
            "iter 6190: loss 3.9076, time 281.05ms, mfu 20.03%\n",
            "iter 6200: loss 3.9156, time 280.80ms, mfu 20.06%\n",
            "iter 6210: loss 3.9300, time 279.86ms, mfu 20.09%\n",
            "iter 6220: loss 4.0629, time 280.50ms, mfu 20.12%\n",
            "iter 6230: loss 4.1304, time 281.23ms, mfu 20.13%\n",
            "iter 6240: loss 4.0117, time 281.29ms, mfu 20.15%\n",
            "iter 6250: loss 4.0224, time 280.91ms, mfu 20.17%\n",
            "iter 6260: loss 3.9034, time 280.21ms, mfu 20.19%\n",
            "iter 6270: loss 4.0298, time 280.98ms, mfu 20.20%\n",
            "iter 6280: loss 3.9657, time 281.50ms, mfu 20.20%\n",
            "iter 6290: loss 3.8584, time 281.01ms, mfu 20.21%\n",
            "step 6300: train loss 3.9935, val loss 3.9753\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 6300: loss 4.1103, time 37339.00ms, mfu 18.21%\n",
            "iter 6310: loss 3.9964, time 280.60ms, mfu 18.42%\n",
            "iter 6320: loss 4.0191, time 280.16ms, mfu 18.62%\n",
            "iter 6330: loss 4.1172, time 279.83ms, mfu 18.79%\n",
            "iter 6340: loss 3.9646, time 281.41ms, mfu 18.94%\n",
            "iter 6350: loss 3.9639, time 281.31ms, mfu 19.08%\n",
            "iter 6360: loss 4.0562, time 280.37ms, mfu 19.20%\n",
            "iter 6370: loss 4.0217, time 279.94ms, mfu 19.32%\n",
            "iter 6380: loss 3.9540, time 279.89ms, mfu 19.43%\n",
            "iter 6390: loss 4.0238, time 279.81ms, mfu 19.52%\n",
            "iter 6400: loss 3.9964, time 282.23ms, mfu 19.59%\n",
            "iter 6410: loss 4.0089, time 280.97ms, mfu 19.66%\n",
            "iter 6420: loss 3.9937, time 280.62ms, mfu 19.73%\n",
            "iter 6430: loss 3.9641, time 280.32ms, mfu 19.79%\n",
            "iter 6440: loss 3.9979, time 280.02ms, mfu 19.85%\n",
            "iter 6450: loss 4.0595, time 281.42ms, mfu 19.89%\n",
            "iter 6460: loss 4.1934, time 281.46ms, mfu 19.93%\n",
            "iter 6470: loss 4.0201, time 280.30ms, mfu 19.97%\n",
            "iter 6480: loss 3.9299, time 280.19ms, mfu 20.01%\n",
            "iter 6490: loss 3.9757, time 281.28ms, mfu 20.04%\n",
            "iter 6500: loss 3.9239, time 280.75ms, mfu 20.07%\n",
            "iter 6510: loss 3.8976, time 279.61ms, mfu 20.10%\n",
            "iter 6520: loss 3.9526, time 281.24ms, mfu 20.12%\n",
            "iter 6530: loss 3.9491, time 281.16ms, mfu 20.14%\n",
            "iter 6540: loss 3.9611, time 280.39ms, mfu 20.16%\n",
            "iter 6550: loss 4.1085, time 279.43ms, mfu 20.19%\n",
            "iter 6560: loss 3.9880, time 281.56ms, mfu 20.19%\n",
            "iter 6570: loss 3.9088, time 281.35ms, mfu 20.20%\n",
            "iter 6580: loss 3.7887, time 280.49ms, mfu 20.22%\n",
            "iter 6590: loss 3.9123, time 280.08ms, mfu 20.23%\n",
            "step 6600: train loss 3.9853, val loss 3.9857\n",
            "iter 6600: loss 3.9495, time 35286.36ms, mfu 18.23%\n",
            "iter 6610: loss 3.9824, time 279.69ms, mfu 18.44%\n",
            "iter 6620: loss 4.1339, time 280.13ms, mfu 18.64%\n",
            "iter 6630: loss 3.9476, time 281.11ms, mfu 18.80%\n",
            "iter 6640: loss 3.9343, time 281.17ms, mfu 18.95%\n",
            "iter 6650: loss 3.9494, time 279.33ms, mfu 19.10%\n",
            "iter 6660: loss 3.8878, time 279.97ms, mfu 19.23%\n",
            "iter 6670: loss 3.9017, time 281.35ms, mfu 19.33%\n",
            "iter 6680: loss 4.1143, time 280.95ms, mfu 19.43%\n",
            "iter 6690: loss 4.0160, time 280.99ms, mfu 19.52%\n",
            "iter 6700: loss 3.9781, time 279.75ms, mfu 19.61%\n",
            "iter 6710: loss 3.9761, time 281.02ms, mfu 19.68%\n",
            "iter 6720: loss 3.9371, time 281.27ms, mfu 19.74%\n",
            "iter 6730: loss 4.0331, time 281.45ms, mfu 19.79%\n",
            "iter 6740: loss 3.9785, time 281.10ms, mfu 19.84%\n",
            "iter 6750: loss 3.8473, time 281.13ms, mfu 19.89%\n",
            "iter 6760: loss 4.0261, time 281.09ms, mfu 19.93%\n",
            "iter 6770: loss 4.0138, time 281.05ms, mfu 19.97%\n",
            "iter 6780: loss 4.0162, time 279.75ms, mfu 20.01%\n",
            "iter 6790: loss 3.9300, time 279.81ms, mfu 20.05%\n",
            "iter 6800: loss 3.9873, time 281.69ms, mfu 20.07%\n",
            "iter 6810: loss 4.1069, time 281.16ms, mfu 20.09%\n",
            "iter 6820: loss 3.9824, time 280.46ms, mfu 20.12%\n",
            "iter 6830: loss 3.9862, time 279.30ms, mfu 20.15%\n",
            "iter 6840: loss 3.9622, time 281.17ms, mfu 20.16%\n",
            "iter 6850: loss 3.9620, time 281.51ms, mfu 20.17%\n",
            "iter 6860: loss 4.0467, time 280.40ms, mfu 20.19%\n",
            "iter 6870: loss 3.9491, time 280.37ms, mfu 20.21%\n",
            "iter 6880: loss 4.0230, time 281.12ms, mfu 20.22%\n",
            "iter 6890: loss 3.9800, time 281.54ms, mfu 20.22%\n",
            "step 6900: train loss 3.9936, val loss 3.9811\n",
            "iter 6900: loss 3.9601, time 35301.05ms, mfu 18.21%\n",
            "iter 6910: loss 3.9713, time 281.45ms, mfu 18.42%\n",
            "iter 6920: loss 3.9569, time 281.50ms, mfu 18.61%\n",
            "iter 6930: loss 3.8916, time 280.46ms, mfu 18.78%\n",
            "iter 6940: loss 3.9707, time 279.65ms, mfu 18.94%\n",
            "iter 6950: loss 4.1088, time 280.67ms, mfu 19.08%\n",
            "iter 6960: loss 3.9329, time 281.16ms, mfu 19.20%\n",
            "iter 6970: loss 3.9513, time 280.00ms, mfu 19.32%\n",
            "iter 6980: loss 3.9487, time 279.90ms, mfu 19.43%\n",
            "iter 6990: loss 3.9522, time 281.35ms, mfu 19.51%\n",
            "iter 7000: loss 4.0114, time 280.78ms, mfu 19.59%\n",
            "iter 7010: loss 4.0017, time 279.60ms, mfu 19.67%\n",
            "iter 7020: loss 3.9327, time 280.63ms, mfu 19.74%\n",
            "iter 7030: loss 3.9305, time 281.48ms, mfu 19.79%\n",
            "iter 7040: loss 4.0209, time 280.96ms, mfu 19.84%\n",
            "iter 7050: loss 3.9615, time 280.47ms, mfu 19.89%\n",
            "iter 7060: loss 4.0331, time 279.99ms, mfu 19.94%\n",
            "iter 7070: loss 3.9089, time 281.30ms, mfu 19.98%\n",
            "iter 7080: loss 4.0241, time 282.10ms, mfu 20.00%\n",
            "iter 7090: loss 4.0044, time 281.44ms, mfu 20.03%\n",
            "iter 7100: loss 4.0786, time 280.97ms, mfu 20.06%\n",
            "iter 7110: loss 3.9763, time 281.32ms, mfu 20.08%\n",
            "iter 7120: loss 4.0261, time 281.18ms, mfu 20.10%\n",
            "iter 7130: loss 3.9512, time 281.23ms, mfu 20.12%\n",
            "iter 7140: loss 3.9944, time 280.20ms, mfu 20.14%\n",
            "iter 7150: loss 4.0653, time 280.01ms, mfu 20.17%\n",
            "iter 7160: loss 4.0156, time 280.53ms, mfu 20.18%\n",
            "iter 7170: loss 4.0135, time 281.29ms, mfu 20.19%\n",
            "iter 7180: loss 4.1004, time 280.82ms, mfu 20.21%\n",
            "iter 7190: loss 4.0147, time 280.83ms, mfu 20.22%\n",
            "step 7200: train loss 3.9848, val loss 3.9870\n",
            "iter 7200: loss 4.0004, time 35283.72ms, mfu 18.21%\n",
            "iter 7210: loss 3.9896, time 279.98ms, mfu 18.43%\n",
            "iter 7220: loss 3.9621, time 279.68ms, mfu 18.63%\n",
            "iter 7230: loss 4.0171, time 280.85ms, mfu 18.80%\n",
            "iter 7240: loss 3.9908, time 281.52ms, mfu 18.94%\n",
            "iter 7250: loss 4.0106, time 281.22ms, mfu 19.08%\n",
            "iter 7260: loss 4.0793, time 280.98ms, mfu 19.20%\n",
            "iter 7270: loss 3.9752, time 281.01ms, mfu 19.31%\n",
            "iter 7280: loss 3.9676, time 279.92ms, mfu 19.42%\n",
            "iter 7290: loss 4.0254, time 281.19ms, mfu 19.51%\n",
            "iter 7300: loss 3.9555, time 281.67ms, mfu 19.58%\n",
            "iter 7310: loss 3.9554, time 281.13ms, mfu 19.65%\n",
            "iter 7320: loss 4.0087, time 280.30ms, mfu 19.72%\n",
            "iter 7330: loss 3.9779, time 279.86ms, mfu 19.79%\n",
            "iter 7340: loss 3.9811, time 281.05ms, mfu 19.84%\n",
            "iter 7350: loss 4.0183, time 281.49ms, mfu 19.88%\n",
            "iter 7360: loss 4.0550, time 281.07ms, mfu 19.93%\n",
            "iter 7370: loss 3.9819, time 280.85ms, mfu 19.96%\n",
            "iter 7380: loss 3.9416, time 279.89ms, mfu 20.01%\n",
            "iter 7390: loss 3.9942, time 280.19ms, mfu 20.04%\n",
            "iter 7400: loss 3.9262, time 281.03ms, mfu 20.07%\n",
            "iter 7410: loss 3.9941, time 281.33ms, mfu 20.09%\n",
            "iter 7420: loss 4.0648, time 280.38ms, mfu 20.12%\n",
            "iter 7430: loss 3.8474, time 280.02ms, mfu 20.14%\n",
            "iter 7440: loss 3.9825, time 280.41ms, mfu 20.16%\n",
            "iter 7450: loss 3.8791, time 280.51ms, mfu 20.18%\n",
            "iter 7460: loss 3.7813, time 280.61ms, mfu 20.20%\n",
            "iter 7470: loss 3.9850, time 280.13ms, mfu 20.21%\n",
            "iter 7480: loss 3.9854, time 279.93ms, mfu 20.23%\n",
            "iter 7490: loss 3.9832, time 280.50ms, mfu 20.24%\n",
            "step 7500: train loss 3.9889, val loss 3.9726\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 7500: loss 4.1139, time 37340.11ms, mfu 18.23%\n",
            "iter 7510: loss 4.0738, time 281.62ms, mfu 18.44%\n",
            "iter 7520: loss 3.9918, time 280.78ms, mfu 18.62%\n",
            "iter 7530: loss 4.0166, time 280.37ms, mfu 18.80%\n",
            "iter 7540: loss 4.1124, time 280.06ms, mfu 18.95%\n",
            "iter 7550: loss 4.0189, time 279.86ms, mfu 19.10%\n",
            "iter 7560: loss 3.9185, time 279.89ms, mfu 19.23%\n",
            "iter 7570: loss 4.0244, time 279.99ms, mfu 19.34%\n",
            "iter 7580: loss 3.9593, time 280.96ms, mfu 19.44%\n",
            "iter 7590: loss 3.9667, time 281.13ms, mfu 19.52%\n",
            "iter 7600: loss 4.1104, time 280.81ms, mfu 19.60%\n",
            "iter 7610: loss 4.0251, time 280.98ms, mfu 19.67%\n",
            "iter 7620: loss 3.9510, time 279.86ms, mfu 19.75%\n",
            "iter 7630: loss 3.9194, time 281.00ms, mfu 19.80%\n",
            "iter 7640: loss 4.0424, time 281.52ms, mfu 19.85%\n",
            "iter 7650: loss 4.0358, time 280.05ms, mfu 19.90%\n",
            "iter 7660: loss 4.0895, time 280.05ms, mfu 19.95%\n",
            "iter 7670: loss 3.9640, time 281.28ms, mfu 19.98%\n",
            "iter 7680: loss 4.0629, time 281.16ms, mfu 20.01%\n",
            "iter 7690: loss 3.9925, time 280.90ms, mfu 20.04%\n",
            "iter 7700: loss 3.9358, time 280.62ms, mfu 20.07%\n",
            "iter 7710: loss 3.9215, time 281.49ms, mfu 20.09%\n",
            "iter 7720: loss 4.0318, time 280.08ms, mfu 20.12%\n",
            "iter 7730: loss 4.0541, time 280.09ms, mfu 20.15%\n",
            "iter 7740: loss 4.0171, time 280.12ms, mfu 20.17%\n",
            "iter 7750: loss 3.9577, time 281.02ms, mfu 20.18%\n",
            "iter 7760: loss 3.9936, time 281.14ms, mfu 20.19%\n",
            "iter 7770: loss 3.9474, time 281.04ms, mfu 20.20%\n",
            "iter 7780: loss 3.9527, time 280.44ms, mfu 20.22%\n",
            "iter 7790: loss 4.1877, time 280.60ms, mfu 20.23%\n",
            "step 7800: train loss 3.9876, val loss 3.9736\n",
            "iter 7800: loss 4.0623, time 35293.21ms, mfu 18.22%\n",
            "iter 7810: loss 3.9285, time 281.62ms, mfu 18.43%\n",
            "iter 7820: loss 3.9399, time 281.55ms, mfu 18.61%\n",
            "iter 7830: loss 3.9680, time 281.05ms, mfu 18.78%\n",
            "iter 7840: loss 3.9957, time 279.99ms, mfu 18.94%\n",
            "iter 7850: loss 3.9407, time 279.88ms, mfu 19.08%\n",
            "iter 7860: loss 3.9852, time 281.19ms, mfu 19.21%\n",
            "iter 7870: loss 4.0573, time 281.39ms, mfu 19.31%\n",
            "iter 7880: loss 3.9652, time 279.62ms, mfu 19.42%\n",
            "iter 7890: loss 4.0187, time 279.89ms, mfu 19.52%\n",
            "iter 7900: loss 3.9877, time 279.95ms, mfu 19.60%\n",
            "iter 7910: loss 3.9232, time 280.00ms, mfu 19.68%\n",
            "iter 7920: loss 4.0768, time 280.78ms, mfu 19.75%\n",
            "iter 7930: loss 3.9147, time 281.62ms, mfu 19.80%\n",
            "iter 7940: loss 4.0767, time 279.08ms, mfu 19.86%\n",
            "iter 7950: loss 3.9679, time 280.38ms, mfu 19.91%\n",
            "iter 7960: loss 4.0669, time 280.88ms, mfu 19.95%\n",
            "iter 7970: loss 3.9070, time 281.24ms, mfu 19.99%\n",
            "iter 7980: loss 3.8787, time 281.53ms, mfu 20.01%\n",
            "iter 7990: loss 3.8753, time 280.95ms, mfu 20.04%\n",
            "iter 8000: loss 4.0160, time 279.74ms, mfu 20.08%\n",
            "iter 8010: loss 3.9746, time 280.77ms, mfu 20.10%\n",
            "iter 8020: loss 4.0186, time 281.27ms, mfu 20.12%\n",
            "iter 8030: loss 3.9883, time 280.63ms, mfu 20.14%\n",
            "iter 8040: loss 4.0543, time 279.87ms, mfu 20.17%\n",
            "iter 8050: loss 4.0546, time 280.02ms, mfu 20.19%\n",
            "iter 8060: loss 3.9721, time 280.77ms, mfu 20.20%\n",
            "iter 8070: loss 4.0757, time 281.32ms, mfu 20.21%\n",
            "iter 8080: loss 4.0936, time 280.50ms, mfu 20.22%\n",
            "iter 8090: loss 3.8466, time 279.65ms, mfu 20.24%\n",
            "step 8100: train loss 3.9768, val loss 3.9723\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 8100: loss 3.9911, time 37313.00ms, mfu 18.23%\n",
            "iter 8110: loss 3.9975, time 280.18ms, mfu 18.45%\n",
            "iter 8120: loss 3.8883, time 279.84ms, mfu 18.64%\n",
            "iter 8130: loss 4.0241, time 280.18ms, mfu 18.81%\n",
            "iter 8140: loss 4.0985, time 280.30ms, mfu 18.97%\n",
            "iter 8150: loss 4.0075, time 281.27ms, mfu 19.10%\n",
            "iter 8160: loss 3.9241, time 280.07ms, mfu 19.23%\n",
            "iter 8170: loss 4.0563, time 280.54ms, mfu 19.34%\n",
            "iter 8180: loss 4.0311, time 281.82ms, mfu 19.43%\n",
            "iter 8190: loss 4.0661, time 280.59ms, mfu 19.52%\n",
            "iter 8200: loss 4.0416, time 279.71ms, mfu 19.61%\n",
            "iter 8210: loss 4.0206, time 280.67ms, mfu 19.68%\n",
            "iter 8220: loss 4.0214, time 281.28ms, mfu 19.74%\n",
            "iter 8230: loss 4.0889, time 280.70ms, mfu 19.80%\n",
            "iter 8240: loss 3.9733, time 280.39ms, mfu 19.85%\n",
            "iter 8250: loss 4.0958, time 280.31ms, mfu 19.90%\n",
            "iter 8260: loss 3.9897, time 280.01ms, mfu 19.95%\n",
            "iter 8270: loss 3.8599, time 280.16ms, mfu 19.99%\n",
            "iter 8280: loss 3.9345, time 279.99ms, mfu 20.03%\n",
            "iter 8290: loss 3.9236, time 280.88ms, mfu 20.06%\n",
            "iter 8300: loss 4.1030, time 281.16ms, mfu 20.08%\n",
            "iter 8310: loss 4.0905, time 281.13ms, mfu 20.10%\n",
            "iter 8320: loss 3.9216, time 280.06ms, mfu 20.13%\n",
            "iter 8330: loss 4.1701, time 280.33ms, mfu 20.15%\n",
            "iter 8340: loss 4.0078, time 281.47ms, mfu 20.17%\n",
            "iter 8350: loss 4.0636, time 281.08ms, mfu 20.18%\n",
            "iter 8360: loss 3.8821, time 280.03ms, mfu 20.20%\n",
            "iter 8370: loss 4.0105, time 280.82ms, mfu 20.21%\n",
            "iter 8380: loss 4.0148, time 280.77ms, mfu 20.22%\n",
            "iter 8390: loss 3.9643, time 280.74ms, mfu 20.23%\n",
            "step 8400: train loss 3.9813, val loss 3.9732\n",
            "iter 8400: loss 4.0080, time 35268.65ms, mfu 18.23%\n",
            "iter 8410: loss 4.0636, time 280.18ms, mfu 18.44%\n",
            "iter 8420: loss 3.9827, time 280.77ms, mfu 18.63%\n",
            "iter 8430: loss 4.0235, time 281.18ms, mfu 18.79%\n",
            "iter 8440: loss 3.9892, time 280.62ms, mfu 18.95%\n",
            "iter 8450: loss 3.9986, time 279.85ms, mfu 19.09%\n",
            "iter 8460: loss 3.9878, time 281.33ms, mfu 19.21%\n",
            "iter 8470: loss 3.9843, time 281.29ms, mfu 19.32%\n",
            "iter 8480: loss 3.9801, time 279.87ms, mfu 19.43%\n",
            "iter 8490: loss 3.9216, time 280.55ms, mfu 19.52%\n",
            "iter 8500: loss 3.9572, time 281.49ms, mfu 19.59%\n",
            "iter 8510: loss 3.8962, time 280.41ms, mfu 19.67%\n",
            "iter 8520: loss 4.0182, time 279.87ms, mfu 19.74%\n",
            "iter 8530: loss 3.9108, time 281.88ms, mfu 19.79%\n",
            "iter 8540: loss 3.9654, time 281.32ms, mfu 19.84%\n",
            "iter 8550: loss 3.9121, time 280.23ms, mfu 19.89%\n",
            "iter 8560: loss 3.8490, time 279.98ms, mfu 19.94%\n",
            "iter 8570: loss 3.9775, time 281.28ms, mfu 19.97%\n",
            "iter 8580: loss 3.9265, time 280.92ms, mfu 20.01%\n",
            "iter 8590: loss 3.9841, time 279.70ms, mfu 20.05%\n",
            "iter 8600: loss 3.8656, time 279.86ms, mfu 20.08%\n",
            "iter 8610: loss 3.9616, time 279.69ms, mfu 20.11%\n",
            "iter 8620: loss 4.0264, time 280.56ms, mfu 20.14%\n",
            "iter 8630: loss 3.8809, time 281.23ms, mfu 20.15%\n",
            "iter 8640: loss 3.9729, time 281.36ms, mfu 20.16%\n",
            "iter 8650: loss 3.9559, time 280.25ms, mfu 20.18%\n",
            "iter 8660: loss 3.9281, time 279.97ms, mfu 20.20%\n",
            "iter 8670: loss 4.0496, time 280.91ms, mfu 20.21%\n",
            "iter 8680: loss 4.0148, time 281.37ms, mfu 20.22%\n",
            "iter 8690: loss 4.0279, time 280.51ms, mfu 20.23%\n",
            "step 8700: train loss 3.9799, val loss 3.9756\n",
            "iter 8700: loss 4.0240, time 35279.47ms, mfu 18.23%\n",
            "iter 8710: loss 3.9984, time 280.90ms, mfu 18.43%\n",
            "iter 8720: loss 3.9807, time 280.88ms, mfu 18.62%\n",
            "iter 8730: loss 4.0617, time 281.04ms, mfu 18.79%\n",
            "iter 8740: loss 3.7584, time 280.97ms, mfu 18.94%\n",
            "iter 8750: loss 4.0128, time 281.27ms, mfu 19.08%\n",
            "iter 8760: loss 4.0342, time 280.40ms, mfu 19.20%\n",
            "iter 8770: loss 4.0130, time 279.95ms, mfu 19.32%\n",
            "iter 8780: loss 3.9963, time 281.22ms, mfu 19.42%\n",
            "iter 8790: loss 4.0087, time 281.55ms, mfu 19.50%\n",
            "iter 8800: loss 3.9376, time 280.31ms, mfu 19.59%\n",
            "iter 8810: loss 3.9583, time 280.40ms, mfu 19.66%\n",
            "iter 8820: loss 3.9651, time 281.83ms, mfu 19.72%\n",
            "iter 8830: loss 4.0183, time 281.99ms, mfu 19.77%\n",
            "iter 8840: loss 3.9777, time 279.60ms, mfu 19.84%\n",
            "iter 8850: loss 3.8503, time 280.16ms, mfu 19.89%\n",
            "iter 8860: loss 4.0235, time 281.42ms, mfu 19.93%\n",
            "iter 8870: loss 4.0285, time 280.57ms, mfu 19.97%\n",
            "iter 8880: loss 3.8683, time 280.38ms, mfu 20.01%\n",
            "iter 8890: loss 4.0896, time 280.08ms, mfu 20.04%\n",
            "iter 8900: loss 4.1116, time 281.12ms, mfu 20.07%\n",
            "iter 8910: loss 4.0150, time 281.57ms, mfu 20.09%\n",
            "iter 8920: loss 3.9104, time 279.77ms, mfu 20.12%\n",
            "iter 8930: loss 3.9266, time 280.04ms, mfu 20.15%\n",
            "iter 8940: loss 4.0317, time 280.81ms, mfu 20.16%\n",
            "iter 8950: loss 3.9368, time 281.84ms, mfu 20.17%\n",
            "iter 8960: loss 3.9753, time 280.23ms, mfu 20.19%\n",
            "iter 8970: loss 4.0271, time 280.27ms, mfu 20.21%\n",
            "iter 8980: loss 3.9987, time 281.45ms, mfu 20.21%\n",
            "iter 8990: loss 3.9524, time 281.02ms, mfu 20.22%\n",
            "step 9000: train loss 3.9710, val loss 3.9705\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9000: loss 3.9604, time 37292.47ms, mfu 18.22%\n",
            "iter 9010: loss 4.0128, time 280.87ms, mfu 18.43%\n",
            "iter 9020: loss 3.9376, time 280.60ms, mfu 18.62%\n",
            "iter 9030: loss 4.0243, time 279.86ms, mfu 18.79%\n",
            "iter 9040: loss 3.9817, time 280.50ms, mfu 18.95%\n",
            "iter 9050: loss 3.8450, time 281.43ms, mfu 19.08%\n",
            "iter 9060: loss 3.9555, time 280.87ms, mfu 19.20%\n",
            "iter 9070: loss 3.8467, time 280.89ms, mfu 19.32%\n",
            "iter 9080: loss 3.9784, time 280.27ms, mfu 19.42%\n",
            "iter 9090: loss 3.9212, time 279.99ms, mfu 19.52%\n",
            "iter 9100: loss 4.0065, time 280.02ms, mfu 19.60%\n",
            "iter 9110: loss 3.9597, time 280.83ms, mfu 19.67%\n",
            "iter 9120: loss 3.9350, time 281.13ms, mfu 19.74%\n",
            "iter 9130: loss 4.0283, time 280.27ms, mfu 19.80%\n",
            "iter 9140: loss 4.0604, time 279.97ms, mfu 19.86%\n",
            "iter 9150: loss 4.0222, time 281.62ms, mfu 19.90%\n",
            "iter 9160: loss 4.1311, time 281.00ms, mfu 19.94%\n",
            "iter 9170: loss 4.0155, time 280.14ms, mfu 19.98%\n",
            "iter 9180: loss 3.8408, time 280.90ms, mfu 20.01%\n",
            "iter 9190: loss 3.8242, time 281.36ms, mfu 20.04%\n",
            "iter 9200: loss 3.9190, time 279.86ms, mfu 20.08%\n",
            "iter 9210: loss 4.0478, time 280.32ms, mfu 20.10%\n",
            "iter 9220: loss 3.8943, time 281.40ms, mfu 20.12%\n",
            "iter 9230: loss 3.9600, time 280.62ms, mfu 20.14%\n",
            "iter 9240: loss 4.0762, time 280.11ms, mfu 20.16%\n",
            "iter 9250: loss 3.8300, time 280.94ms, mfu 20.18%\n",
            "iter 9260: loss 3.9489, time 280.23ms, mfu 20.20%\n",
            "iter 9270: loss 4.0053, time 280.25ms, mfu 20.21%\n",
            "iter 9280: loss 3.9286, time 281.56ms, mfu 20.22%\n",
            "iter 9290: loss 4.0259, time 280.92ms, mfu 20.23%\n",
            "step 9300: train loss 3.9808, val loss 3.9692\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9300: loss 3.8569, time 37295.42ms, mfu 18.22%\n",
            "iter 9310: loss 4.0510, time 280.38ms, mfu 18.43%\n",
            "iter 9320: loss 4.0159, time 281.29ms, mfu 18.62%\n",
            "iter 9330: loss 3.7961, time 280.99ms, mfu 18.79%\n",
            "iter 9340: loss 3.8705, time 280.13ms, mfu 18.95%\n",
            "iter 9350: loss 4.0317, time 280.98ms, mfu 19.08%\n",
            "iter 9360: loss 3.9656, time 281.32ms, mfu 19.20%\n",
            "iter 9370: loss 4.0638, time 279.84ms, mfu 19.32%\n",
            "iter 9380: loss 3.9041, time 279.89ms, mfu 19.43%\n",
            "iter 9390: loss 3.9471, time 280.72ms, mfu 19.52%\n",
            "iter 9400: loss 3.9390, time 281.43ms, mfu 19.59%\n",
            "iter 9410: loss 4.0507, time 280.87ms, mfu 19.67%\n",
            "iter 9420: loss 3.8782, time 281.00ms, mfu 19.73%\n",
            "iter 9430: loss 3.9794, time 281.24ms, mfu 19.79%\n",
            "iter 9440: loss 3.8856, time 280.44ms, mfu 19.84%\n",
            "iter 9450: loss 3.9285, time 280.14ms, mfu 19.89%\n",
            "iter 9460: loss 4.0156, time 280.74ms, mfu 19.94%\n",
            "iter 9470: loss 3.9843, time 281.22ms, mfu 19.97%\n",
            "iter 9480: loss 3.9297, time 280.53ms, mfu 20.01%\n",
            "iter 9490: loss 4.1014, time 279.96ms, mfu 20.05%\n",
            "iter 9500: loss 3.8854, time 280.76ms, mfu 20.07%\n",
            "iter 9510: loss 4.0218, time 281.48ms, mfu 20.09%\n",
            "iter 9520: loss 4.0212, time 279.62ms, mfu 20.12%\n",
            "iter 9530: loss 4.0139, time 280.00ms, mfu 20.15%\n",
            "iter 9540: loss 3.8253, time 281.31ms, mfu 20.16%\n",
            "iter 9550: loss 3.9468, time 280.18ms, mfu 20.18%\n",
            "iter 9560: loss 3.9034, time 279.80ms, mfu 20.20%\n",
            "iter 9570: loss 4.0733, time 280.59ms, mfu 20.22%\n",
            "iter 9580: loss 3.9755, time 281.49ms, mfu 20.22%\n",
            "iter 9590: loss 3.9697, time 280.58ms, mfu 20.23%\n",
            "step 9600: train loss 3.9740, val loss 3.9665\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9600: loss 3.9492, time 37296.74ms, mfu 18.23%\n",
            "iter 9610: loss 3.9358, time 281.22ms, mfu 18.43%\n",
            "iter 9620: loss 3.9335, time 280.29ms, mfu 18.63%\n",
            "iter 9630: loss 4.0238, time 280.11ms, mfu 18.80%\n",
            "iter 9640: loss 3.9489, time 281.39ms, mfu 18.95%\n",
            "iter 9650: loss 3.9935, time 280.71ms, mfu 19.09%\n",
            "iter 9660: loss 4.0950, time 279.74ms, mfu 19.22%\n",
            "iter 9670: loss 4.0974, time 281.50ms, mfu 19.32%\n",
            "iter 9680: loss 3.9987, time 280.72ms, mfu 19.42%\n",
            "iter 9690: loss 3.9352, time 279.99ms, mfu 19.52%\n",
            "iter 9700: loss 3.8712, time 281.14ms, mfu 19.60%\n",
            "iter 9710: loss 3.9542, time 281.41ms, mfu 19.66%\n",
            "iter 9720: loss 3.8518, time 280.39ms, mfu 19.73%\n",
            "iter 9730: loss 3.9425, time 279.83ms, mfu 19.80%\n",
            "iter 9740: loss 4.0267, time 280.95ms, mfu 19.85%\n",
            "iter 9750: loss 3.9506, time 281.14ms, mfu 19.89%\n",
            "iter 9760: loss 3.9882, time 280.27ms, mfu 19.94%\n",
            "iter 9770: loss 3.9724, time 280.96ms, mfu 19.98%\n",
            "iter 9780: loss 3.9638, time 281.28ms, mfu 20.01%\n",
            "iter 9790: loss 4.0012, time 280.92ms, mfu 20.04%\n",
            "iter 9800: loss 3.9327, time 280.41ms, mfu 20.07%\n",
            "iter 9810: loss 3.9794, time 281.04ms, mfu 20.09%\n",
            "iter 9820: loss 3.9817, time 280.71ms, mfu 20.12%\n",
            "iter 9830: loss 3.9898, time 281.00ms, mfu 20.14%\n",
            "iter 9840: loss 3.8969, time 280.26ms, mfu 20.16%\n",
            "iter 9850: loss 3.9195, time 280.16ms, mfu 20.18%\n",
            "iter 9860: loss 4.0687, time 281.78ms, mfu 20.19%\n",
            "iter 9870: loss 4.0007, time 280.70ms, mfu 20.20%\n",
            "iter 9880: loss 3.8939, time 280.50ms, mfu 20.21%\n",
            "iter 9890: loss 4.0911, time 279.99ms, mfu 20.23%\n",
            "step 9900: train loss 3.9680, val loss 3.9701\n",
            "iter 9900: loss 3.9409, time 35276.93ms, mfu 18.22%\n",
            "iter 9910: loss 3.8849, time 279.94ms, mfu 18.44%\n",
            "iter 9920: loss 4.0254, time 280.62ms, mfu 18.63%\n",
            "iter 9930: loss 3.9170, time 281.75ms, mfu 18.79%\n",
            "iter 9940: loss 4.0132, time 280.40ms, mfu 18.95%\n",
            "iter 9950: loss 3.9503, time 279.91ms, mfu 19.09%\n",
            "iter 9960: loss 3.9712, time 281.41ms, mfu 19.21%\n",
            "iter 9970: loss 3.8134, time 281.18ms, mfu 19.32%\n",
            "iter 9980: loss 3.9986, time 279.98ms, mfu 19.42%\n",
            "iter 9990: loss 3.9462, time 281.29ms, mfu 19.51%\n",
            "iter 10000: loss 3.8837, time 281.49ms, mfu 19.59%\n"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gftxsbvJUCYR"
      },
      "outputs": [],
      "source": [
        "# start = \"How to join a tbank-research?\"\n",
        "# start = \"How to join a tbank-research?\"\n",
        "start = \"It's snow.\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib1voUfVUCYR",
        "outputId": "f36bf217-1e21-40d1-fdd6-37c7d50535fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-9f0241acb2d4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHUmXQWLUCYS",
        "outputId": "6cfba90a-639a-4ae9-8fc8-6574ebfe788d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's snow. It's still the same,\" she said.\n",
            "\n",
            "\"And I want it to lose.\"<|endoftext|>The Army soldiers who went to work in Afghanistan have been arrested on the site, including the last one for two years for the Veterans Affairs of the Army.\n",
            "\n",
            "By the time the Army will be charged, at least three thousand soldiers, the Defense Department says.\n",
            "\n",
            "The Army has been involved since March 31, which includes the new \"an\" system for troops in Afghanistan, Afghanistan, the Pentagon's Secretary of Defense, and the military's top commander.\n",
            "\n",
            "The more than 100 troops are expected to be in this year's first two years for the Pentagon and Lockheed Martin.\n",
            "\n",
            "The Pentagon has the second largest collection of detainees, which includes the 10,000 prisoners convicted of crimes in Afghanistan and Afghanistan, according to the National Centers for National Guard, the Defense Department said, according to the Pentagon's Office on the new report.\n",
            "\n",
            "The military has been in the ranks of military officers across the globe, with the highest level of job security since the Army is the top-ranked country's list of detainees.\n",
            "\n",
            "Story continues below advertisement\n",
            "\n",
            "The Army has more than 1,000 soldiers in Afghanistan, and more than 3,000 have been killed in Afghanistan, according to the National Institute for Civil Services.<|endoftext|>Image copyright EPA Image caption A new poll of 1,092 people had been killed on a mosque in the Khawramabad area by the police state of Bengaluru , a busing station in the capital, the capital of the country's capital, in the capital, the capital of the capital, the capital of the country's capital.\n",
            "\n",
            "The data collected by the police group showed that there were two types of homes in the capital, which are now around 8,000 people, most on the street in the capital, according to the figures, in the country of Bengaluru, a town in the capital, the capital, at the centre of the district.\n",
            "\n",
            "The toll group was at 0.2% in the first four years of the year, with the average time for the state's total population.\n",
            "\n",
            "Prime Minister Narendra Modi, he told reporters news footage about how his country's capital is now, that it wasn't the first time a police officer involved in the riots, but the prime minister for the next three years was just the result of his leadership in recent years.\n",
            "\n",
            "He said there were few people who\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"We have a good time.\"\n",
            "\n",
            "She said her team had a good day and another good Friday, which was on her way. She's now battling with her family, and she's so worried that the team has what it can do with it. \"It's a good day.\"\n",
            "\n",
            "She said the team was in shock and that it was a good start for a game. And just for the team and it was good news for her team and she's trying to get her own team.\n",
            "\n",
            "Her team's wife, Mark Sanchez, will be playing with the Houston Texans for the season.\n",
            "\n",
            "\"It's really good for the team,\" said Ross.\n",
            "\n",
            "The Bears also were in the midst of an argument about the team's progress.\n",
            "\n",
            "\"My team's been on track to win the game and I've never really got the support of my team and it's an argument that I think we can play with the team and our team. I was really excited about if it was up and we didn't have that opportunity to play against us in the playoffs and for the team.\"\n",
            "\n",
            "The Bears signed a $18 million deal, but the team never completed an offer.\n",
            "\n",
            "If the team had to have an offer, they could have been without a contract. So for now, the Bears have a lot more.\n",
            "\n",
            "Follow @DavidBatt<|endoftext|>\"The Great Britain\" redirects here. For more information, see the relevant links to the original version<|endoftext|>A new story of one of the most ridiculous stories on this planet in the history of the world.\n",
            "\n",
            "It may have been the first time (and it's a bit strange to see) but as you know it, the truth is that the world is struggling to grasp the facts of life in the first place. The story of the world goes on and on, and the experience of the future is how we move forward with those that we are at the center of the world. In the first week of Friday's 11th anniversary of the Great Britain period, the first world world where the United States was the world's first world world to see itself as the world's greatest nation to experience. Now, the first time the world has been to the realization that the world is changing, and what we know today is the world that the world is changing.\n",
            "\n",
            "It is the real world that we,\n",
            "\n",
            "The world is now, in fact, the world\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"It's the same place for the home, but no one else has to worry about,\" he said.\n",
            "\n",
            "\"You're being more, you're out of a path to where you have a good summer and a lot of things.\"\n",
            "\n",
            "As for the second quarter, players with their own team have been in their third quarter and their third quarter -- most of the first pair of games - which can't handle any team that can handle any team in the next three or seven years.\n",
            "\n",
            "Even six of the players in the group are competing in the second quarter, and that means they're ready to play the game.\n",
            "\n",
            "\"I think they're playing and in a way, they're in a way different place,\" said Alex \"to get their team up and compete for the next level.\"\n",
            "\n",
            "So it's possible they're playing with their own group and have to be prepared to play against them.\n",
            "\n",
            "\"There's no need for that,\" said Eric Upper. \"I think that's what we're doing, I think we'll be building a football game, and we're just going to be able to play like a football game.\"\n",
            "\n",
            "The first step was not for the third quarter but was to be released, with only a few of the players in the group.\n",
            "\n",
            "At the end of the fourth quarter, players played the first round, but only two were in the group.\n",
            "\n",
            "The second quarter continued to become the third quarter, with 12 players in the group with their own team.\n",
            "\n",
            "\"We're up against the puck. We're out of a strong team and we want to try to convince ourselves,\" said Upper, who now runs his players' game.\n",
            "\n",
            "It's unclear if the players in the group made more than three players in the group, and the other players would have to re-play this game.\n",
            "\n",
            "\"There's no question but then it's unfortunate,\" said Upper.\n",
            "\n",
            "The problem was that they thought they were going to change their mind against a team that they got the right fit for.\n",
            "\n",
            "\"When we're talking about this team, we want to find out how to play. We want to be with the same team.\"\n",
            "\n",
            "A similar situation has resulted in the club's second quarter.\n",
            "\n",
            "The first half of the game has not yet been finalized.\n",
            "\n",
            "That's because of the first quarter that the team had more than\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "His parents say they need to work when a car is needed to be able to take a hard hit, but they've tried to improve the driving.\n",
            "\n",
            "The car, under its control, is now on the roads.\n",
            "\n",
            "\"It's going to get to the bike because we're doing everything that goes through,\" he said.\n",
            "\n",
            "\"We're talking to San Francisco for a while and we're just looking at the bike so we can keep that bike safer on the road.\"\n",
            "\n",
            "\"It's going to make a lot of progress. But it's just trying to see what happens in San Francisco that happen, from the standpoint of being able to have a very hard time.\"\n",
            "\n",
            "The video from the San Francisco Chronicle reports that the bike's traffic had been delayed for a while, on the other end of the day, and that there were reports that the bike is safe, and that it wasn't clear what to expect as the bike's traffic was being delayed for about a month.\n",
            "\n",
            "\"We've got a lot of traffic,\" he said. \"If we get in the car, we'll start working with the bike.\"\n",
            "\n",
            "Sallyher is a longtime San Francisco resident and living in San Francisco, but he says he has the money to buy a bike or a bike.\n",
            "\n",
            "\"I think that's actually more than what we really want to do,\" he said. \"We're really looking for the bike and we actually are really trying to do that.\"\n",
            "\n",
            "The project works out to date that the bike is working on, but it's also not the only way to get it done, he said.\n",
            "\n",
            "Sallyher, who asked the city's chief executive of the Denver City Council, said she doesn't know about this, but she never saw it.\n",
            "\n",
            "\"I just wanted to keep that bike,\" she said. \"But we're really looking at the bike to make a difference, because we know what's going to have. We've got to keep the bike here and look at what this happened to our bike and that's a different project from the city.\"\n",
            "\n",
            "Sallyher has since moved from the city at the end of this year, and has a good track record of bike-friendly bike.\"\n",
            "\n",
            "Sallyher's bike has been slow in the last six years, and she has been the first to do that with the bike.<|endoftext|>I have always believed that the people\n",
            "---------------\n",
            "It's snow. Now, the first thing I've played in the past 15 years, I'm going to try to do a lot of things to do.\n",
            "\n",
            "So, as long as we've had those days, I used to it a little bit later and then they started working on our own. So I was in some kind of kind of playing on the phone and playing on the phone.\n",
            "\n",
            "I'm very proud of it so much, as long as I'd like to play the games, but you're like, \"Oh, you know what you do.\" And we both worked really hard on the iPhone.\n",
            "\n",
            "But I just don't care if I'm going to feel like I'm going to try but I mean I can't get them to play it on my iPhone, and I'm going to try and get them to play it on my iPhone and I'm going to be able to play it on my iPhone. And I think it's going to happen and I'm not going to try and just get me to play it on my iPhone and I'm going to try and get it on my iPhone, so I can try and get me to play it on my iPhone and I'm going to try and get myself back to the device.\n",
            "\n",
            "I'm really proud of it so much. I think I want to try and do whatever it takes to get in it. I'm hoping for the next few months and I'll be able to play the games in general, so I want to try and get me in the next couple weeks and months and I think it's going to be a huge fight with the new iPhone. I don't know if it's going to happen when I'm going to try and get it on my iPhone and then I'm going to try and get it on my iPhone.\n",
            "\n",
            "It's worth noting that the phone's iPhones are actually the result of the iPhones, so I'm not going to go through next week. But I think it's going to be a big fight for the iPhone. I'm also going to try and get the iPhone with it, to do it that's a great fight for the iPhone.\n",
            "\n",
            "If the iPhone was a great thing, I'd say back in the end and I think it's going to be a big fight for the iPhone, but I think it's a different story. I try to make this happen, and it's going to play it on the iPhone as a surprise.\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egbXtGm4UCYS"
      },
      "outputs": [],
      "source": [
        "start = \"How to join a tbank-research?\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUyskvgHUCYS",
        "outputId": "31fca2d9-2ce2-4b24-e454-339a61cc03bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to join a tbank-research?\n",
            "\n",
            "Yes, that is a big deal. It is worth the price. Where is it like the gold? There is no price for the bank and its debt is no one. But what does it do?\n",
            "\n",
            "Well, it is not a small amount of money on the banks. The amount of money they are being invested, which is an essential part of the asset. That means the money that the banks do not have to rely.\n",
            "\n",
            "But we have no confidence in our investments.\n",
            "\n",
            "But that said, the banks have no confidence in their investments, so they have to rely on the money. They have a lot of money and need to work. They have the money and money back from the money.\n",
            "\n",
            "But that is because many investors actually are doing a lot of money than money. I can say that while people have been doing a lot of money. I have a lot of cash.\n",
            "\n",
            "And they are getting the money they have.\n",
            "\n",
            "And they will be using them with a lot of money.\n",
            "\n",
            "And they are already spending their money. They have billions of dollars. And they have the money. They have trillions of dollars. They have to be able to look for the other way, and they have billions of dollars. So they have billions of dollars. And they have trillions of dollars.\n",
            "\n",
            "They are out of these things. It has to be them now.\n",
            "\n",
            "And they have millions of dollars.\n",
            "\n",
            "And they have billions of dollars every time they have, and they have billions of dollars over the next 10 years.\n",
            "\n",
            "And it is the one thing.\n",
            "\n",
            "So what is the bank doing?\n",
            "\n",
            "Well, your money has to be able to get the goods and services necessary. Of course, if you are in need of that, you go out there and take your money out and start doing it. You have to put everything in your place.\n",
            "\n",
            "And the money is going to make money. You have to invest in it. You have to put a tax on you.\n",
            "\n",
            "On top of all the money you want.\n",
            "\n",
            "And the money is going to be a big investment. How you can use the money.\n",
            "\n",
            "And you have to make money.\n",
            "\n",
            "And that is the kind of money you want to make money. You have to put all their money out of debt. You have to put all those funds in it, you have to put them\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "If you had a friend who, and who, as a journalist in a major economy manager and now a politician, I’d want to know. But I’d like to go to the D.E. to learn how to navigate the world, and learn how to do the job; they’d have to learn the lessons that I call “bail” to go to the N.C. to get through the D.E. and the D.E. as a journalist. The T.E. is a top-secret and senior staff writer, and I am very excited about the D.E.’s work.\n",
            "\n",
            "To receive a call, visit the D.E.’s workshop, follow or go to the N.E.’s conference, and find a friend who works on the project. He explains why he’s just the start.\n",
            "\n",
            "What about you, and how to make your team do it, and how to become a director?\n",
            "\n",
            "I’d love to talk to the general public about that, when I go to the N.C., and ask me to write a letter to David, for the first time. Here’s what we’re talking about:\n",
            "\n",
            "In the end, we’re going to be at least three people at a certain time. We’re going to be at least two people at that time and we’re going to be there for those around the world. And for everyone being a scientist, there’s a lot of people who are so great that we’re going to be in those offices and making sure we’re going to see the things we’re in and see how we’re going to be here. And it’s time to get started.\n",
            "\n",
            "What did you think of that?\n",
            "\n",
            "I’d like to talk to the people themselves. And I have a different attitude, I think. I’ve always been passionate about things, and it’s my job, and we’ve talked a lot and we’d like to think about the art, but I’d like to do a lot of work on that and I think it’s just fun. So, it’s kind of cool. It’s a way of doing that\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "The US economy is growing at a constant pace, with the unemployment rate as much as 12 percent as the economy increases. But it is seen that job growth is going up and the growth is still improving in the short term. But if we get to look to what is necessary, we’re still in the recession. And that’s a good thing, and this is a good thing.\n",
            "\n",
            "The US economy was in an economic crisis. For the first time since World War II, the US economy was rising for second purpose in 2009 and it’s still strong.\n",
            "\n",
            "The US economy was up 10 percent and the economy is still at a rate that the US economy is at a rate that has to be kept at a rate that is now going up and the jobs will rise. So the US economy is now the top priority for the United States. That’s not in the US economy that’s a bit higher than the US economy.\n",
            "\n",
            "“The reason I feel is that the US economy is going up, and the US economy is going up,” states the US economy is going up.\n",
            "\n",
            "A typical example of the US economy is not a single household.\n",
            "\n",
            "The US economy is very weak in the US economy, but with economic growth, the US economy is falling.\n",
            "\n",
            "It is a good thing to see what happened on the US economy. The US economy, which is very close to the US economy, is now rising and the US economy is already the world’s most vulnerable.\n",
            "\n",
            "The US economy is not just in the US, but in the last decade Europe is at its most vulnerable, as it has been for a very long time. And while the US economy is very weak in the US economy economy in the US, it is still not strong enough to make that economy worse.<|endoftext|>As the Philippines announced last week, the South Korean state of South Korea’s President Chaear Valley is preparing a state of emergency.\n",
            "\n",
            "The US, which is headed by the UN’s Ministry of Health, has decided to make plans to do so by sending letters to its citizens.\n",
            "\n",
            "The UN’s Office of National Health, which is headed by Prime Minister Shinzo Abe, is expected to speak to reporters when meeting a bilateral meeting between the two nations of South Korea and the US.\n",
            "\n",
            "The US and the US, China, the\n",
            "---------------\n",
            "How to join a tbank-research? Our best local data center is the largest economy, from the Central Bank and the Bank of America. It can provide more stable, more efficient, and better-developed products.\n",
            "\n",
            "You can check out our “Bitcoin Market” here.\n",
            "\n",
            "For more information about the market, click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for the table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click in that spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for an spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on the charts.\n",
            "\n",
            "Click there for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for it on a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on here for a spreadsheet.\n",
            "\n",
            "Click here for a table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click right for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click to see a list.\n",
            "\n",
            "Click here for a visualization.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click\n",
            "---------------\n",
            "How to join a tbank-research? Share: Send us a feed.\n",
            "\n",
            "About 2,000+ of our customers will receive about £7 to £4.50.\n",
            "\n",
            "Most of the over 50,000 customers will receive £4.50.\n",
            "\n",
            "“People who need a clean-up,” said Chris Harwinger, founder of the G2C board. “For example, they need to be able to do what they need to do.\n",
            "\n",
            "“When you got all the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money they want, the money they want.\n",
            "\n",
            "“I think we get the money you want, the money you want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, and the money their money they want isn’t being invested in.\n",
            "\n",
            "“We are very much invested in the money they want, the money they want. They want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want.”\n",
            "\n",
            "The cost of this scheme is a huge amount of money you can want.\n",
            "\n",
            "“I wanted so many people working in this small city. They are very wealthy people, I think, we can make a lot of money and it is a nice place to stay in the city and start with the money they want. But they are very wealthy people, they want.”\n",
            "\n",
            "Of the $1 million people in the city are from the public, so some of this is not a big deal but a lot of people have gotten so far.\n",
            "\n",
            "Over the past two years, we have seen only about 1,000 people living in the town of Qee. We have seen about 1,000 people working in the city, six million people working in the city, and 20 million people working in the city. We have seen over\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFxAUJDcUCYT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjURIxWLcC45"
      },
      "source": [
        "## GPT: Reflex-Router-Attention (6)\n",
        "\n",
        "**2 layer: SA for 6 head**\n",
        "\n",
        "**3-6 layer: Reflex-Router-Attention for 4 head for all previous hidden states: fitting linear combination; SA for 2 head (const for all layer)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt5t-TNmcC46"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "\n",
        "class Router(nn.Module):\n",
        "  def __init__(self, n_head, i_layer):\n",
        "    super().__init__()\n",
        "    self.weights = nn.ParameterDict({str(i): nn.Parameter(torch.randn(i_layer)) for i in range(n_head - 2)}) #.to(device) #cuda\n",
        "\n",
        "  def forward(self, k_vs, head_i):\n",
        "    return sum(weight * k_v for weight, k_v in zip(self.weights[str(head_i)], k_vs))\n",
        "\n",
        "\n",
        "class ReflexRouterAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.router = Router(config.n_head, i_layer + 1)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        #i_hidden_state:\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        #reflex!\n",
        "        if self.model_type == 'simple' or (self.model_type == 'reflex' and hidden_states.__len__() < 3):\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v,\n",
        "                                                                 attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        else:\n",
        "            attn = []\n",
        "            for i in range(self.n_head - 2):\n",
        "              # for i's head (i <= 4) CA Router linear combination of existing hidden states from all prev layer and also current!\n",
        "              q_i, k_i, v_i = q[:, i,:, :], self.router([hidden_state[0][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i), self.router([hidden_state[1][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i)\n",
        "              attn.append(torch.nn.functional.scaled_dot_product_attention(q_i, k_i, v_i, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "              # del q_i, k_i, v_i\n",
        "            #SA two heads\n",
        "            attn.append(torch.nn.functional.scaled_dot_product_attention(q[:, 4, :, :], k[:, 4, :, :], v[:, 4, :, :], attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "            attn.append(torch.nn.functional.scaled_dot_product_attention(q[:, 5, :, :], k[:, 5, :, :], v[:, 5, :, :], attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "            y = torch.cat(attn, dim=1)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y, (k, v)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = ReflexRouterAttention(config, i_layer)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        attn_kv = self.attn(self.ln_1(x), hidden_states)\n",
        "        x = x + attn_kv[0] #y for 0\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_kv[1] #hidden_states_i for k,v for 1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config, _) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        hidden_states = []\n",
        "\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x, hidden_state = block(x, hidden_states)\n",
        "\n",
        "            # level2: for router\n",
        "            if self.config.model_type == 'reflex':\n",
        "                hidden_states.append(hidden_state)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "RflTE2zicC46",
        "outputId": "144239d3-d7eb-4f5c-deaf-ac2ea67b66a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ],
            "text/plain": [
              "__main__.GPT"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "021oJlL4cC47"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'reflex' # or 'simple'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyMFf5hucC47",
        "outputId": "3172d20d-5d09-499f-eae6-960609936bef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFcKzbBIcC47"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'reflexAttnGPT_with_router+2headSA={config}_1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcGxZbkscC47"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/reglex_attn_GPT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ61TqCtcC48",
        "outputId": "63907eb1-9289-43e7-b379-ce2f426c91b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokens per iteration will be: 32,768\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ET5oanjAcC48"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch8P72VvcC48",
        "outputId": "28d5fd97-3c8d-4f9a-c303-717c70de4832"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnTRjNKocC49",
        "outputId": "0d94c7b0-c14e-46b0-ba76-4c84bccd5931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 37, with 10,068 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-29-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8C5cUSM0cC49",
        "outputId": "8e0bc702-e844-471d-c657-08c63335423b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:r2ep0c6j) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">reflexAttnGPT_with_router+2headSA=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')_1</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/r2ep0c6j' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/r2ep0c6j</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241125_224058-r2ep0c6j/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:r2ep0c6j). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241125_224322-5aiil301</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/5aiil301' target=\"_blank\">reflexAttnGPT_with_router+2headSA=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')_1</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/5aiil301' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/5aiil301</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-20-68d847b1ab55> line 155 \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:40.783000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-20-68d847b1ab55> line 102 \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.209000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-20-68d847b1ab55> line 17 \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.298000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-20-68d847b1ab55> line 45 \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.514000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-20-68d847b1ab55> line 86 \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.663000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-20-68d847b1ab55> line 26 \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1125 22:43:41.811000 3512 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: train loss 11.0057, val loss 11.0071\n",
            "iter 0: loss 11.0027, time 45654.23ms, mfu -100.00%\n",
            "iter 10: loss 9.9044, time 256.80ms, mfu 22.22%\n",
            "iter 20: loss 9.2929, time 256.78ms, mfu 22.22%\n",
            "iter 30: loss 8.6891, time 255.88ms, mfu 22.23%\n",
            "iter 40: loss 8.0162, time 256.50ms, mfu 22.23%\n",
            "iter 50: loss 7.5290, time 256.73ms, mfu 22.23%\n",
            "iter 60: loss 7.4738, time 256.99ms, mfu 22.23%\n",
            "iter 70: loss 6.9618, time 256.72ms, mfu 22.23%\n",
            "iter 80: loss 7.1177, time 256.12ms, mfu 22.23%\n",
            "iter 90: loss 6.7267, time 256.58ms, mfu 22.23%\n",
            "iter 100: loss 6.6683, time 257.15ms, mfu 22.23%\n",
            "iter 110: loss 6.5046, time 257.60ms, mfu 22.22%\n",
            "iter 120: loss 6.4768, time 256.52ms, mfu 22.22%\n",
            "iter 130: loss 6.6146, time 257.91ms, mfu 22.21%\n",
            "iter 140: loss 6.4848, time 256.95ms, mfu 22.21%\n",
            "iter 150: loss 6.4060, time 256.69ms, mfu 22.21%\n",
            "iter 160: loss 6.2394, time 256.54ms, mfu 22.22%\n",
            "iter 170: loss 6.3807, time 257.64ms, mfu 22.21%\n",
            "iter 180: loss 6.2057, time 257.64ms, mfu 22.20%\n",
            "iter 190: loss 6.5093, time 256.95ms, mfu 22.20%\n",
            "iter 200: loss 6.3163, time 257.95ms, mfu 22.20%\n",
            "iter 210: loss 6.1475, time 257.10ms, mfu 22.19%\n",
            "iter 220: loss 6.1076, time 257.42ms, mfu 22.19%\n",
            "iter 230: loss 6.0458, time 256.40ms, mfu 22.20%\n",
            "iter 240: loss 6.2824, time 257.06ms, mfu 22.20%\n",
            "iter 250: loss 6.0714, time 257.03ms, mfu 22.20%\n",
            "iter 260: loss 5.9862, time 257.51ms, mfu 22.19%\n",
            "iter 270: loss 6.2384, time 256.68ms, mfu 22.20%\n",
            "iter 280: loss 5.9807, time 256.80ms, mfu 22.20%\n",
            "iter 290: loss 6.0401, time 257.59ms, mfu 22.20%\n",
            "step 300: train loss 6.0389, val loss 5.9903\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 300: loss 6.1192, time 35951.53ms, mfu 19.99%\n",
            "iter 310: loss 6.0551, time 256.31ms, mfu 20.22%\n",
            "iter 320: loss 5.9837, time 256.79ms, mfu 20.42%\n",
            "iter 330: loss 5.9159, time 256.31ms, mfu 20.60%\n",
            "iter 340: loss 5.9406, time 256.41ms, mfu 20.77%\n",
            "iter 350: loss 5.9310, time 256.72ms, mfu 20.91%\n",
            "iter 360: loss 5.9669, time 257.15ms, mfu 21.04%\n",
            "iter 370: loss 5.9974, time 257.19ms, mfu 21.16%\n",
            "iter 380: loss 5.9236, time 257.19ms, mfu 21.26%\n",
            "iter 390: loss 5.9462, time 256.49ms, mfu 21.36%\n",
            "iter 400: loss 5.7770, time 256.79ms, mfu 21.44%\n",
            "iter 410: loss 5.8660, time 256.89ms, mfu 21.52%\n",
            "iter 420: loss 5.8036, time 256.99ms, mfu 21.59%\n",
            "iter 430: loss 6.0004, time 257.31ms, mfu 21.65%\n",
            "iter 440: loss 5.8670, time 257.77ms, mfu 21.70%\n",
            "iter 450: loss 5.8937, time 256.74ms, mfu 21.75%\n",
            "iter 460: loss 5.7261, time 256.89ms, mfu 21.80%\n",
            "iter 470: loss 5.8968, time 257.18ms, mfu 21.83%\n",
            "iter 480: loss 5.7871, time 256.34ms, mfu 21.88%\n",
            "iter 490: loss 5.8108, time 256.84ms, mfu 21.91%\n",
            "iter 500: loss 5.7337, time 257.10ms, mfu 21.94%\n",
            "iter 510: loss 5.6481, time 256.91ms, mfu 21.97%\n",
            "iter 520: loss 5.7223, time 257.14ms, mfu 21.99%\n",
            "iter 530: loss 5.7400, time 256.64ms, mfu 22.01%\n",
            "iter 540: loss 5.5314, time 256.25ms, mfu 22.04%\n",
            "iter 550: loss 5.6445, time 257.55ms, mfu 22.05%\n",
            "iter 560: loss 5.5095, time 256.86ms, mfu 22.07%\n",
            "iter 570: loss 5.5559, time 256.54ms, mfu 22.08%\n",
            "iter 580: loss 5.4511, time 256.82ms, mfu 22.10%\n",
            "iter 590: loss 5.5545, time 256.79ms, mfu 22.11%\n",
            "step 600: train loss 5.5223, val loss 5.4800\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 600: loss 5.3843, time 36744.05ms, mfu 19.91%\n",
            "iter 610: loss 5.4093, time 257.32ms, mfu 20.14%\n",
            "iter 620: loss 5.3616, time 256.41ms, mfu 20.35%\n",
            "iter 630: loss 5.4606, time 256.96ms, mfu 20.54%\n",
            "iter 640: loss 5.5034, time 257.36ms, mfu 20.70%\n",
            "iter 650: loss 5.4036, time 256.72ms, mfu 20.85%\n",
            "iter 660: loss 5.6267, time 256.61ms, mfu 20.99%\n",
            "iter 670: loss 5.3351, time 257.47ms, mfu 21.11%\n",
            "iter 680: loss 5.3515, time 257.20ms, mfu 21.22%\n",
            "iter 690: loss 5.2995, time 256.44ms, mfu 21.32%\n",
            "iter 700: loss 5.4677, time 257.03ms, mfu 21.41%\n",
            "iter 710: loss 5.4547, time 256.37ms, mfu 21.49%\n",
            "iter 720: loss 5.2788, time 256.20ms, mfu 21.57%\n",
            "iter 730: loss 5.2782, time 256.18ms, mfu 21.64%\n",
            "iter 740: loss 5.4415, time 257.17ms, mfu 21.70%\n",
            "iter 750: loss 5.2906, time 256.89ms, mfu 21.75%\n",
            "iter 760: loss 5.3805, time 256.76ms, mfu 21.79%\n",
            "iter 770: loss 5.1526, time 256.59ms, mfu 21.84%\n",
            "iter 780: loss 5.1925, time 257.26ms, mfu 21.87%\n",
            "iter 790: loss 5.2740, time 257.60ms, mfu 21.90%\n",
            "iter 800: loss 5.2007, time 256.27ms, mfu 21.94%\n",
            "iter 810: loss 5.2231, time 256.93ms, mfu 21.96%\n",
            "iter 820: loss 5.2514, time 257.42ms, mfu 21.98%\n",
            "iter 830: loss 5.0445, time 257.12ms, mfu 22.01%\n",
            "iter 840: loss 5.3314, time 256.78ms, mfu 22.03%\n",
            "iter 850: loss 5.2669, time 257.29ms, mfu 22.04%\n",
            "iter 860: loss 5.1640, time 257.92ms, mfu 22.05%\n",
            "iter 870: loss 5.2696, time 257.02ms, mfu 22.06%\n",
            "iter 880: loss 5.1297, time 257.03ms, mfu 22.08%\n",
            "iter 890: loss 5.1795, time 257.49ms, mfu 22.09%\n",
            "step 900: train loss 5.1452, val loss 5.1023\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 900: loss 5.1844, time 36640.84ms, mfu 19.89%\n",
            "iter 910: loss 5.2760, time 256.77ms, mfu 20.13%\n",
            "iter 920: loss 5.2520, time 256.84ms, mfu 20.34%\n",
            "iter 930: loss 5.0430, time 257.59ms, mfu 20.52%\n",
            "iter 940: loss 5.1215, time 256.18ms, mfu 20.69%\n",
            "iter 950: loss 5.1390, time 257.06ms, mfu 20.84%\n",
            "iter 960: loss 5.0545, time 257.81ms, mfu 20.97%\n",
            "iter 970: loss 4.9391, time 256.35ms, mfu 21.10%\n",
            "iter 980: loss 4.9509, time 257.36ms, mfu 21.21%\n",
            "iter 990: loss 5.0773, time 257.38ms, mfu 21.30%\n",
            "iter 1000: loss 5.0446, time 256.20ms, mfu 21.40%\n",
            "iter 1010: loss 5.1274, time 257.11ms, mfu 21.48%\n",
            "iter 1020: loss 4.9960, time 257.54ms, mfu 21.55%\n",
            "iter 1030: loss 5.0838, time 256.59ms, mfu 21.62%\n",
            "iter 1040: loss 5.0927, time 256.82ms, mfu 21.68%\n",
            "iter 1050: loss 4.9820, time 256.71ms, mfu 21.73%\n",
            "iter 1060: loss 5.0233, time 257.02ms, mfu 21.78%\n",
            "iter 1070: loss 4.9007, time 256.63ms, mfu 21.82%\n",
            "iter 1080: loss 5.0071, time 256.90ms, mfu 21.86%\n",
            "iter 1090: loss 4.9080, time 256.87ms, mfu 21.90%\n",
            "iter 1100: loss 4.9856, time 256.88ms, mfu 21.93%\n",
            "iter 1110: loss 4.8885, time 256.82ms, mfu 21.96%\n",
            "iter 1120: loss 4.7604, time 257.32ms, mfu 21.98%\n",
            "iter 1130: loss 5.0028, time 256.78ms, mfu 22.00%\n",
            "iter 1140: loss 4.9587, time 256.50ms, mfu 22.03%\n",
            "iter 1150: loss 5.0319, time 257.38ms, mfu 22.04%\n",
            "iter 1160: loss 4.9637, time 257.18ms, mfu 22.06%\n",
            "iter 1170: loss 4.9506, time 257.00ms, mfu 22.07%\n",
            "iter 1180: loss 4.8832, time 256.67ms, mfu 22.09%\n",
            "iter 1190: loss 4.8491, time 257.85ms, mfu 22.09%\n",
            "step 1200: train loss 4.8628, val loss 4.8331\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1200: loss 4.8507, time 36379.92ms, mfu 19.90%\n",
            "iter 1210: loss 4.8926, time 256.42ms, mfu 20.13%\n",
            "iter 1220: loss 4.8520, time 256.97ms, mfu 20.34%\n",
            "iter 1230: loss 4.9114, time 256.97ms, mfu 20.53%\n",
            "iter 1240: loss 4.9372, time 256.74ms, mfu 20.70%\n",
            "iter 1250: loss 4.7926, time 256.92ms, mfu 20.85%\n",
            "iter 1260: loss 4.7692, time 256.80ms, mfu 20.98%\n",
            "iter 1270: loss 4.7679, time 256.06ms, mfu 21.11%\n",
            "iter 1280: loss 4.8088, time 257.12ms, mfu 21.22%\n",
            "iter 1290: loss 4.7727, time 257.38ms, mfu 21.32%\n",
            "iter 1300: loss 4.7622, time 256.53ms, mfu 21.41%\n",
            "iter 1310: loss 4.7619, time 256.67ms, mfu 21.49%\n",
            "iter 1320: loss 4.8265, time 257.19ms, mfu 21.56%\n",
            "iter 1330: loss 4.7471, time 256.52ms, mfu 21.63%\n",
            "iter 1340: loss 4.8082, time 257.44ms, mfu 21.68%\n",
            "iter 1350: loss 4.7170, time 257.86ms, mfu 21.73%\n",
            "iter 1360: loss 4.7756, time 256.48ms, mfu 21.78%\n",
            "iter 1370: loss 4.7652, time 256.87ms, mfu 21.82%\n",
            "iter 1380: loss 4.7309, time 257.61ms, mfu 21.86%\n",
            "iter 1390: loss 4.6633, time 255.82ms, mfu 21.90%\n",
            "iter 1400: loss 4.7468, time 256.80ms, mfu 21.93%\n",
            "iter 1410: loss 4.7156, time 257.77ms, mfu 21.95%\n",
            "iter 1420: loss 4.6610, time 256.86ms, mfu 21.98%\n",
            "iter 1430: loss 4.7214, time 257.31ms, mfu 22.00%\n",
            "iter 1440: loss 4.6057, time 257.09ms, mfu 22.02%\n",
            "iter 1450: loss 4.7167, time 256.59ms, mfu 22.04%\n",
            "iter 1460: loss 4.6809, time 256.89ms, mfu 22.06%\n",
            "iter 1470: loss 4.5844, time 257.00ms, mfu 22.07%\n",
            "iter 1480: loss 4.6552, time 256.88ms, mfu 22.09%\n",
            "iter 1490: loss 4.6431, time 257.54ms, mfu 22.09%\n",
            "step 1500: train loss 4.6629, val loss 4.6374\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1500: loss 4.6858, time 36437.60ms, mfu 19.90%\n",
            "iter 1510: loss 4.8238, time 257.75ms, mfu 20.12%\n",
            "iter 1520: loss 4.7350, time 256.95ms, mfu 20.33%\n",
            "iter 1530: loss 4.7592, time 257.21ms, mfu 20.52%\n",
            "iter 1540: loss 4.7586, time 257.93ms, mfu 20.68%\n",
            "iter 1550: loss 4.6274, time 256.84ms, mfu 20.83%\n",
            "iter 1560: loss 4.7507, time 256.80ms, mfu 20.97%\n",
            "iter 1570: loss 4.5627, time 258.29ms, mfu 21.08%\n",
            "iter 1580: loss 4.6744, time 256.89ms, mfu 21.19%\n",
            "iter 1590: loss 4.5987, time 257.00ms, mfu 21.30%\n",
            "iter 1600: loss 4.6190, time 257.08ms, mfu 21.39%\n",
            "iter 1610: loss 4.6888, time 256.94ms, mfu 21.47%\n",
            "iter 1620: loss 4.6728, time 257.12ms, mfu 21.54%\n",
            "iter 1630: loss 4.7422, time 256.27ms, mfu 21.61%\n",
            "iter 1640: loss 4.5727, time 256.50ms, mfu 21.68%\n",
            "iter 1650: loss 4.6256, time 257.33ms, mfu 21.73%\n",
            "iter 1660: loss 4.6232, time 257.15ms, mfu 21.77%\n",
            "iter 1670: loss 4.4652, time 256.58ms, mfu 21.82%\n",
            "iter 1680: loss 4.5948, time 256.66ms, mfu 21.86%\n",
            "iter 1690: loss 4.5383, time 257.37ms, mfu 21.89%\n",
            "iter 1700: loss 4.5701, time 256.74ms, mfu 21.92%\n",
            "iter 1710: loss 4.5461, time 257.12ms, mfu 21.95%\n",
            "iter 1720: loss 4.5660, time 257.58ms, mfu 21.97%\n",
            "iter 1730: loss 4.5369, time 256.78ms, mfu 22.00%\n",
            "iter 1740: loss 4.5142, time 257.37ms, mfu 22.01%\n",
            "iter 1750: loss 4.5630, time 257.12ms, mfu 22.03%\n",
            "iter 1760: loss 4.5748, time 257.06ms, mfu 22.05%\n",
            "iter 1770: loss 4.5198, time 257.87ms, mfu 22.06%\n",
            "iter 1780: loss 4.4624, time 256.76ms, mfu 22.07%\n",
            "iter 1790: loss 4.4994, time 257.18ms, mfu 22.08%\n",
            "step 1800: train loss 4.5226, val loss 4.5138\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1800: loss 4.5730, time 36408.06ms, mfu 19.89%\n",
            "iter 1810: loss 4.5601, time 257.01ms, mfu 20.12%\n",
            "iter 1820: loss 4.4815, time 256.62ms, mfu 20.33%\n",
            "iter 1830: loss 4.4569, time 257.50ms, mfu 20.52%\n",
            "iter 1840: loss 4.4538, time 256.59ms, mfu 20.69%\n",
            "iter 1850: loss 4.4666, time 257.55ms, mfu 20.83%\n",
            "iter 1860: loss 4.5622, time 256.79ms, mfu 20.97%\n",
            "iter 1870: loss 4.6034, time 256.65ms, mfu 21.10%\n",
            "iter 1880: loss 4.4430, time 256.83ms, mfu 21.21%\n",
            "iter 1890: loss 4.5927, time 257.29ms, mfu 21.31%\n",
            "iter 1900: loss 4.5503, time 256.60ms, mfu 21.40%\n",
            "iter 1910: loss 4.5182, time 257.30ms, mfu 21.48%\n",
            "iter 1920: loss 4.3711, time 256.90ms, mfu 21.55%\n",
            "iter 1930: loss 4.4466, time 257.55ms, mfu 21.61%\n",
            "iter 1940: loss 4.4688, time 257.26ms, mfu 21.67%\n",
            "iter 1950: loss 4.3859, time 256.96ms, mfu 21.72%\n",
            "iter 1960: loss 4.3765, time 256.25ms, mfu 21.78%\n",
            "iter 1970: loss 4.4394, time 257.24ms, mfu 21.82%\n",
            "iter 1980: loss 4.5154, time 256.83ms, mfu 21.86%\n",
            "iter 1990: loss 4.4596, time 257.01ms, mfu 21.89%\n",
            "iter 2000: loss 4.4609, time 257.93ms, mfu 21.91%\n",
            "iter 2010: loss 4.5587, time 256.47ms, mfu 21.95%\n",
            "iter 2020: loss 4.4820, time 256.95ms, mfu 21.97%\n",
            "iter 2030: loss 4.4349, time 257.62ms, mfu 21.99%\n",
            "iter 2040: loss 4.5758, time 257.14ms, mfu 22.01%\n",
            "iter 2050: loss 4.5378, time 256.69ms, mfu 22.03%\n",
            "iter 2060: loss 4.4199, time 257.32ms, mfu 22.05%\n",
            "iter 2070: loss 4.5177, time 257.25ms, mfu 22.06%\n",
            "iter 2080: loss 4.4490, time 257.16ms, mfu 22.07%\n",
            "iter 2090: loss 4.4160, time 257.40ms, mfu 22.08%\n",
            "step 2100: train loss 4.4284, val loss 4.4171\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2100: loss 4.4906, time 36395.37ms, mfu 19.89%\n",
            "iter 2110: loss 4.4640, time 257.37ms, mfu 20.12%\n",
            "iter 2120: loss 4.4130, time 256.55ms, mfu 20.33%\n",
            "iter 2130: loss 4.4507, time 257.02ms, mfu 20.52%\n",
            "iter 2140: loss 4.2958, time 256.98ms, mfu 20.69%\n",
            "iter 2150: loss 4.5338, time 257.78ms, mfu 20.83%\n",
            "iter 2160: loss 4.3923, time 257.31ms, mfu 20.97%\n",
            "iter 2170: loss 4.4219, time 256.52ms, mfu 21.09%\n",
            "iter 2180: loss 4.3836, time 256.82ms, mfu 21.21%\n",
            "iter 2190: loss 4.4276, time 257.77ms, mfu 21.30%\n",
            "iter 2200: loss 4.5391, time 256.33ms, mfu 21.39%\n",
            "iter 2210: loss 4.4696, time 256.69ms, mfu 21.48%\n",
            "iter 2220: loss 4.2120, time 256.84ms, mfu 21.55%\n",
            "iter 2230: loss 4.3487, time 256.96ms, mfu 21.62%\n",
            "iter 2240: loss 4.2611, time 257.90ms, mfu 21.67%\n",
            "iter 2250: loss 4.3565, time 257.29ms, mfu 21.72%\n",
            "iter 2260: loss 4.4097, time 256.63ms, mfu 21.77%\n",
            "iter 2270: loss 4.3449, time 257.22ms, mfu 21.81%\n",
            "iter 2280: loss 4.2991, time 257.65ms, mfu 21.85%\n",
            "iter 2290: loss 4.3758, time 256.67ms, mfu 21.88%\n",
            "iter 2300: loss 4.2722, time 256.72ms, mfu 21.92%\n",
            "iter 2310: loss 4.3817, time 257.20ms, mfu 21.94%\n",
            "iter 2320: loss 4.2303, time 256.42ms, mfu 21.98%\n",
            "iter 2330: loss 4.4971, time 256.80ms, mfu 22.00%\n",
            "iter 2340: loss 4.3102, time 257.07ms, mfu 22.02%\n",
            "iter 2350: loss 4.3790, time 257.22ms, mfu 22.04%\n",
            "iter 2360: loss 4.3736, time 258.78ms, mfu 22.04%\n",
            "iter 2370: loss 4.3722, time 257.34ms, mfu 22.05%\n",
            "iter 2380: loss 4.3406, time 256.61ms, mfu 22.07%\n",
            "iter 2390: loss 4.2519, time 256.67ms, mfu 22.09%\n",
            "step 2400: train loss 4.3434, val loss 4.3223\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2400: loss 4.3520, time 36416.80ms, mfu 19.89%\n",
            "iter 2410: loss 4.3811, time 257.56ms, mfu 20.12%\n",
            "iter 2420: loss 4.2171, time 256.87ms, mfu 20.33%\n",
            "iter 2430: loss 4.2532, time 257.18ms, mfu 20.51%\n",
            "iter 2440: loss 4.2422, time 257.39ms, mfu 20.68%\n",
            "iter 2450: loss 4.3665, time 256.54ms, mfu 20.84%\n",
            "iter 2460: loss 4.3311, time 256.75ms, mfu 20.97%\n",
            "iter 2470: loss 4.2565, time 257.24ms, mfu 21.10%\n",
            "iter 2480: loss 4.2763, time 257.37ms, mfu 21.20%\n",
            "iter 2490: loss 4.4258, time 257.51ms, mfu 21.30%\n",
            "iter 2500: loss 4.2729, time 258.02ms, mfu 21.38%\n",
            "iter 2510: loss 4.2898, time 257.91ms, mfu 21.45%\n",
            "iter 2520: loss 4.3092, time 256.90ms, mfu 21.53%\n",
            "iter 2530: loss 4.3242, time 256.80ms, mfu 21.60%\n",
            "iter 2540: loss 4.3477, time 257.08ms, mfu 21.66%\n",
            "iter 2550: loss 4.3146, time 257.58ms, mfu 21.71%\n",
            "iter 2560: loss 4.3050, time 257.04ms, mfu 21.76%\n",
            "iter 2570: loss 4.2977, time 257.07ms, mfu 21.80%\n",
            "iter 2580: loss 4.3378, time 257.52ms, mfu 21.84%\n",
            "iter 2590: loss 4.3244, time 256.10ms, mfu 21.88%\n",
            "iter 2600: loss 4.3699, time 256.21ms, mfu 21.92%\n",
            "iter 2610: loss 4.2663, time 257.85ms, mfu 21.94%\n",
            "iter 2620: loss 4.1426, time 256.42ms, mfu 21.97%\n",
            "iter 2630: loss 4.4109, time 257.24ms, mfu 21.99%\n",
            "iter 2640: loss 4.2571, time 257.80ms, mfu 22.01%\n",
            "iter 2650: loss 4.2975, time 256.31ms, mfu 22.03%\n",
            "iter 2660: loss 4.2445, time 257.86ms, mfu 22.04%\n",
            "iter 2670: loss 4.4051, time 257.88ms, mfu 22.05%\n",
            "iter 2680: loss 4.2704, time 256.64ms, mfu 22.07%\n",
            "iter 2690: loss 4.1210, time 256.93ms, mfu 22.08%\n",
            "step 2700: train loss 4.2620, val loss 4.2554\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2700: loss 4.1920, time 36467.87ms, mfu 19.89%\n",
            "iter 2710: loss 4.2367, time 256.94ms, mfu 20.12%\n",
            "iter 2720: loss 4.1219, time 256.80ms, mfu 20.33%\n",
            "iter 2730: loss 4.3312, time 258.20ms, mfu 20.51%\n",
            "iter 2740: loss 4.2525, time 256.96ms, mfu 20.68%\n",
            "iter 2750: loss 4.3671, time 257.33ms, mfu 20.83%\n",
            "iter 2760: loss 4.3030, time 257.82ms, mfu 20.96%\n",
            "iter 2770: loss 4.1795, time 256.74ms, mfu 21.08%\n",
            "iter 2780: loss 4.2698, time 257.79ms, mfu 21.19%\n",
            "iter 2790: loss 4.3633, time 258.70ms, mfu 21.28%\n",
            "iter 2800: loss 4.2543, time 256.19ms, mfu 21.38%\n",
            "iter 2810: loss 4.3018, time 257.70ms, mfu 21.45%\n",
            "iter 2820: loss 4.1495, time 256.98ms, mfu 21.53%\n",
            "iter 2830: loss 4.3081, time 256.81ms, mfu 21.60%\n",
            "iter 2840: loss 4.2724, time 256.22ms, mfu 21.66%\n",
            "iter 2850: loss 4.1798, time 257.80ms, mfu 21.71%\n",
            "iter 2860: loss 4.2567, time 257.37ms, mfu 21.76%\n",
            "iter 2870: loss 4.2244, time 257.43ms, mfu 21.80%\n",
            "iter 2880: loss 4.3373, time 256.79ms, mfu 21.84%\n",
            "iter 2890: loss 4.2230, time 256.28ms, mfu 21.88%\n",
            "iter 2900: loss 4.2322, time 256.53ms, mfu 21.92%\n",
            "iter 2910: loss 4.1385, time 256.89ms, mfu 21.95%\n",
            "iter 2920: loss 4.1801, time 258.06ms, mfu 21.96%\n",
            "iter 2930: loss 4.1766, time 256.34ms, mfu 21.99%\n",
            "iter 2940: loss 4.1805, time 256.85ms, mfu 22.02%\n",
            "iter 2950: loss 4.2450, time 257.23ms, mfu 22.03%\n",
            "iter 2960: loss 4.2146, time 256.38ms, mfu 22.05%\n",
            "iter 2970: loss 4.1936, time 256.88ms, mfu 22.07%\n",
            "iter 2980: loss 4.1287, time 257.70ms, mfu 22.08%\n",
            "iter 2990: loss 4.1369, time 257.35ms, mfu 22.09%\n",
            "step 3000: train loss 4.2009, val loss 4.1974\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3000: loss 4.3838, time 36190.58ms, mfu 19.89%\n",
            "iter 3010: loss 4.2876, time 257.71ms, mfu 20.12%\n",
            "iter 3020: loss 4.2714, time 257.10ms, mfu 20.33%\n",
            "iter 3030: loss 4.2564, time 257.58ms, mfu 20.51%\n",
            "iter 3040: loss 4.2062, time 257.68ms, mfu 20.67%\n",
            "iter 3050: loss 4.1534, time 256.78ms, mfu 20.83%\n",
            "iter 3060: loss 4.2544, time 257.36ms, mfu 20.96%\n",
            "iter 3070: loss 4.1464, time 257.61ms, mfu 21.08%\n",
            "iter 3080: loss 4.0876, time 257.05ms, mfu 21.19%\n",
            "iter 3090: loss 4.1815, time 258.02ms, mfu 21.28%\n",
            "iter 3100: loss 4.1410, time 257.23ms, mfu 21.37%\n",
            "iter 3110: loss 4.1008, time 256.83ms, mfu 21.46%\n",
            "iter 3120: loss 4.1975, time 257.21ms, mfu 21.53%\n",
            "iter 3130: loss 4.2019, time 257.59ms, mfu 21.59%\n",
            "iter 3140: loss 4.1969, time 256.63ms, mfu 21.66%\n",
            "iter 3150: loss 4.1498, time 257.38ms, mfu 21.71%\n",
            "iter 3160: loss 4.0778, time 257.13ms, mfu 21.76%\n",
            "iter 3170: loss 4.2571, time 257.76ms, mfu 21.79%\n",
            "iter 3180: loss 4.1335, time 256.79ms, mfu 21.84%\n",
            "iter 3190: loss 4.2008, time 257.21ms, mfu 21.87%\n",
            "iter 3200: loss 4.1928, time 257.40ms, mfu 21.90%\n",
            "iter 3210: loss 4.1626, time 256.91ms, mfu 21.93%\n",
            "iter 3220: loss 4.1958, time 257.98ms, mfu 21.95%\n",
            "iter 3230: loss 4.3164, time 257.34ms, mfu 21.97%\n",
            "iter 3240: loss 4.0396, time 256.67ms, mfu 22.00%\n",
            "iter 3250: loss 4.2228, time 257.38ms, mfu 22.02%\n",
            "iter 3260: loss 4.1954, time 256.85ms, mfu 22.04%\n",
            "iter 3270: loss 4.1723, time 257.29ms, mfu 22.05%\n",
            "iter 3280: loss 4.0759, time 257.86ms, mfu 22.06%\n",
            "iter 3290: loss 4.0806, time 256.78ms, mfu 22.07%\n",
            "step 3300: train loss 4.1471, val loss 4.1340\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3300: loss 4.2972, time 36504.43ms, mfu 19.88%\n",
            "iter 3310: loss 4.1966, time 257.03ms, mfu 20.11%\n",
            "iter 3320: loss 4.2364, time 256.89ms, mfu 20.32%\n",
            "iter 3330: loss 4.1463, time 257.39ms, mfu 20.51%\n",
            "iter 3340: loss 4.1641, time 256.71ms, mfu 20.68%\n",
            "iter 3350: loss 4.1841, time 257.10ms, mfu 20.83%\n",
            "iter 3360: loss 4.1401, time 257.52ms, mfu 20.96%\n",
            "iter 3370: loss 4.0710, time 257.93ms, mfu 21.08%\n",
            "iter 3380: loss 4.2432, time 257.25ms, mfu 21.19%\n",
            "iter 3390: loss 4.1133, time 257.34ms, mfu 21.29%\n",
            "iter 3400: loss 4.0948, time 256.69ms, mfu 21.38%\n",
            "iter 3410: loss 4.1753, time 257.30ms, mfu 21.46%\n",
            "iter 3420: loss 4.0428, time 257.60ms, mfu 21.53%\n",
            "iter 3430: loss 4.1223, time 256.64ms, mfu 21.60%\n",
            "iter 3440: loss 4.2037, time 257.20ms, mfu 21.66%\n",
            "iter 3450: loss 4.0858, time 257.53ms, mfu 21.71%\n",
            "iter 3460: loss 4.1606, time 256.59ms, mfu 21.76%\n",
            "iter 3470: loss 4.1106, time 256.38ms, mfu 21.81%\n",
            "iter 3480: loss 4.1328, time 257.44ms, mfu 21.85%\n",
            "iter 3490: loss 4.2170, time 257.37ms, mfu 21.88%\n",
            "iter 3500: loss 4.0457, time 257.26ms, mfu 21.91%\n",
            "iter 3510: loss 4.0506, time 256.77ms, mfu 21.94%\n",
            "iter 3520: loss 4.0926, time 256.80ms, mfu 21.97%\n",
            "iter 3530: loss 4.1732, time 257.24ms, mfu 21.99%\n",
            "iter 3540: loss 4.1514, time 256.65ms, mfu 22.01%\n",
            "iter 3550: loss 4.1933, time 257.70ms, mfu 22.03%\n",
            "iter 3560: loss 4.1076, time 257.14ms, mfu 22.04%\n",
            "iter 3570: loss 4.1690, time 257.57ms, mfu 22.05%\n",
            "iter 3580: loss 4.0069, time 256.64ms, mfu 22.07%\n",
            "iter 3590: loss 4.1251, time 256.74ms, mfu 22.09%\n",
            "step 3600: train loss 4.0932, val loss 4.0899\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3600: loss 4.1413, time 36420.58ms, mfu 19.89%\n",
            "iter 3610: loss 4.0337, time 256.69ms, mfu 20.13%\n",
            "iter 3620: loss 4.1345, time 257.21ms, mfu 20.33%\n",
            "iter 3630: loss 4.1021, time 256.61ms, mfu 20.52%\n",
            "iter 3640: loss 4.1322, time 257.87ms, mfu 20.68%\n",
            "iter 3650: loss 4.1682, time 256.56ms, mfu 20.84%\n",
            "iter 3660: loss 4.0649, time 257.18ms, mfu 20.97%\n",
            "iter 3670: loss 3.9958, time 257.07ms, mfu 21.10%\n",
            "iter 3680: loss 4.1871, time 257.28ms, mfu 21.20%\n",
            "iter 3690: loss 4.0056, time 257.33ms, mfu 21.30%\n",
            "iter 3700: loss 3.9838, time 256.74ms, mfu 21.39%\n",
            "iter 3710: loss 4.1313, time 257.67ms, mfu 21.47%\n",
            "iter 3720: loss 4.0743, time 255.87ms, mfu 21.55%\n",
            "iter 3730: loss 4.0807, time 256.79ms, mfu 21.62%\n",
            "iter 3740: loss 4.0180, time 257.78ms, mfu 21.67%\n",
            "iter 3750: loss 4.1470, time 257.15ms, mfu 21.72%\n",
            "iter 3760: loss 4.2203, time 256.99ms, mfu 21.77%\n",
            "iter 3770: loss 4.1249, time 257.34ms, mfu 21.81%\n",
            "iter 3780: loss 4.0807, time 256.92ms, mfu 21.85%\n",
            "iter 3790: loss 4.1579, time 257.30ms, mfu 21.88%\n",
            "iter 3800: loss 4.0267, time 256.93ms, mfu 21.92%\n",
            "iter 3810: loss 4.0496, time 256.87ms, mfu 21.95%\n",
            "iter 3820: loss 3.9384, time 257.40ms, mfu 21.97%\n",
            "iter 3830: loss 4.1414, time 257.68ms, mfu 21.99%\n",
            "iter 3840: loss 3.9877, time 256.85ms, mfu 22.01%\n",
            "iter 3850: loss 4.1027, time 257.33ms, mfu 22.02%\n",
            "iter 3860: loss 4.1823, time 257.30ms, mfu 22.04%\n",
            "iter 3870: loss 4.0017, time 256.82ms, mfu 22.06%\n",
            "iter 3880: loss 4.1011, time 257.61ms, mfu 22.07%\n",
            "iter 3890: loss 4.0722, time 257.40ms, mfu 22.08%\n",
            "step 3900: train loss 4.0602, val loss 4.0550\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3900: loss 4.0299, time 36404.03ms, mfu 19.88%\n",
            "iter 3910: loss 4.0744, time 256.77ms, mfu 20.12%\n",
            "iter 3920: loss 4.0344, time 257.34ms, mfu 20.32%\n",
            "iter 3930: loss 3.9918, time 257.60ms, mfu 20.51%\n",
            "iter 3940: loss 4.0514, time 256.98ms, mfu 20.68%\n",
            "iter 3950: loss 4.0913, time 256.64ms, mfu 20.83%\n",
            "iter 3960: loss 3.9392, time 257.23ms, mfu 20.97%\n",
            "iter 3970: loss 4.0515, time 256.58ms, mfu 21.09%\n",
            "iter 3980: loss 4.0390, time 257.06ms, mfu 21.20%\n",
            "iter 3990: loss 4.0948, time 257.32ms, mfu 21.30%\n",
            "iter 4000: loss 4.0018, time 256.21ms, mfu 21.40%\n",
            "iter 4010: loss 4.1487, time 257.60ms, mfu 21.47%\n",
            "iter 4020: loss 4.1304, time 256.91ms, mfu 21.55%\n",
            "iter 4030: loss 4.1128, time 257.30ms, mfu 21.61%\n",
            "iter 4040: loss 4.0643, time 257.14ms, mfu 21.67%\n",
            "iter 4050: loss 4.0115, time 256.66ms, mfu 21.72%\n",
            "iter 4060: loss 3.9186, time 257.48ms, mfu 21.77%\n",
            "iter 4070: loss 4.0222, time 256.90ms, mfu 21.81%\n",
            "iter 4080: loss 3.8465, time 256.98ms, mfu 21.85%\n",
            "iter 4090: loss 3.8036, time 257.88ms, mfu 21.88%\n",
            "iter 4100: loss 4.0790, time 256.14ms, mfu 21.92%\n",
            "iter 4110: loss 4.0707, time 257.22ms, mfu 21.95%\n",
            "iter 4120: loss 3.9867, time 257.81ms, mfu 21.96%\n",
            "iter 4130: loss 3.9937, time 257.03ms, mfu 21.99%\n",
            "iter 4140: loss 3.9335, time 256.86ms, mfu 22.01%\n",
            "iter 4150: loss 3.9293, time 256.98ms, mfu 22.03%\n",
            "iter 4160: loss 4.0518, time 256.70ms, mfu 22.05%\n",
            "iter 4170: loss 3.9955, time 257.86ms, mfu 22.06%\n",
            "iter 4180: loss 3.9725, time 256.82ms, mfu 22.07%\n",
            "iter 4190: loss 4.0964, time 256.63ms, mfu 22.09%\n",
            "step 4200: train loss 4.0309, val loss 4.0282\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4200: loss 4.0546, time 36392.27ms, mfu 19.90%\n",
            "iter 4210: loss 4.0833, time 256.78ms, mfu 20.13%\n",
            "iter 4220: loss 4.0425, time 257.56ms, mfu 20.33%\n",
            "iter 4230: loss 4.0585, time 255.61ms, mfu 20.53%\n",
            "iter 4240: loss 4.0057, time 257.37ms, mfu 20.69%\n",
            "iter 4250: loss 3.9461, time 257.79ms, mfu 20.84%\n",
            "iter 4260: loss 4.0479, time 257.50ms, mfu 20.97%\n",
            "iter 4270: loss 4.1046, time 257.36ms, mfu 21.09%\n",
            "iter 4280: loss 3.9424, time 257.01ms, mfu 21.20%\n",
            "iter 4290: loss 4.0402, time 256.85ms, mfu 21.30%\n",
            "iter 4300: loss 3.9835, time 258.51ms, mfu 21.38%\n",
            "iter 4310: loss 4.0199, time 257.59ms, mfu 21.46%\n",
            "iter 4320: loss 3.9446, time 256.48ms, mfu 21.54%\n",
            "iter 4330: loss 4.0570, time 256.82ms, mfu 21.60%\n",
            "iter 4340: loss 3.9074, time 257.53ms, mfu 21.66%\n",
            "iter 4350: loss 4.0187, time 257.40ms, mfu 21.71%\n",
            "iter 4360: loss 3.9655, time 256.53ms, mfu 21.76%\n",
            "iter 4370: loss 4.0078, time 256.06ms, mfu 21.82%\n",
            "iter 4380: loss 4.0448, time 257.19ms, mfu 21.85%\n",
            "iter 4390: loss 3.9859, time 257.33ms, mfu 21.88%\n",
            "iter 4400: loss 3.9113, time 256.70ms, mfu 21.92%\n",
            "iter 4410: loss 4.0475, time 257.37ms, mfu 21.94%\n",
            "iter 4420: loss 4.1462, time 256.28ms, mfu 21.98%\n",
            "iter 4430: loss 4.0458, time 257.29ms, mfu 22.00%\n",
            "iter 4440: loss 4.0463, time 257.24ms, mfu 22.01%\n",
            "iter 4450: loss 4.0212, time 215.36ms, mfu 22.46%\n",
            "iter 4460: loss 3.9012, time 257.34ms, mfu 22.43%\n",
            "iter 4470: loss 3.9830, time 257.21ms, mfu 22.41%\n",
            "iter 4480: loss 4.0416, time 256.79ms, mfu 22.39%\n",
            "iter 4490: loss 3.9521, time 257.84ms, mfu 22.36%\n",
            "step 4500: train loss 4.0020, val loss 4.0101\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4500: loss 3.9266, time 36167.11ms, mfu 20.14%\n",
            "iter 4510: loss 3.9922, time 256.82ms, mfu 20.35%\n",
            "iter 4520: loss 4.0381, time 256.76ms, mfu 20.54%\n",
            "iter 4530: loss 4.1095, time 256.83ms, mfu 20.71%\n",
            "iter 4540: loss 3.9853, time 256.88ms, mfu 20.86%\n",
            "iter 4550: loss 4.0093, time 257.84ms, mfu 20.98%\n",
            "iter 4560: loss 3.9898, time 256.68ms, mfu 21.11%\n",
            "iter 4570: loss 3.9901, time 256.67ms, mfu 21.22%\n",
            "iter 4580: loss 4.0351, time 257.52ms, mfu 21.31%\n",
            "iter 4590: loss 3.8732, time 256.97ms, mfu 21.40%\n",
            "iter 4600: loss 4.0627, time 256.30ms, mfu 21.49%\n",
            "iter 4610: loss 4.0892, time 257.19ms, mfu 21.56%\n",
            "iter 4620: loss 3.9338, time 257.37ms, mfu 21.62%\n",
            "iter 4630: loss 4.1037, time 257.05ms, mfu 21.68%\n",
            "iter 4640: loss 4.0430, time 257.51ms, mfu 21.73%\n",
            "iter 4650: loss 4.0320, time 257.25ms, mfu 21.77%\n",
            "iter 4660: loss 4.0590, time 256.76ms, mfu 21.82%\n",
            "iter 4670: loss 4.1092, time 256.74ms, mfu 21.86%\n",
            "iter 4680: loss 4.0864, time 256.74ms, mfu 21.89%\n",
            "iter 4690: loss 4.0499, time 256.06ms, mfu 21.93%\n",
            "iter 4700: loss 3.8708, time 257.34ms, mfu 21.96%\n",
            "iter 4710: loss 3.9746, time 256.68ms, mfu 21.98%\n",
            "iter 4720: loss 4.0445, time 256.26ms, mfu 22.01%\n",
            "iter 4730: loss 3.9538, time 257.42ms, mfu 22.03%\n",
            "iter 4740: loss 3.9035, time 257.13ms, mfu 22.04%\n",
            "iter 4750: loss 4.0551, time 256.79ms, mfu 22.06%\n",
            "iter 4760: loss 4.0448, time 257.88ms, mfu 22.07%\n",
            "iter 4770: loss 4.0582, time 256.54ms, mfu 22.09%\n",
            "iter 4780: loss 3.9914, time 257.51ms, mfu 22.09%\n",
            "iter 4790: loss 3.9907, time 257.32ms, mfu 22.10%\n",
            "step 4800: train loss 4.0021, val loss 3.9940\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4800: loss 4.1584, time 36434.36ms, mfu 19.91%\n",
            "iter 4810: loss 3.9250, time 257.57ms, mfu 20.13%\n",
            "iter 4820: loss 3.8933, time 256.73ms, mfu 20.34%\n",
            "iter 4830: loss 3.9689, time 257.10ms, mfu 20.53%\n",
            "iter 4840: loss 4.1071, time 257.20ms, mfu 20.69%\n",
            "iter 4850: loss 4.0661, time 257.05ms, mfu 20.84%\n",
            "iter 4860: loss 4.0221, time 256.98ms, mfu 20.98%\n",
            "iter 4870: loss 3.9023, time 257.72ms, mfu 21.09%\n",
            "iter 4880: loss 4.0793, time 256.36ms, mfu 21.21%\n",
            "iter 4890: loss 4.0375, time 257.13ms, mfu 21.31%\n",
            "iter 4900: loss 4.1354, time 256.75ms, mfu 21.40%\n",
            "iter 4910: loss 3.9439, time 256.74ms, mfu 21.48%\n",
            "iter 4920: loss 3.9922, time 257.72ms, mfu 21.55%\n",
            "iter 4930: loss 3.9236, time 256.36ms, mfu 21.62%\n",
            "iter 4940: loss 3.9605, time 257.85ms, mfu 21.67%\n",
            "iter 4950: loss 4.0823, time 256.59ms, mfu 21.73%\n",
            "iter 4960: loss 3.9636, time 257.01ms, mfu 21.77%\n",
            "iter 4970: loss 3.9780, time 257.71ms, mfu 21.81%\n",
            "iter 4980: loss 3.9029, time 256.69ms, mfu 21.85%\n",
            "iter 4990: loss 3.9434, time 257.60ms, mfu 21.88%\n",
            "iter 5000: loss 3.9940, time 257.76ms, mfu 21.91%\n",
            "iter 5010: loss 3.9914, time 256.73ms, mfu 21.94%\n",
            "iter 5020: loss 3.9428, time 257.39ms, mfu 21.96%\n",
            "iter 5030: loss 3.9547, time 257.07ms, mfu 21.99%\n",
            "iter 5040: loss 3.9842, time 257.46ms, mfu 22.00%\n",
            "iter 5050: loss 4.0101, time 256.94ms, mfu 22.02%\n",
            "iter 5060: loss 4.0322, time 256.58ms, mfu 22.05%\n",
            "iter 5070: loss 4.0333, time 257.47ms, mfu 22.06%\n",
            "iter 5080: loss 3.9591, time 257.34ms, mfu 22.07%\n",
            "iter 5090: loss 3.8945, time 256.81ms, mfu 22.08%\n",
            "step 5100: train loss 3.9906, val loss 3.9943\n",
            "iter 5100: loss 4.0325, time 34412.96ms, mfu 19.89%\n",
            "iter 5110: loss 4.0088, time 258.23ms, mfu 20.11%\n",
            "iter 5120: loss 4.1613, time 256.76ms, mfu 20.32%\n",
            "iter 5130: loss 3.9333, time 257.74ms, mfu 20.50%\n",
            "iter 5140: loss 4.0377, time 257.45ms, mfu 20.67%\n",
            "iter 5150: loss 4.0287, time 256.69ms, mfu 20.83%\n",
            "iter 5160: loss 4.0494, time 256.82ms, mfu 20.97%\n",
            "iter 5170: loss 4.0280, time 256.88ms, mfu 21.09%\n",
            "iter 5180: loss 3.9956, time 257.92ms, mfu 21.19%\n",
            "iter 5190: loss 3.8262, time 256.68ms, mfu 21.30%\n",
            "iter 5200: loss 3.9344, time 256.68ms, mfu 21.39%\n",
            "iter 5210: loss 3.9911, time 258.05ms, mfu 21.46%\n",
            "iter 5220: loss 4.0033, time 256.43ms, mfu 21.54%\n",
            "iter 5230: loss 3.8966, time 257.65ms, mfu 21.60%\n",
            "iter 5240: loss 3.9984, time 257.32ms, mfu 21.66%\n",
            "iter 5250: loss 4.0553, time 257.23ms, mfu 21.71%\n",
            "iter 5260: loss 4.0746, time 257.20ms, mfu 21.76%\n",
            "iter 5270: loss 3.9048, time 257.10ms, mfu 21.80%\n",
            "iter 5280: loss 4.0880, time 257.18ms, mfu 21.84%\n",
            "iter 5290: loss 3.9355, time 256.81ms, mfu 21.88%\n",
            "iter 5300: loss 3.9834, time 258.02ms, mfu 21.90%\n",
            "iter 5310: loss 4.1651, time 256.81ms, mfu 21.93%\n",
            "iter 5320: loss 3.9712, time 257.47ms, mfu 21.96%\n",
            "iter 5330: loss 4.0646, time 257.01ms, mfu 21.98%\n",
            "iter 5340: loss 3.9781, time 257.31ms, mfu 22.00%\n",
            "iter 5350: loss 3.8946, time 257.59ms, mfu 22.02%\n",
            "iter 5360: loss 4.0017, time 256.79ms, mfu 22.04%\n",
            "iter 5370: loss 4.0499, time 257.81ms, mfu 22.05%\n",
            "iter 5380: loss 4.0374, time 256.54ms, mfu 22.07%\n",
            "iter 5390: loss 4.0147, time 257.62ms, mfu 22.07%\n",
            "step 5400: train loss 3.9940, val loss 3.9901\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5400: loss 4.1093, time 36500.69ms, mfu 19.88%\n",
            "iter 5410: loss 4.1229, time 257.13ms, mfu 20.11%\n",
            "iter 5420: loss 3.7588, time 257.18ms, mfu 20.32%\n",
            "iter 5430: loss 3.9440, time 256.32ms, mfu 20.51%\n",
            "iter 5440: loss 4.0752, time 257.06ms, mfu 20.68%\n",
            "iter 5450: loss 4.0060, time 257.40ms, mfu 20.83%\n",
            "iter 5460: loss 4.0235, time 256.62ms, mfu 20.97%\n",
            "iter 5470: loss 3.9193, time 256.43ms, mfu 21.10%\n",
            "iter 5480: loss 3.9902, time 256.92ms, mfu 21.21%\n",
            "iter 5490: loss 4.0258, time 257.08ms, mfu 21.31%\n",
            "iter 5500: loss 4.0684, time 256.54ms, mfu 21.40%\n",
            "iter 5510: loss 4.0339, time 256.32ms, mfu 21.49%\n",
            "iter 5520: loss 4.0239, time 256.49ms, mfu 21.56%\n",
            "iter 5530: loss 4.0365, time 256.87ms, mfu 21.63%\n",
            "iter 5540: loss 3.9353, time 256.78ms, mfu 21.69%\n",
            "iter 5550: loss 3.9914, time 256.23ms, mfu 21.75%\n",
            "iter 5560: loss 4.0441, time 256.98ms, mfu 21.79%\n",
            "iter 5570: loss 4.0175, time 257.28ms, mfu 21.83%\n",
            "iter 5580: loss 4.0436, time 256.70ms, mfu 21.87%\n",
            "iter 5590: loss 4.0659, time 257.04ms, mfu 21.90%\n",
            "iter 5600: loss 4.0116, time 256.33ms, mfu 21.94%\n",
            "iter 5610: loss 3.9540, time 256.07ms, mfu 21.97%\n",
            "iter 5620: loss 4.1394, time 256.57ms, mfu 22.00%\n",
            "iter 5630: loss 3.9578, time 257.16ms, mfu 22.02%\n",
            "iter 5640: loss 3.9754, time 256.62ms, mfu 22.04%\n",
            "iter 5650: loss 4.0032, time 256.28ms, mfu 22.06%\n",
            "iter 5660: loss 4.0411, time 256.77ms, mfu 22.08%\n",
            "iter 5670: loss 4.0343, time 256.85ms, mfu 22.09%\n",
            "iter 5680: loss 4.0874, time 256.42ms, mfu 22.11%\n",
            "iter 5690: loss 4.0309, time 256.17ms, mfu 22.12%\n",
            "step 5700: train loss 3.9880, val loss 3.9763\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5700: loss 4.0724, time 36517.23ms, mfu 19.93%\n",
            "iter 5710: loss 3.9558, time 255.15ms, mfu 20.17%\n",
            "iter 5720: loss 3.9630, time 256.49ms, mfu 20.38%\n",
            "iter 5730: loss 4.0424, time 256.63ms, mfu 20.56%\n",
            "iter 5740: loss 4.0588, time 256.60ms, mfu 20.73%\n",
            "iter 5750: loss 3.9815, time 257.56ms, mfu 20.87%\n",
            "iter 5760: loss 4.0977, time 256.57ms, mfu 21.01%\n",
            "iter 5770: loss 3.9008, time 255.99ms, mfu 21.14%\n",
            "iter 5780: loss 3.9376, time 256.14ms, mfu 21.25%\n",
            "iter 5790: loss 4.0183, time 257.14ms, mfu 21.35%\n",
            "iter 5800: loss 4.0640, time 256.72ms, mfu 21.43%\n",
            "iter 5810: loss 3.9490, time 256.02ms, mfu 21.52%\n",
            "iter 5820: loss 4.0151, time 256.15ms, mfu 21.59%\n",
            "iter 5830: loss 4.1382, time 256.88ms, mfu 21.66%\n",
            "iter 5840: loss 4.0338, time 256.79ms, mfu 21.71%\n",
            "iter 5850: loss 4.0926, time 256.35ms, mfu 21.77%\n",
            "iter 5860: loss 4.0415, time 256.07ms, mfu 21.82%\n",
            "iter 5870: loss 3.9820, time 256.45ms, mfu 21.86%\n",
            "iter 5880: loss 4.0072, time 257.60ms, mfu 21.89%\n",
            "iter 5890: loss 3.9874, time 256.80ms, mfu 21.92%\n",
            "iter 5900: loss 4.0609, time 256.98ms, mfu 21.95%\n",
            "iter 5910: loss 3.8186, time 255.32ms, mfu 21.99%\n",
            "iter 5920: loss 3.9941, time 257.03ms, mfu 22.01%\n",
            "iter 5930: loss 4.0395, time 257.01ms, mfu 22.03%\n",
            "iter 5940: loss 3.8641, time 256.74ms, mfu 22.05%\n",
            "iter 5950: loss 3.9645, time 256.90ms, mfu 22.07%\n",
            "iter 5960: loss 4.1053, time 256.45ms, mfu 22.08%\n",
            "iter 5970: loss 4.0594, time 256.27ms, mfu 22.10%\n",
            "iter 5980: loss 3.9852, time 255.89ms, mfu 22.12%\n",
            "iter 5990: loss 3.9844, time 256.86ms, mfu 22.13%\n",
            "step 6000: train loss 3.9854, val loss 3.9813\n",
            "iter 6000: loss 3.9091, time 34358.51ms, mfu 19.94%\n",
            "iter 6010: loss 3.9950, time 257.84ms, mfu 20.15%\n",
            "iter 6020: loss 3.9282, time 256.68ms, mfu 20.36%\n",
            "iter 6030: loss 3.9970, time 257.04ms, mfu 20.55%\n",
            "iter 6040: loss 4.1041, time 257.63ms, mfu 20.71%\n",
            "iter 6050: loss 4.0662, time 257.56ms, mfu 20.85%\n",
            "iter 6060: loss 3.9346, time 257.16ms, mfu 20.98%\n",
            "iter 6070: loss 4.0954, time 257.21ms, mfu 21.10%\n",
            "iter 6080: loss 3.9441, time 256.44ms, mfu 21.22%\n",
            "iter 6090: loss 3.9365, time 257.59ms, mfu 21.31%\n",
            "iter 6100: loss 4.0872, time 257.67ms, mfu 21.40%\n",
            "iter 6110: loss 3.9581, time 256.37ms, mfu 21.48%\n",
            "iter 6120: loss 4.0781, time 257.33ms, mfu 21.55%\n",
            "iter 6130: loss 4.0024, time 257.51ms, mfu 21.61%\n",
            "iter 6140: loss 3.9925, time 256.59ms, mfu 21.67%\n",
            "iter 6150: loss 3.9521, time 257.69ms, mfu 21.72%\n",
            "iter 6160: loss 4.0006, time 257.30ms, mfu 21.77%\n",
            "iter 6170: loss 3.9631, time 257.09ms, mfu 21.81%\n",
            "iter 6180: loss 3.9314, time 257.74ms, mfu 21.84%\n",
            "iter 6190: loss 3.9938, time 256.65ms, mfu 21.88%\n",
            "iter 6200: loss 3.9419, time 258.42ms, mfu 21.90%\n",
            "iter 6210: loss 3.9949, time 256.90ms, mfu 21.93%\n",
            "iter 6220: loss 3.9016, time 257.68ms, mfu 21.95%\n",
            "iter 6230: loss 3.9842, time 256.77ms, mfu 21.98%\n",
            "iter 6240: loss 3.9687, time 258.01ms, mfu 21.99%\n",
            "iter 6250: loss 3.9422, time 257.39ms, mfu 22.01%\n",
            "iter 6260: loss 3.9715, time 257.54ms, mfu 22.03%\n",
            "iter 6270: loss 3.9693, time 257.01ms, mfu 22.04%\n",
            "iter 6280: loss 4.0191, time 257.93ms, mfu 22.05%\n",
            "iter 6290: loss 3.8955, time 256.91ms, mfu 22.07%\n",
            "step 6300: train loss 3.9859, val loss 3.9694\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 6300: loss 4.0673, time 36436.80ms, mfu 19.88%\n",
            "iter 6310: loss 3.9909, time 257.39ms, mfu 20.11%\n",
            "iter 6320: loss 3.9556, time 257.67ms, mfu 20.31%\n",
            "iter 6330: loss 3.8866, time 256.77ms, mfu 20.50%\n",
            "iter 6340: loss 4.0158, time 257.17ms, mfu 20.67%\n",
            "iter 6350: loss 3.9908, time 256.82ms, mfu 20.82%\n",
            "iter 6360: loss 3.8498, time 257.09ms, mfu 20.96%\n",
            "iter 6370: loss 3.8702, time 257.42ms, mfu 21.08%\n",
            "iter 6380: loss 4.0755, time 256.57ms, mfu 21.20%\n",
            "iter 6390: loss 3.9792, time 256.40ms, mfu 21.30%\n",
            "iter 6400: loss 3.9157, time 257.34ms, mfu 21.39%\n",
            "iter 6410: loss 4.0736, time 257.15ms, mfu 21.47%\n",
            "iter 6420: loss 4.0365, time 256.89ms, mfu 21.54%\n",
            "iter 6430: loss 3.9598, time 256.60ms, mfu 21.61%\n",
            "iter 6440: loss 4.0670, time 256.67ms, mfu 21.67%\n",
            "iter 6450: loss 3.9232, time 256.53ms, mfu 21.73%\n",
            "iter 6460: loss 3.9530, time 257.61ms, mfu 21.77%\n",
            "iter 6470: loss 3.9955, time 257.36ms, mfu 21.81%\n",
            "iter 6480: loss 3.9832, time 256.28ms, mfu 21.86%\n",
            "iter 6490: loss 4.0757, time 257.00ms, mfu 21.89%\n",
            "iter 6500: loss 4.0018, time 257.10ms, mfu 21.92%\n",
            "iter 6510: loss 4.0068, time 256.80ms, mfu 21.95%\n",
            "iter 6520: loss 3.9888, time 256.61ms, mfu 21.98%\n",
            "iter 6530: loss 3.9054, time 257.29ms, mfu 22.00%\n",
            "iter 6540: loss 4.0788, time 257.60ms, mfu 22.02%\n",
            "iter 6550: loss 3.9881, time 257.34ms, mfu 22.03%\n",
            "iter 6560: loss 3.9864, time 258.02ms, mfu 22.04%\n",
            "iter 6570: loss 4.0694, time 256.52ms, mfu 22.06%\n",
            "iter 6580: loss 4.0970, time 257.29ms, mfu 22.07%\n",
            "iter 6590: loss 4.1288, time 257.39ms, mfu 22.08%\n",
            "step 6600: train loss 3.9782, val loss 3.9789\n",
            "iter 6600: loss 4.0181, time 34398.56ms, mfu 19.89%\n",
            "iter 6610: loss 3.8528, time 257.53ms, mfu 20.12%\n",
            "iter 6620: loss 3.8768, time 257.25ms, mfu 20.32%\n",
            "iter 6630: loss 4.0416, time 256.74ms, mfu 20.51%\n",
            "iter 6640: loss 3.9843, time 257.55ms, mfu 20.68%\n",
            "iter 6650: loss 4.0091, time 258.22ms, mfu 20.82%\n",
            "iter 6660: loss 4.0145, time 256.84ms, mfu 20.96%\n",
            "iter 6670: loss 4.0772, time 257.27ms, mfu 21.08%\n",
            "iter 6680: loss 4.0688, time 256.25ms, mfu 21.20%\n",
            "iter 6690: loss 3.8928, time 257.60ms, mfu 21.29%\n",
            "iter 6700: loss 3.8805, time 256.55ms, mfu 21.39%\n",
            "iter 6710: loss 4.0089, time 257.39ms, mfu 21.47%\n",
            "iter 6720: loss 3.9384, time 256.77ms, mfu 21.54%\n",
            "iter 6730: loss 3.9738, time 257.28ms, mfu 21.61%\n",
            "iter 6740: loss 4.0228, time 257.26ms, mfu 21.66%\n",
            "iter 6750: loss 3.9565, time 256.82ms, mfu 21.72%\n",
            "iter 6760: loss 3.9110, time 257.81ms, mfu 21.76%\n",
            "iter 6770: loss 4.0154, time 257.05ms, mfu 21.80%\n",
            "iter 6780: loss 3.8685, time 257.23ms, mfu 21.84%\n",
            "iter 6790: loss 3.9942, time 256.94ms, mfu 21.88%\n",
            "iter 6800: loss 3.9209, time 257.03ms, mfu 21.91%\n",
            "iter 6810: loss 3.9768, time 258.04ms, mfu 21.93%\n",
            "iter 6820: loss 4.0306, time 257.35ms, mfu 21.95%\n",
            "iter 6830: loss 3.8253, time 257.19ms, mfu 21.98%\n",
            "iter 6840: loss 4.0163, time 257.07ms, mfu 22.00%\n",
            "iter 6850: loss 3.9821, time 257.18ms, mfu 22.02%\n",
            "iter 6860: loss 3.9416, time 257.39ms, mfu 22.03%\n",
            "iter 6870: loss 3.9309, time 256.74ms, mfu 22.05%\n",
            "iter 6880: loss 4.0030, time 257.22ms, mfu 22.07%\n",
            "iter 6890: loss 4.0022, time 257.11ms, mfu 22.08%\n",
            "step 6900: train loss 3.9870, val loss 3.9757\n",
            "iter 6900: loss 4.0268, time 34470.52ms, mfu 19.89%\n",
            "iter 6910: loss 4.1115, time 257.83ms, mfu 20.11%\n",
            "iter 6920: loss 3.9251, time 257.02ms, mfu 20.32%\n",
            "iter 6930: loss 3.8878, time 257.45ms, mfu 20.50%\n",
            "iter 6940: loss 3.9867, time 257.11ms, mfu 20.67%\n",
            "iter 6950: loss 3.9843, time 256.98ms, mfu 20.83%\n",
            "iter 6960: loss 4.1021, time 257.51ms, mfu 20.96%\n",
            "iter 6970: loss 3.9445, time 257.83ms, mfu 21.08%\n",
            "iter 6980: loss 3.9233, time 257.16ms, mfu 21.19%\n",
            "iter 6990: loss 4.0536, time 258.57ms, mfu 21.28%\n",
            "iter 7000: loss 3.9821, time 257.22ms, mfu 21.37%\n",
            "iter 7010: loss 4.0541, time 257.74ms, mfu 21.44%\n",
            "iter 7020: loss 3.9442, time 257.26ms, mfu 21.52%\n",
            "iter 7030: loss 4.0376, time 257.55ms, mfu 21.58%\n",
            "iter 7040: loss 3.9596, time 257.18ms, mfu 21.64%\n",
            "iter 7050: loss 3.9899, time 257.41ms, mfu 21.69%\n",
            "iter 7060: loss 3.9661, time 256.97ms, mfu 21.74%\n",
            "iter 7070: loss 3.8348, time 257.90ms, mfu 21.78%\n",
            "iter 7080: loss 4.0291, time 256.72ms, mfu 21.83%\n",
            "iter 7090: loss 3.9269, time 257.85ms, mfu 21.86%\n",
            "iter 7100: loss 3.9414, time 257.38ms, mfu 21.89%\n",
            "iter 7110: loss 3.8584, time 257.59ms, mfu 21.91%\n",
            "iter 7120: loss 3.9608, time 257.55ms, mfu 21.94%\n",
            "iter 7130: loss 3.9510, time 257.11ms, mfu 21.96%\n",
            "iter 7140: loss 3.9764, time 257.95ms, mfu 21.98%\n",
            "iter 7150: loss 4.0067, time 256.99ms, mfu 22.00%\n",
            "iter 7160: loss 3.8981, time 258.04ms, mfu 22.01%\n",
            "iter 7170: loss 4.0187, time 256.95ms, mfu 22.03%\n",
            "iter 7180: loss 4.0341, time 257.70ms, mfu 22.04%\n",
            "iter 7190: loss 3.8958, time 257.07ms, mfu 22.06%\n",
            "step 7200: train loss 3.9786, val loss 3.9808\n",
            "iter 7200: loss 4.0196, time 34467.43ms, mfu 19.87%\n",
            "iter 7210: loss 3.9982, time 258.05ms, mfu 20.09%\n",
            "iter 7220: loss 4.0204, time 256.50ms, mfu 20.31%\n",
            "iter 7230: loss 3.9998, time 258.02ms, mfu 20.49%\n",
            "iter 7240: loss 3.9837, time 256.88ms, mfu 20.66%\n",
            "iter 7250: loss 4.0543, time 257.12ms, mfu 20.81%\n",
            "iter 7260: loss 3.9745, time 257.71ms, mfu 20.95%\n",
            "iter 7270: loss 4.0564, time 257.28ms, mfu 21.07%\n",
            "iter 7280: loss 4.0038, time 257.66ms, mfu 21.18%\n",
            "iter 7290: loss 4.0075, time 256.94ms, mfu 21.28%\n",
            "iter 7300: loss 3.9924, time 257.55ms, mfu 21.37%\n",
            "iter 7310: loss 3.9390, time 257.41ms, mfu 21.45%\n",
            "iter 7320: loss 3.9537, time 257.31ms, mfu 21.52%\n",
            "iter 7330: loss 3.9863, time 258.29ms, mfu 21.58%\n",
            "iter 7340: loss 4.1276, time 257.52ms, mfu 21.64%\n",
            "iter 7350: loss 3.9951, time 257.04ms, mfu 21.69%\n",
            "iter 7360: loss 3.9335, time 257.42ms, mfu 21.74%\n",
            "iter 7370: loss 3.8953, time 258.48ms, mfu 21.77%\n",
            "iter 7380: loss 4.0448, time 256.90ms, mfu 21.82%\n",
            "iter 7390: loss 3.9903, time 257.41ms, mfu 21.85%\n",
            "iter 7400: loss 3.9385, time 257.76ms, mfu 21.88%\n",
            "iter 7410: loss 3.9535, time 256.96ms, mfu 21.91%\n",
            "iter 7420: loss 3.9590, time 258.40ms, mfu 21.93%\n",
            "iter 7430: loss 4.1016, time 257.40ms, mfu 21.95%\n",
            "iter 7440: loss 3.9154, time 258.00ms, mfu 21.97%\n",
            "iter 7450: loss 4.0571, time 256.76ms, mfu 21.99%\n",
            "iter 7460: loss 4.0618, time 258.30ms, mfu 22.00%\n",
            "iter 7470: loss 3.8847, time 257.10ms, mfu 22.02%\n",
            "iter 7480: loss 3.9555, time 257.72ms, mfu 22.04%\n",
            "iter 7490: loss 3.9887, time 257.46ms, mfu 22.05%\n",
            "step 7500: train loss 3.9853, val loss 3.9671\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 7500: loss 3.9344, time 36216.37ms, mfu 19.86%\n",
            "iter 7510: loss 3.9227, time 256.85ms, mfu 20.09%\n",
            "iter 7520: loss 4.0776, time 257.44ms, mfu 20.30%\n",
            "iter 7530: loss 4.0927, time 257.07ms, mfu 20.49%\n",
            "iter 7540: loss 3.8989, time 256.93ms, mfu 20.66%\n",
            "iter 7550: loss 3.9511, time 257.70ms, mfu 20.81%\n",
            "iter 7560: loss 3.9032, time 256.73ms, mfu 20.95%\n",
            "iter 7570: loss 3.9080, time 257.63ms, mfu 21.07%\n",
            "iter 7580: loss 3.9055, time 256.80ms, mfu 21.19%\n",
            "iter 7590: loss 3.9307, time 255.96ms, mfu 21.30%\n",
            "iter 7600: loss 3.9887, time 257.09ms, mfu 21.39%\n",
            "iter 7610: loss 3.9818, time 256.83ms, mfu 21.47%\n",
            "iter 7620: loss 3.8733, time 257.60ms, mfu 21.54%\n",
            "iter 7630: loss 3.8623, time 256.96ms, mfu 21.60%\n",
            "iter 7640: loss 3.9982, time 257.04ms, mfu 21.66%\n",
            "iter 7650: loss 4.0619, time 256.18ms, mfu 21.72%\n",
            "iter 7660: loss 3.8483, time 256.65ms, mfu 21.78%\n",
            "iter 7670: loss 3.9997, time 258.22ms, mfu 21.81%\n",
            "iter 7680: loss 3.9905, time 256.73ms, mfu 21.85%\n",
            "iter 7690: loss 4.0075, time 257.20ms, mfu 21.88%\n",
            "iter 7700: loss 3.9957, time 256.92ms, mfu 21.92%\n",
            "iter 7710: loss 4.0808, time 257.12ms, mfu 21.94%\n",
            "iter 7720: loss 3.9601, time 256.91ms, mfu 21.97%\n",
            "iter 7730: loss 3.8301, time 258.71ms, mfu 21.98%\n",
            "iter 7740: loss 3.9606, time 257.28ms, mfu 22.00%\n",
            "iter 7750: loss 4.0651, time 256.86ms, mfu 22.02%\n",
            "iter 7760: loss 4.0263, time 257.15ms, mfu 22.04%\n",
            "iter 7770: loss 4.0548, time 257.47ms, mfu 22.05%\n",
            "iter 7780: loss 3.9927, time 257.28ms, mfu 22.06%\n",
            "iter 7790: loss 3.9619, time 256.65ms, mfu 22.08%\n",
            "step 7800: train loss 3.9824, val loss 3.9674\n",
            "iter 7800: loss 3.9829, time 34400.77ms, mfu 19.89%\n",
            "iter 7810: loss 3.9119, time 257.25ms, mfu 20.12%\n",
            "iter 7820: loss 3.7593, time 256.94ms, mfu 20.33%\n",
            "iter 7830: loss 3.9250, time 257.27ms, mfu 20.51%\n",
            "iter 7840: loss 3.9809, time 257.33ms, mfu 20.68%\n",
            "iter 7850: loss 3.9531, time 257.79ms, mfu 20.82%\n",
            "iter 7860: loss 4.0032, time 257.51ms, mfu 20.96%\n",
            "iter 7870: loss 4.0932, time 256.60ms, mfu 21.08%\n",
            "iter 7880: loss 4.0483, time 257.22ms, mfu 21.19%\n",
            "iter 7890: loss 3.9705, time 257.14ms, mfu 21.29%\n",
            "iter 7900: loss 4.1526, time 256.80ms, mfu 21.39%\n",
            "iter 7910: loss 3.9128, time 257.47ms, mfu 21.46%\n",
            "iter 7920: loss 3.9683, time 257.19ms, mfu 21.54%\n",
            "iter 7930: loss 4.0302, time 257.01ms, mfu 21.60%\n",
            "iter 7940: loss 3.8990, time 258.01ms, mfu 21.65%\n",
            "iter 7950: loss 3.9752, time 256.73ms, mfu 21.71%\n",
            "iter 7960: loss 4.0107, time 257.98ms, mfu 21.75%\n",
            "iter 7970: loss 3.9603, time 256.87ms, mfu 21.80%\n",
            "iter 7980: loss 3.8989, time 257.32ms, mfu 21.84%\n",
            "iter 7990: loss 3.9807, time 256.64ms, mfu 21.88%\n",
            "iter 8000: loss 3.9465, time 257.54ms, mfu 21.90%\n",
            "iter 8010: loss 3.9181, time 257.60ms, mfu 21.93%\n",
            "iter 8020: loss 3.9558, time 257.12ms, mfu 21.95%\n",
            "iter 8030: loss 3.8887, time 257.47ms, mfu 21.98%\n",
            "iter 8040: loss 4.0255, time 257.43ms, mfu 21.99%\n",
            "iter 8050: loss 3.9317, time 256.77ms, mfu 22.02%\n",
            "iter 8060: loss 3.9279, time 257.38ms, mfu 22.03%\n",
            "iter 8070: loss 3.9992, time 257.48ms, mfu 22.05%\n",
            "iter 8080: loss 3.9784, time 256.95ms, mfu 22.06%\n",
            "iter 8090: loss 3.9617, time 258.06ms, mfu 22.07%\n",
            "step 8100: train loss 3.9712, val loss 3.9680\n",
            "iter 8100: loss 3.7739, time 34400.48ms, mfu 19.88%\n",
            "iter 8110: loss 4.0083, time 256.94ms, mfu 20.11%\n",
            "iter 8120: loss 3.9721, time 257.96ms, mfu 20.31%\n",
            "iter 8130: loss 4.1228, time 256.67ms, mfu 20.50%\n",
            "iter 8140: loss 3.9671, time 257.80ms, mfu 20.67%\n",
            "iter 8150: loss 4.1107, time 257.17ms, mfu 20.82%\n",
            "iter 8160: loss 3.9947, time 258.17ms, mfu 20.95%\n",
            "iter 8170: loss 4.0169, time 257.48ms, mfu 21.07%\n",
            "iter 8180: loss 3.8747, time 257.72ms, mfu 21.17%\n",
            "iter 8190: loss 3.8924, time 256.94ms, mfu 21.28%\n",
            "iter 8200: loss 4.0882, time 257.27ms, mfu 21.37%\n",
            "iter 8210: loss 3.9569, time 257.03ms, mfu 21.45%\n",
            "iter 8220: loss 4.0001, time 257.33ms, mfu 21.52%\n",
            "iter 8230: loss 4.0544, time 256.59ms, mfu 21.59%\n",
            "iter 8240: loss 3.8542, time 257.16ms, mfu 21.65%\n",
            "iter 8250: loss 4.0277, time 257.34ms, mfu 21.71%\n",
            "iter 8260: loss 3.9257, time 257.41ms, mfu 21.75%\n",
            "iter 8270: loss 3.9854, time 257.58ms, mfu 21.79%\n",
            "iter 8280: loss 4.0362, time 256.98ms, mfu 21.83%\n",
            "iter 8290: loss 4.0542, time 256.56ms, mfu 21.87%\n",
            "iter 8300: loss 3.9535, time 257.94ms, mfu 21.90%\n",
            "iter 8310: loss 3.9683, time 257.08ms, mfu 21.93%\n",
            "iter 8320: loss 4.0030, time 256.97ms, mfu 21.96%\n",
            "iter 8330: loss 3.9564, time 258.12ms, mfu 21.97%\n",
            "iter 8340: loss 3.8512, time 256.77ms, mfu 22.00%\n",
            "iter 8350: loss 3.9715, time 257.22ms, mfu 22.01%\n",
            "iter 8360: loss 4.0436, time 257.09ms, mfu 22.03%\n",
            "iter 8370: loss 3.8756, time 256.65ms, mfu 22.05%\n",
            "iter 8380: loss 3.8850, time 256.73ms, mfu 22.07%\n",
            "iter 8390: loss 4.1045, time 257.37ms, mfu 22.08%\n",
            "step 8400: train loss 3.9758, val loss 3.9672\n",
            "iter 8400: loss 4.0552, time 34448.61ms, mfu 19.89%\n",
            "iter 8410: loss 3.9663, time 256.53ms, mfu 20.12%\n",
            "iter 8420: loss 3.8964, time 256.58ms, mfu 20.34%\n",
            "iter 8430: loss 4.0318, time 256.60ms, mfu 20.53%\n",
            "iter 8440: loss 4.0248, time 256.11ms, mfu 20.70%\n",
            "iter 8450: loss 3.9721, time 257.05ms, mfu 20.85%\n",
            "iter 8460: loss 4.0132, time 257.37ms, mfu 20.98%\n",
            "iter 8470: loss 4.0746, time 256.69ms, mfu 21.11%\n",
            "iter 8480: loss 4.0136, time 256.18ms, mfu 21.22%\n",
            "iter 8490: loss 3.9863, time 256.82ms, mfu 21.32%\n",
            "iter 8500: loss 3.9839, time 257.22ms, mfu 21.41%\n",
            "iter 8510: loss 3.9549, time 255.99ms, mfu 21.50%\n",
            "iter 8520: loss 3.9567, time 256.89ms, mfu 21.57%\n",
            "iter 8530: loss 3.9745, time 256.80ms, mfu 21.63%\n",
            "iter 8540: loss 3.9589, time 256.26ms, mfu 21.70%\n",
            "iter 8550: loss 3.9303, time 256.07ms, mfu 21.76%\n",
            "iter 8560: loss 4.0665, time 256.73ms, mfu 21.80%\n",
            "iter 8570: loss 4.0308, time 257.08ms, mfu 21.84%\n",
            "iter 8580: loss 3.9388, time 256.78ms, mfu 21.88%\n",
            "iter 8590: loss 3.9805, time 256.21ms, mfu 21.92%\n",
            "iter 8600: loss 4.0101, time 256.84ms, mfu 21.95%\n",
            "iter 8610: loss 3.8639, time 256.94ms, mfu 21.97%\n",
            "iter 8620: loss 4.0533, time 257.07ms, mfu 22.00%\n",
            "iter 8630: loss 4.0343, time 256.98ms, mfu 22.02%\n",
            "iter 8640: loss 3.9745, time 257.30ms, mfu 22.03%\n",
            "iter 8650: loss 4.0147, time 257.57ms, mfu 22.05%\n",
            "iter 8660: loss 3.9399, time 256.84ms, mfu 22.06%\n",
            "iter 8670: loss 3.9416, time 256.43ms, mfu 22.08%\n",
            "iter 8680: loss 3.9835, time 256.18ms, mfu 22.10%\n",
            "iter 8690: loss 4.0353, time 256.78ms, mfu 22.11%\n",
            "step 8700: train loss 3.9749, val loss 3.9713\n",
            "iter 8700: loss 4.0106, time 34313.23ms, mfu 19.92%\n",
            "iter 8710: loss 3.8710, time 256.40ms, mfu 20.15%\n",
            "iter 8720: loss 3.9416, time 256.75ms, mfu 20.36%\n",
            "iter 8730: loss 3.8797, time 257.44ms, mfu 20.54%\n",
            "iter 8740: loss 3.9870, time 256.59ms, mfu 20.71%\n",
            "iter 8750: loss 3.8816, time 256.52ms, mfu 20.86%\n",
            "iter 8760: loss 3.9736, time 256.70ms, mfu 21.00%\n",
            "iter 8770: loss 3.9539, time 256.65ms, mfu 21.12%\n",
            "iter 8780: loss 3.9900, time 256.78ms, mfu 21.23%\n",
            "iter 8790: loss 3.9599, time 257.09ms, mfu 21.33%\n",
            "iter 8800: loss 3.9724, time 257.34ms, mfu 21.41%\n",
            "iter 8810: loss 3.9242, time 257.77ms, mfu 21.49%\n",
            "iter 8820: loss 3.9671, time 256.50ms, mfu 21.56%\n",
            "iter 8830: loss 4.0215, time 256.58ms, mfu 21.63%\n",
            "iter 8840: loss 3.8752, time 256.75ms, mfu 21.69%\n",
            "iter 8850: loss 3.8969, time 257.42ms, mfu 21.74%\n",
            "iter 8860: loss 3.9534, time 258.52ms, mfu 21.77%\n",
            "iter 8870: loss 4.0017, time 256.37ms, mfu 21.82%\n",
            "iter 8880: loss 3.8161, time 257.29ms, mfu 21.85%\n",
            "iter 8890: loss 3.9747, time 256.71ms, mfu 21.89%\n",
            "iter 8900: loss 3.9593, time 257.24ms, mfu 21.92%\n",
            "iter 8910: loss 4.0063, time 257.34ms, mfu 21.95%\n",
            "iter 8920: loss 3.8935, time 256.78ms, mfu 21.97%\n",
            "iter 8930: loss 3.8752, time 258.04ms, mfu 21.99%\n",
            "iter 8940: loss 3.8843, time 256.95ms, mfu 22.01%\n",
            "iter 8950: loss 3.9013, time 258.23ms, mfu 22.02%\n",
            "iter 8960: loss 3.9722, time 256.71ms, mfu 22.04%\n",
            "iter 8970: loss 3.9134, time 256.94ms, mfu 22.06%\n",
            "iter 8980: loss 3.9136, time 257.32ms, mfu 22.07%\n",
            "iter 8990: loss 3.8237, time 256.46ms, mfu 22.09%\n",
            "step 9000: train loss 3.9663, val loss 3.9661\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9000: loss 4.1416, time 36431.51ms, mfu 19.89%\n",
            "iter 9010: loss 3.9873, time 257.47ms, mfu 20.12%\n",
            "iter 9020: loss 3.9890, time 257.96ms, mfu 20.32%\n",
            "iter 9030: loss 3.8561, time 256.88ms, mfu 20.51%\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 39.56 GiB of which 4.64 GiB is free. Process 42533 has 34.92 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c12f8252d313>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# backward pass, with gradient scaling if training in fp16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;31m# clip the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 39.56 GiB of which 4.64 GiB is free. Process 42533 has 34.92 GiB memory in use. Of the allocated memory 22.89 GiB is allocated by PyTorch, and 11.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oQDu-z6cC49"
      },
      "outputs": [],
      "source": [
        "# start = \"How to join a tbank-research?\"\n",
        "# start = \"How to join a tbank-research?\"\n",
        "start = \"It's snow.\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zID-xuIcC4-",
        "outputId": "f36bf217-1e21-40d1-fdd6-37c7d50535fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-9f0241acb2d4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZggYfxnYcC4-",
        "outputId": "6cfba90a-639a-4ae9-8fc8-6574ebfe788d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's snow. It's still the same,\" she said.\n",
            "\n",
            "\"And I want it to lose.\"<|endoftext|>The Army soldiers who went to work in Afghanistan have been arrested on the site, including the last one for two years for the Veterans Affairs of the Army.\n",
            "\n",
            "By the time the Army will be charged, at least three thousand soldiers, the Defense Department says.\n",
            "\n",
            "The Army has been involved since March 31, which includes the new \"an\" system for troops in Afghanistan, Afghanistan, the Pentagon's Secretary of Defense, and the military's top commander.\n",
            "\n",
            "The more than 100 troops are expected to be in this year's first two years for the Pentagon and Lockheed Martin.\n",
            "\n",
            "The Pentagon has the second largest collection of detainees, which includes the 10,000 prisoners convicted of crimes in Afghanistan and Afghanistan, according to the National Centers for National Guard, the Defense Department said, according to the Pentagon's Office on the new report.\n",
            "\n",
            "The military has been in the ranks of military officers across the globe, with the highest level of job security since the Army is the top-ranked country's list of detainees.\n",
            "\n",
            "Story continues below advertisement\n",
            "\n",
            "The Army has more than 1,000 soldiers in Afghanistan, and more than 3,000 have been killed in Afghanistan, according to the National Institute for Civil Services.<|endoftext|>Image copyright EPA Image caption A new poll of 1,092 people had been killed on a mosque in the Khawramabad area by the police state of Bengaluru , a busing station in the capital, the capital of the country's capital, in the capital, the capital of the capital, the capital of the country's capital.\n",
            "\n",
            "The data collected by the police group showed that there were two types of homes in the capital, which are now around 8,000 people, most on the street in the capital, according to the figures, in the country of Bengaluru, a town in the capital, the capital, at the centre of the district.\n",
            "\n",
            "The toll group was at 0.2% in the first four years of the year, with the average time for the state's total population.\n",
            "\n",
            "Prime Minister Narendra Modi, he told reporters news footage about how his country's capital is now, that it wasn't the first time a police officer involved in the riots, but the prime minister for the next three years was just the result of his leadership in recent years.\n",
            "\n",
            "He said there were few people who\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"We have a good time.\"\n",
            "\n",
            "She said her team had a good day and another good Friday, which was on her way. She's now battling with her family, and she's so worried that the team has what it can do with it. \"It's a good day.\"\n",
            "\n",
            "She said the team was in shock and that it was a good start for a game. And just for the team and it was good news for her team and she's trying to get her own team.\n",
            "\n",
            "Her team's wife, Mark Sanchez, will be playing with the Houston Texans for the season.\n",
            "\n",
            "\"It's really good for the team,\" said Ross.\n",
            "\n",
            "The Bears also were in the midst of an argument about the team's progress.\n",
            "\n",
            "\"My team's been on track to win the game and I've never really got the support of my team and it's an argument that I think we can play with the team and our team. I was really excited about if it was up and we didn't have that opportunity to play against us in the playoffs and for the team.\"\n",
            "\n",
            "The Bears signed a $18 million deal, but the team never completed an offer.\n",
            "\n",
            "If the team had to have an offer, they could have been without a contract. So for now, the Bears have a lot more.\n",
            "\n",
            "Follow @DavidBatt<|endoftext|>\"The Great Britain\" redirects here. For more information, see the relevant links to the original version<|endoftext|>A new story of one of the most ridiculous stories on this planet in the history of the world.\n",
            "\n",
            "It may have been the first time (and it's a bit strange to see) but as you know it, the truth is that the world is struggling to grasp the facts of life in the first place. The story of the world goes on and on, and the experience of the future is how we move forward with those that we are at the center of the world. In the first week of Friday's 11th anniversary of the Great Britain period, the first world world where the United States was the world's first world world to see itself as the world's greatest nation to experience. Now, the first time the world has been to the realization that the world is changing, and what we know today is the world that the world is changing.\n",
            "\n",
            "It is the real world that we,\n",
            "\n",
            "The world is now, in fact, the world\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"It's the same place for the home, but no one else has to worry about,\" he said.\n",
            "\n",
            "\"You're being more, you're out of a path to where you have a good summer and a lot of things.\"\n",
            "\n",
            "As for the second quarter, players with their own team have been in their third quarter and their third quarter -- most of the first pair of games - which can't handle any team that can handle any team in the next three or seven years.\n",
            "\n",
            "Even six of the players in the group are competing in the second quarter, and that means they're ready to play the game.\n",
            "\n",
            "\"I think they're playing and in a way, they're in a way different place,\" said Alex \"to get their team up and compete for the next level.\"\n",
            "\n",
            "So it's possible they're playing with their own group and have to be prepared to play against them.\n",
            "\n",
            "\"There's no need for that,\" said Eric Upper. \"I think that's what we're doing, I think we'll be building a football game, and we're just going to be able to play like a football game.\"\n",
            "\n",
            "The first step was not for the third quarter but was to be released, with only a few of the players in the group.\n",
            "\n",
            "At the end of the fourth quarter, players played the first round, but only two were in the group.\n",
            "\n",
            "The second quarter continued to become the third quarter, with 12 players in the group with their own team.\n",
            "\n",
            "\"We're up against the puck. We're out of a strong team and we want to try to convince ourselves,\" said Upper, who now runs his players' game.\n",
            "\n",
            "It's unclear if the players in the group made more than three players in the group, and the other players would have to re-play this game.\n",
            "\n",
            "\"There's no question but then it's unfortunate,\" said Upper.\n",
            "\n",
            "The problem was that they thought they were going to change their mind against a team that they got the right fit for.\n",
            "\n",
            "\"When we're talking about this team, we want to find out how to play. We want to be with the same team.\"\n",
            "\n",
            "A similar situation has resulted in the club's second quarter.\n",
            "\n",
            "The first half of the game has not yet been finalized.\n",
            "\n",
            "That's because of the first quarter that the team had more than\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "His parents say they need to work when a car is needed to be able to take a hard hit, but they've tried to improve the driving.\n",
            "\n",
            "The car, under its control, is now on the roads.\n",
            "\n",
            "\"It's going to get to the bike because we're doing everything that goes through,\" he said.\n",
            "\n",
            "\"We're talking to San Francisco for a while and we're just looking at the bike so we can keep that bike safer on the road.\"\n",
            "\n",
            "\"It's going to make a lot of progress. But it's just trying to see what happens in San Francisco that happen, from the standpoint of being able to have a very hard time.\"\n",
            "\n",
            "The video from the San Francisco Chronicle reports that the bike's traffic had been delayed for a while, on the other end of the day, and that there were reports that the bike is safe, and that it wasn't clear what to expect as the bike's traffic was being delayed for about a month.\n",
            "\n",
            "\"We've got a lot of traffic,\" he said. \"If we get in the car, we'll start working with the bike.\"\n",
            "\n",
            "Sallyher is a longtime San Francisco resident and living in San Francisco, but he says he has the money to buy a bike or a bike.\n",
            "\n",
            "\"I think that's actually more than what we really want to do,\" he said. \"We're really looking for the bike and we actually are really trying to do that.\"\n",
            "\n",
            "The project works out to date that the bike is working on, but it's also not the only way to get it done, he said.\n",
            "\n",
            "Sallyher, who asked the city's chief executive of the Denver City Council, said she doesn't know about this, but she never saw it.\n",
            "\n",
            "\"I just wanted to keep that bike,\" she said. \"But we're really looking at the bike to make a difference, because we know what's going to have. We've got to keep the bike here and look at what this happened to our bike and that's a different project from the city.\"\n",
            "\n",
            "Sallyher has since moved from the city at the end of this year, and has a good track record of bike-friendly bike.\"\n",
            "\n",
            "Sallyher's bike has been slow in the last six years, and she has been the first to do that with the bike.<|endoftext|>I have always believed that the people\n",
            "---------------\n",
            "It's snow. Now, the first thing I've played in the past 15 years, I'm going to try to do a lot of things to do.\n",
            "\n",
            "So, as long as we've had those days, I used to it a little bit later and then they started working on our own. So I was in some kind of kind of playing on the phone and playing on the phone.\n",
            "\n",
            "I'm very proud of it so much, as long as I'd like to play the games, but you're like, \"Oh, you know what you do.\" And we both worked really hard on the iPhone.\n",
            "\n",
            "But I just don't care if I'm going to feel like I'm going to try but I mean I can't get them to play it on my iPhone, and I'm going to try and get them to play it on my iPhone and I'm going to be able to play it on my iPhone. And I think it's going to happen and I'm not going to try and just get me to play it on my iPhone and I'm going to try and get it on my iPhone, so I can try and get me to play it on my iPhone and I'm going to try and get myself back to the device.\n",
            "\n",
            "I'm really proud of it so much. I think I want to try and do whatever it takes to get in it. I'm hoping for the next few months and I'll be able to play the games in general, so I want to try and get me in the next couple weeks and months and I think it's going to be a huge fight with the new iPhone. I don't know if it's going to happen when I'm going to try and get it on my iPhone and then I'm going to try and get it on my iPhone.\n",
            "\n",
            "It's worth noting that the phone's iPhones are actually the result of the iPhones, so I'm not going to go through next week. But I think it's going to be a big fight for the iPhone. I'm also going to try and get the iPhone with it, to do it that's a great fight for the iPhone.\n",
            "\n",
            "If the iPhone was a great thing, I'd say back in the end and I think it's going to be a big fight for the iPhone, but I think it's a different story. I try to make this happen, and it's going to play it on the iPhone as a surprise.\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7w6jukAcC4-"
      },
      "outputs": [],
      "source": [
        "start = \"How to join a tbank-research?\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGvinMIicC4-",
        "outputId": "31fca2d9-2ce2-4b24-e454-339a61cc03bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to join a tbank-research?\n",
            "\n",
            "Yes, that is a big deal. It is worth the price. Where is it like the gold? There is no price for the bank and its debt is no one. But what does it do?\n",
            "\n",
            "Well, it is not a small amount of money on the banks. The amount of money they are being invested, which is an essential part of the asset. That means the money that the banks do not have to rely.\n",
            "\n",
            "But we have no confidence in our investments.\n",
            "\n",
            "But that said, the banks have no confidence in their investments, so they have to rely on the money. They have a lot of money and need to work. They have the money and money back from the money.\n",
            "\n",
            "But that is because many investors actually are doing a lot of money than money. I can say that while people have been doing a lot of money. I have a lot of cash.\n",
            "\n",
            "And they are getting the money they have.\n",
            "\n",
            "And they will be using them with a lot of money.\n",
            "\n",
            "And they are already spending their money. They have billions of dollars. And they have the money. They have trillions of dollars. They have to be able to look for the other way, and they have billions of dollars. So they have billions of dollars. And they have trillions of dollars.\n",
            "\n",
            "They are out of these things. It has to be them now.\n",
            "\n",
            "And they have millions of dollars.\n",
            "\n",
            "And they have billions of dollars every time they have, and they have billions of dollars over the next 10 years.\n",
            "\n",
            "And it is the one thing.\n",
            "\n",
            "So what is the bank doing?\n",
            "\n",
            "Well, your money has to be able to get the goods and services necessary. Of course, if you are in need of that, you go out there and take your money out and start doing it. You have to put everything in your place.\n",
            "\n",
            "And the money is going to make money. You have to invest in it. You have to put a tax on you.\n",
            "\n",
            "On top of all the money you want.\n",
            "\n",
            "And the money is going to be a big investment. How you can use the money.\n",
            "\n",
            "And you have to make money.\n",
            "\n",
            "And that is the kind of money you want to make money. You have to put all their money out of debt. You have to put all those funds in it, you have to put them\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "If you had a friend who, and who, as a journalist in a major economy manager and now a politician, I’d want to know. But I’d like to go to the D.E. to learn how to navigate the world, and learn how to do the job; they’d have to learn the lessons that I call “bail” to go to the N.C. to get through the D.E. and the D.E. as a journalist. The T.E. is a top-secret and senior staff writer, and I am very excited about the D.E.’s work.\n",
            "\n",
            "To receive a call, visit the D.E.’s workshop, follow or go to the N.E.’s conference, and find a friend who works on the project. He explains why he’s just the start.\n",
            "\n",
            "What about you, and how to make your team do it, and how to become a director?\n",
            "\n",
            "I’d love to talk to the general public about that, when I go to the N.C., and ask me to write a letter to David, for the first time. Here’s what we’re talking about:\n",
            "\n",
            "In the end, we’re going to be at least three people at a certain time. We’re going to be at least two people at that time and we’re going to be there for those around the world. And for everyone being a scientist, there’s a lot of people who are so great that we’re going to be in those offices and making sure we’re going to see the things we’re in and see how we’re going to be here. And it’s time to get started.\n",
            "\n",
            "What did you think of that?\n",
            "\n",
            "I’d like to talk to the people themselves. And I have a different attitude, I think. I’ve always been passionate about things, and it’s my job, and we’ve talked a lot and we’d like to think about the art, but I’d like to do a lot of work on that and I think it’s just fun. So, it’s kind of cool. It’s a way of doing that\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "The US economy is growing at a constant pace, with the unemployment rate as much as 12 percent as the economy increases. But it is seen that job growth is going up and the growth is still improving in the short term. But if we get to look to what is necessary, we’re still in the recession. And that’s a good thing, and this is a good thing.\n",
            "\n",
            "The US economy was in an economic crisis. For the first time since World War II, the US economy was rising for second purpose in 2009 and it’s still strong.\n",
            "\n",
            "The US economy was up 10 percent and the economy is still at a rate that the US economy is at a rate that has to be kept at a rate that is now going up and the jobs will rise. So the US economy is now the top priority for the United States. That’s not in the US economy that’s a bit higher than the US economy.\n",
            "\n",
            "“The reason I feel is that the US economy is going up, and the US economy is going up,” states the US economy is going up.\n",
            "\n",
            "A typical example of the US economy is not a single household.\n",
            "\n",
            "The US economy is very weak in the US economy, but with economic growth, the US economy is falling.\n",
            "\n",
            "It is a good thing to see what happened on the US economy. The US economy, which is very close to the US economy, is now rising and the US economy is already the world’s most vulnerable.\n",
            "\n",
            "The US economy is not just in the US, but in the last decade Europe is at its most vulnerable, as it has been for a very long time. And while the US economy is very weak in the US economy economy in the US, it is still not strong enough to make that economy worse.<|endoftext|>As the Philippines announced last week, the South Korean state of South Korea’s President Chaear Valley is preparing a state of emergency.\n",
            "\n",
            "The US, which is headed by the UN’s Ministry of Health, has decided to make plans to do so by sending letters to its citizens.\n",
            "\n",
            "The UN’s Office of National Health, which is headed by Prime Minister Shinzo Abe, is expected to speak to reporters when meeting a bilateral meeting between the two nations of South Korea and the US.\n",
            "\n",
            "The US and the US, China, the\n",
            "---------------\n",
            "How to join a tbank-research? Our best local data center is the largest economy, from the Central Bank and the Bank of America. It can provide more stable, more efficient, and better-developed products.\n",
            "\n",
            "You can check out our “Bitcoin Market” here.\n",
            "\n",
            "For more information about the market, click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for the table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click in that spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for an spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on the charts.\n",
            "\n",
            "Click there for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for it on a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on here for a spreadsheet.\n",
            "\n",
            "Click here for a table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click right for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click to see a list.\n",
            "\n",
            "Click here for a visualization.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click\n",
            "---------------\n",
            "How to join a tbank-research? Share: Send us a feed.\n",
            "\n",
            "About 2,000+ of our customers will receive about £7 to £4.50.\n",
            "\n",
            "Most of the over 50,000 customers will receive £4.50.\n",
            "\n",
            "“People who need a clean-up,” said Chris Harwinger, founder of the G2C board. “For example, they need to be able to do what they need to do.\n",
            "\n",
            "“When you got all the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money they want, the money they want.\n",
            "\n",
            "“I think we get the money you want, the money you want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, and the money their money they want isn’t being invested in.\n",
            "\n",
            "“We are very much invested in the money they want, the money they want. They want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want.”\n",
            "\n",
            "The cost of this scheme is a huge amount of money you can want.\n",
            "\n",
            "“I wanted so many people working in this small city. They are very wealthy people, I think, we can make a lot of money and it is a nice place to stay in the city and start with the money they want. But they are very wealthy people, they want.”\n",
            "\n",
            "Of the $1 million people in the city are from the public, so some of this is not a big deal but a lot of people have gotten so far.\n",
            "\n",
            "Over the past two years, we have seen only about 1,000 people living in the town of Qee. We have seen about 1,000 people working in the city, six million people working in the city, and 20 million people working in the city. We have seen over\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEY8uv_acC4-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vq0c3N68LhO"
      },
      "source": [
        "## GPT: Reflex-Router-Attention and SA const for all layers (7)\n",
        "\n",
        "**2 layer: SA for 6 head**\n",
        "\n",
        "**3-6 layer: Reflex-Router-Attention for 4 head for all previous hidden states: fitting linear combination; SA for 2 head (const for all layer)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97ZQAJPo8LhP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "\n",
        "class Router(nn.Module):\n",
        "  def __init__(self, n_head, i_layer):\n",
        "    super().__init__()\n",
        "    self.weights = nn.ParameterDict({str(i): nn.Parameter(torch.ones(i_layer)) for i in range(n_head - 2)}) #.to(device) #cuda\n",
        "\n",
        "  def forward(self, k_vs, head_i):\n",
        "    return sum(weight * k_v for weight, k_v in zip(self.weights[str(head_i)], k_vs))\n",
        "\n",
        "\n",
        "class ReflexRouterAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.router = Router(config.n_head, i_layer + 1)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        #i_hidden_state:\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        #reflex!\n",
        "        if self.model_type == 'simple' or (self.model_type == 'reflex' and hidden_states.__len__() < 3):\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v,\n",
        "                                                                 attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        else:\n",
        "            attn = []\n",
        "            for i in range(self.n_head - 2):\n",
        "              # for i's head (i <= 4) CA Router linear combination of existing hidden states from all prev layer and also current!\n",
        "              q_i, k_i, v_i = q[:, i,:, :], self.router([hidden_state[0][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i), self.router([hidden_state[1][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i)\n",
        "              attn.append(torch.nn.functional.scaled_dot_product_attention(q_i, k_i, v_i, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "              del q_i, k_i, v_i\n",
        "            #SA two heads\n",
        "            attn.append(torch.nn.functional.scaled_dot_product_attention(q[:, 4, :, :], k[:, 4, :, :], v[:, 4, :, :], attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "            attn.append(torch.nn.functional.scaled_dot_product_attention(q[:, 5, :, :], k[:, 5, :, :], v[:, 5, :, :], attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "            del q\n",
        "            y = torch.cat(attn, dim=1)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        torch.cuda.empty_cache()\n",
        "        return y, (k, v)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = ReflexRouterAttention(config, i_layer)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        attn_kv = self.attn(self.ln_1(x), hidden_states)\n",
        "        x = x + attn_kv[0] #y for 0\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_kv[1] #hidden_states_i for k,v for 1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config, _) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        hidden_states = []\n",
        "\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x, hidden_state = block(x, hidden_states)\n",
        "\n",
        "            # level2: for router\n",
        "            if self.config.model_type == 'reflex':\n",
        "                hidden_states.append(hidden_state)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "e68a96c5-8a2f-4823-b0b0-514091b8fc3c",
        "id": "LW-X65L18LhP"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.GPT"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjqJ40338LhP"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'reflex' # or 'simple'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dd6628-d5f0-4bf6-e91b-75269fcc05cf",
        "id": "cl-CpWZc8LhQ"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmhPY95M8LhQ"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'reflexAttnGPT_with_router+2headSA_ones_intil={config}_1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPEA1vaB8LhQ"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/reglex_attn_GPT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d632b2a-7168-4822-c2f1-d9a8234f456c",
        "id": "zy92XsVl8LhQ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 32,768\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEUpc3Ek8LhQ"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4ff09d-6feb-497d-f09a-a06e69737c32",
        "id": "e9KHW7Ms8LhR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cb7e85-72a8-4514-d8ea-511422dbd3f8",
        "id": "MP7ijwTe8LhR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 37, with 10,068 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c57ed7c8-95e2-4052-8eb0-d4db2e0826d2",
        "id": "dU-Qd9WZ8LhR"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:xxdsodzw) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">reflexAttnGPT_with_router+2headSA_ones_intil=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')_1</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/xxdsodzw' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/xxdsodzw</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241126_151535-xxdsodzw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:xxdsodzw). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241126_152043-t00k0w7z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/t00k0w7z' target=\"_blank\">reflexAttnGPT_with_router+2headSA_ones_intil=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')_1</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/t00k0w7z' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/t00k0w7z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-51-9f57c952888f> line 157 \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.230000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-51-9f57c952888f> line 104 \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.651000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-51-9f57c952888f> line 17 \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.732000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-51-9f57c952888f> line 45 \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:20:59.934000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-51-9f57c952888f> line 88 \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:21:00.083000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-51-9f57c952888f> line 26 \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 15:21:00.218000 7243 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 39.56 GiB of which 5.74 GiB is free. Process 87931 has 33.81 GiB memory in use. Of the allocated memory 31.77 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-c12f8252d313>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# evaluate the loss on train/val sets and write checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaster_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwandb_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-c12f8252d313>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0;31m# Restore the dynamic layer stack depth if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-9f57c952888f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;31m# if we are given some desired targets also calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;31m# inference-time mini-optimization: only forward the lm_head on the very last position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.14 GiB. GPU 0 has a total capacity of 39.56 GiB of which 5.74 GiB is free. Process 87931 has 33.81 GiB memory in use. Of the allocated memory 31.77 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTFD6x8hDB1q",
        "outputId": "1deac309-db38-4e7a-9f4c-cd306bbe419e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70184"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktPT8f2H8LhR"
      },
      "outputs": [],
      "source": [
        "# start = \"How to join a tbank-research?\"\n",
        "# start = \"How to join a tbank-research?\"\n",
        "start = \"It's snow.\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36bf217-1e21-40d1-fdd6-37c7d50535fb",
        "id": "1zlhO9H48LhS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-9f0241acb2d4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfba90a-639a-4ae9-8fc8-6574ebfe788d",
        "id": "uPZHYvek8LhS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's snow. It's still the same,\" she said.\n",
            "\n",
            "\"And I want it to lose.\"<|endoftext|>The Army soldiers who went to work in Afghanistan have been arrested on the site, including the last one for two years for the Veterans Affairs of the Army.\n",
            "\n",
            "By the time the Army will be charged, at least three thousand soldiers, the Defense Department says.\n",
            "\n",
            "The Army has been involved since March 31, which includes the new \"an\" system for troops in Afghanistan, Afghanistan, the Pentagon's Secretary of Defense, and the military's top commander.\n",
            "\n",
            "The more than 100 troops are expected to be in this year's first two years for the Pentagon and Lockheed Martin.\n",
            "\n",
            "The Pentagon has the second largest collection of detainees, which includes the 10,000 prisoners convicted of crimes in Afghanistan and Afghanistan, according to the National Centers for National Guard, the Defense Department said, according to the Pentagon's Office on the new report.\n",
            "\n",
            "The military has been in the ranks of military officers across the globe, with the highest level of job security since the Army is the top-ranked country's list of detainees.\n",
            "\n",
            "Story continues below advertisement\n",
            "\n",
            "The Army has more than 1,000 soldiers in Afghanistan, and more than 3,000 have been killed in Afghanistan, according to the National Institute for Civil Services.<|endoftext|>Image copyright EPA Image caption A new poll of 1,092 people had been killed on a mosque in the Khawramabad area by the police state of Bengaluru , a busing station in the capital, the capital of the country's capital, in the capital, the capital of the capital, the capital of the country's capital.\n",
            "\n",
            "The data collected by the police group showed that there were two types of homes in the capital, which are now around 8,000 people, most on the street in the capital, according to the figures, in the country of Bengaluru, a town in the capital, the capital, at the centre of the district.\n",
            "\n",
            "The toll group was at 0.2% in the first four years of the year, with the average time for the state's total population.\n",
            "\n",
            "Prime Minister Narendra Modi, he told reporters news footage about how his country's capital is now, that it wasn't the first time a police officer involved in the riots, but the prime minister for the next three years was just the result of his leadership in recent years.\n",
            "\n",
            "He said there were few people who\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"We have a good time.\"\n",
            "\n",
            "She said her team had a good day and another good Friday, which was on her way. She's now battling with her family, and she's so worried that the team has what it can do with it. \"It's a good day.\"\n",
            "\n",
            "She said the team was in shock and that it was a good start for a game. And just for the team and it was good news for her team and she's trying to get her own team.\n",
            "\n",
            "Her team's wife, Mark Sanchez, will be playing with the Houston Texans for the season.\n",
            "\n",
            "\"It's really good for the team,\" said Ross.\n",
            "\n",
            "The Bears also were in the midst of an argument about the team's progress.\n",
            "\n",
            "\"My team's been on track to win the game and I've never really got the support of my team and it's an argument that I think we can play with the team and our team. I was really excited about if it was up and we didn't have that opportunity to play against us in the playoffs and for the team.\"\n",
            "\n",
            "The Bears signed a $18 million deal, but the team never completed an offer.\n",
            "\n",
            "If the team had to have an offer, they could have been without a contract. So for now, the Bears have a lot more.\n",
            "\n",
            "Follow @DavidBatt<|endoftext|>\"The Great Britain\" redirects here. For more information, see the relevant links to the original version<|endoftext|>A new story of one of the most ridiculous stories on this planet in the history of the world.\n",
            "\n",
            "It may have been the first time (and it's a bit strange to see) but as you know it, the truth is that the world is struggling to grasp the facts of life in the first place. The story of the world goes on and on, and the experience of the future is how we move forward with those that we are at the center of the world. In the first week of Friday's 11th anniversary of the Great Britain period, the first world world where the United States was the world's first world world to see itself as the world's greatest nation to experience. Now, the first time the world has been to the realization that the world is changing, and what we know today is the world that the world is changing.\n",
            "\n",
            "It is the real world that we,\n",
            "\n",
            "The world is now, in fact, the world\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"It's the same place for the home, but no one else has to worry about,\" he said.\n",
            "\n",
            "\"You're being more, you're out of a path to where you have a good summer and a lot of things.\"\n",
            "\n",
            "As for the second quarter, players with their own team have been in their third quarter and their third quarter -- most of the first pair of games - which can't handle any team that can handle any team in the next three or seven years.\n",
            "\n",
            "Even six of the players in the group are competing in the second quarter, and that means they're ready to play the game.\n",
            "\n",
            "\"I think they're playing and in a way, they're in a way different place,\" said Alex \"to get their team up and compete for the next level.\"\n",
            "\n",
            "So it's possible they're playing with their own group and have to be prepared to play against them.\n",
            "\n",
            "\"There's no need for that,\" said Eric Upper. \"I think that's what we're doing, I think we'll be building a football game, and we're just going to be able to play like a football game.\"\n",
            "\n",
            "The first step was not for the third quarter but was to be released, with only a few of the players in the group.\n",
            "\n",
            "At the end of the fourth quarter, players played the first round, but only two were in the group.\n",
            "\n",
            "The second quarter continued to become the third quarter, with 12 players in the group with their own team.\n",
            "\n",
            "\"We're up against the puck. We're out of a strong team and we want to try to convince ourselves,\" said Upper, who now runs his players' game.\n",
            "\n",
            "It's unclear if the players in the group made more than three players in the group, and the other players would have to re-play this game.\n",
            "\n",
            "\"There's no question but then it's unfortunate,\" said Upper.\n",
            "\n",
            "The problem was that they thought they were going to change their mind against a team that they got the right fit for.\n",
            "\n",
            "\"When we're talking about this team, we want to find out how to play. We want to be with the same team.\"\n",
            "\n",
            "A similar situation has resulted in the club's second quarter.\n",
            "\n",
            "The first half of the game has not yet been finalized.\n",
            "\n",
            "That's because of the first quarter that the team had more than\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "His parents say they need to work when a car is needed to be able to take a hard hit, but they've tried to improve the driving.\n",
            "\n",
            "The car, under its control, is now on the roads.\n",
            "\n",
            "\"It's going to get to the bike because we're doing everything that goes through,\" he said.\n",
            "\n",
            "\"We're talking to San Francisco for a while and we're just looking at the bike so we can keep that bike safer on the road.\"\n",
            "\n",
            "\"It's going to make a lot of progress. But it's just trying to see what happens in San Francisco that happen, from the standpoint of being able to have a very hard time.\"\n",
            "\n",
            "The video from the San Francisco Chronicle reports that the bike's traffic had been delayed for a while, on the other end of the day, and that there were reports that the bike is safe, and that it wasn't clear what to expect as the bike's traffic was being delayed for about a month.\n",
            "\n",
            "\"We've got a lot of traffic,\" he said. \"If we get in the car, we'll start working with the bike.\"\n",
            "\n",
            "Sallyher is a longtime San Francisco resident and living in San Francisco, but he says he has the money to buy a bike or a bike.\n",
            "\n",
            "\"I think that's actually more than what we really want to do,\" he said. \"We're really looking for the bike and we actually are really trying to do that.\"\n",
            "\n",
            "The project works out to date that the bike is working on, but it's also not the only way to get it done, he said.\n",
            "\n",
            "Sallyher, who asked the city's chief executive of the Denver City Council, said she doesn't know about this, but she never saw it.\n",
            "\n",
            "\"I just wanted to keep that bike,\" she said. \"But we're really looking at the bike to make a difference, because we know what's going to have. We've got to keep the bike here and look at what this happened to our bike and that's a different project from the city.\"\n",
            "\n",
            "Sallyher has since moved from the city at the end of this year, and has a good track record of bike-friendly bike.\"\n",
            "\n",
            "Sallyher's bike has been slow in the last six years, and she has been the first to do that with the bike.<|endoftext|>I have always believed that the people\n",
            "---------------\n",
            "It's snow. Now, the first thing I've played in the past 15 years, I'm going to try to do a lot of things to do.\n",
            "\n",
            "So, as long as we've had those days, I used to it a little bit later and then they started working on our own. So I was in some kind of kind of playing on the phone and playing on the phone.\n",
            "\n",
            "I'm very proud of it so much, as long as I'd like to play the games, but you're like, \"Oh, you know what you do.\" And we both worked really hard on the iPhone.\n",
            "\n",
            "But I just don't care if I'm going to feel like I'm going to try but I mean I can't get them to play it on my iPhone, and I'm going to try and get them to play it on my iPhone and I'm going to be able to play it on my iPhone. And I think it's going to happen and I'm not going to try and just get me to play it on my iPhone and I'm going to try and get it on my iPhone, so I can try and get me to play it on my iPhone and I'm going to try and get myself back to the device.\n",
            "\n",
            "I'm really proud of it so much. I think I want to try and do whatever it takes to get in it. I'm hoping for the next few months and I'll be able to play the games in general, so I want to try and get me in the next couple weeks and months and I think it's going to be a huge fight with the new iPhone. I don't know if it's going to happen when I'm going to try and get it on my iPhone and then I'm going to try and get it on my iPhone.\n",
            "\n",
            "It's worth noting that the phone's iPhones are actually the result of the iPhones, so I'm not going to go through next week. But I think it's going to be a big fight for the iPhone. I'm also going to try and get the iPhone with it, to do it that's a great fight for the iPhone.\n",
            "\n",
            "If the iPhone was a great thing, I'd say back in the end and I think it's going to be a big fight for the iPhone, but I think it's a different story. I try to make this happen, and it's going to play it on the iPhone as a surprise.\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLCnEIKb8LhS"
      },
      "outputs": [],
      "source": [
        "start = \"How to join a tbank-research?\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fca2d9-2ce2-4b24-e454-339a61cc03bd",
        "id": "U6dKH0jY8LhS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to join a tbank-research?\n",
            "\n",
            "Yes, that is a big deal. It is worth the price. Where is it like the gold? There is no price for the bank and its debt is no one. But what does it do?\n",
            "\n",
            "Well, it is not a small amount of money on the banks. The amount of money they are being invested, which is an essential part of the asset. That means the money that the banks do not have to rely.\n",
            "\n",
            "But we have no confidence in our investments.\n",
            "\n",
            "But that said, the banks have no confidence in their investments, so they have to rely on the money. They have a lot of money and need to work. They have the money and money back from the money.\n",
            "\n",
            "But that is because many investors actually are doing a lot of money than money. I can say that while people have been doing a lot of money. I have a lot of cash.\n",
            "\n",
            "And they are getting the money they have.\n",
            "\n",
            "And they will be using them with a lot of money.\n",
            "\n",
            "And they are already spending their money. They have billions of dollars. And they have the money. They have trillions of dollars. They have to be able to look for the other way, and they have billions of dollars. So they have billions of dollars. And they have trillions of dollars.\n",
            "\n",
            "They are out of these things. It has to be them now.\n",
            "\n",
            "And they have millions of dollars.\n",
            "\n",
            "And they have billions of dollars every time they have, and they have billions of dollars over the next 10 years.\n",
            "\n",
            "And it is the one thing.\n",
            "\n",
            "So what is the bank doing?\n",
            "\n",
            "Well, your money has to be able to get the goods and services necessary. Of course, if you are in need of that, you go out there and take your money out and start doing it. You have to put everything in your place.\n",
            "\n",
            "And the money is going to make money. You have to invest in it. You have to put a tax on you.\n",
            "\n",
            "On top of all the money you want.\n",
            "\n",
            "And the money is going to be a big investment. How you can use the money.\n",
            "\n",
            "And you have to make money.\n",
            "\n",
            "And that is the kind of money you want to make money. You have to put all their money out of debt. You have to put all those funds in it, you have to put them\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "If you had a friend who, and who, as a journalist in a major economy manager and now a politician, I’d want to know. But I’d like to go to the D.E. to learn how to navigate the world, and learn how to do the job; they’d have to learn the lessons that I call “bail” to go to the N.C. to get through the D.E. and the D.E. as a journalist. The T.E. is a top-secret and senior staff writer, and I am very excited about the D.E.’s work.\n",
            "\n",
            "To receive a call, visit the D.E.’s workshop, follow or go to the N.E.’s conference, and find a friend who works on the project. He explains why he’s just the start.\n",
            "\n",
            "What about you, and how to make your team do it, and how to become a director?\n",
            "\n",
            "I’d love to talk to the general public about that, when I go to the N.C., and ask me to write a letter to David, for the first time. Here’s what we’re talking about:\n",
            "\n",
            "In the end, we’re going to be at least three people at a certain time. We’re going to be at least two people at that time and we’re going to be there for those around the world. And for everyone being a scientist, there’s a lot of people who are so great that we’re going to be in those offices and making sure we’re going to see the things we’re in and see how we’re going to be here. And it’s time to get started.\n",
            "\n",
            "What did you think of that?\n",
            "\n",
            "I’d like to talk to the people themselves. And I have a different attitude, I think. I’ve always been passionate about things, and it’s my job, and we’ve talked a lot and we’d like to think about the art, but I’d like to do a lot of work on that and I think it’s just fun. So, it’s kind of cool. It’s a way of doing that\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "The US economy is growing at a constant pace, with the unemployment rate as much as 12 percent as the economy increases. But it is seen that job growth is going up and the growth is still improving in the short term. But if we get to look to what is necessary, we’re still in the recession. And that’s a good thing, and this is a good thing.\n",
            "\n",
            "The US economy was in an economic crisis. For the first time since World War II, the US economy was rising for second purpose in 2009 and it’s still strong.\n",
            "\n",
            "The US economy was up 10 percent and the economy is still at a rate that the US economy is at a rate that has to be kept at a rate that is now going up and the jobs will rise. So the US economy is now the top priority for the United States. That’s not in the US economy that’s a bit higher than the US economy.\n",
            "\n",
            "“The reason I feel is that the US economy is going up, and the US economy is going up,” states the US economy is going up.\n",
            "\n",
            "A typical example of the US economy is not a single household.\n",
            "\n",
            "The US economy is very weak in the US economy, but with economic growth, the US economy is falling.\n",
            "\n",
            "It is a good thing to see what happened on the US economy. The US economy, which is very close to the US economy, is now rising and the US economy is already the world’s most vulnerable.\n",
            "\n",
            "The US economy is not just in the US, but in the last decade Europe is at its most vulnerable, as it has been for a very long time. And while the US economy is very weak in the US economy economy in the US, it is still not strong enough to make that economy worse.<|endoftext|>As the Philippines announced last week, the South Korean state of South Korea’s President Chaear Valley is preparing a state of emergency.\n",
            "\n",
            "The US, which is headed by the UN’s Ministry of Health, has decided to make plans to do so by sending letters to its citizens.\n",
            "\n",
            "The UN’s Office of National Health, which is headed by Prime Minister Shinzo Abe, is expected to speak to reporters when meeting a bilateral meeting between the two nations of South Korea and the US.\n",
            "\n",
            "The US and the US, China, the\n",
            "---------------\n",
            "How to join a tbank-research? Our best local data center is the largest economy, from the Central Bank and the Bank of America. It can provide more stable, more efficient, and better-developed products.\n",
            "\n",
            "You can check out our “Bitcoin Market” here.\n",
            "\n",
            "For more information about the market, click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for the table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click in that spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for an spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on the charts.\n",
            "\n",
            "Click there for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for it on a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on here for a spreadsheet.\n",
            "\n",
            "Click here for a table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click right for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click to see a list.\n",
            "\n",
            "Click here for a visualization.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click\n",
            "---------------\n",
            "How to join a tbank-research? Share: Send us a feed.\n",
            "\n",
            "About 2,000+ of our customers will receive about £7 to £4.50.\n",
            "\n",
            "Most of the over 50,000 customers will receive £4.50.\n",
            "\n",
            "“People who need a clean-up,” said Chris Harwinger, founder of the G2C board. “For example, they need to be able to do what they need to do.\n",
            "\n",
            "“When you got all the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money they want, the money they want.\n",
            "\n",
            "“I think we get the money you want, the money you want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, and the money their money they want isn’t being invested in.\n",
            "\n",
            "“We are very much invested in the money they want, the money they want. They want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want.”\n",
            "\n",
            "The cost of this scheme is a huge amount of money you can want.\n",
            "\n",
            "“I wanted so many people working in this small city. They are very wealthy people, I think, we can make a lot of money and it is a nice place to stay in the city and start with the money they want. But they are very wealthy people, they want.”\n",
            "\n",
            "Of the $1 million people in the city are from the public, so some of this is not a big deal but a lot of people have gotten so far.\n",
            "\n",
            "Over the past two years, we have seen only about 1,000 people living in the town of Qee. We have seen about 1,000 people working in the city, six million people working in the city, and 20 million people working in the city. We have seen over\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dAIMpJH8LhS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQlU0qVoAS7j"
      },
      "source": [
        "## GPT: Reflex-Router-Attention for all layers (8)\n",
        "\n",
        "**2 layer: SA for 6 head**\n",
        "\n",
        "**3-6 layer: Reflex-Router-Attention for 6 head for all previous hidden states: fitting linear combination**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czCeWOpgAS7k"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "\n",
        "class Router(nn.Module):\n",
        "  def __init__(self, n_head, i_layer):\n",
        "    super().__init__()\n",
        "    self.weights = nn.ParameterDict({str(i): nn.Parameter(torch.ones(i_layer)) for i in range(n_head)}) #.to(device) #cuda\n",
        "\n",
        "  def forward(self, k_vs, head_i):\n",
        "    return sum(weight * k_v for weight, k_v in zip(self.weights[str(head_i)], k_vs))\n",
        "\n",
        "\n",
        "class ReflexRouterAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.router = Router(config.n_head, i_layer + 1)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        #i_hidden_state:\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        #reflex!\n",
        "        if self.model_type == 'simple' or (self.model_type == 'reflex' and hidden_states.__len__() < 3):\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v,\n",
        "                                                                 attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        else:\n",
        "            attn = []\n",
        "            for i in range(self.n_head):\n",
        "              # for i's head\n",
        "              q_i, k_i, v_i = q[:, i,:, :], self.router([hidden_state[0][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i), self.router([hidden_state[1][:, i, :, :] for hidden_state in (hidden_states + [(k, v)])], i)\n",
        "              attn.append(torch.nn.functional.scaled_dot_product_attention(q_i, k_i, v_i, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "              del q_i, k_i, v_i\n",
        "            y = torch.cat(attn, dim=1)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y, (k, v)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = ReflexRouterAttention(config, i_layer)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        attn_kv = self.attn(self.ln_1(x), hidden_states)\n",
        "        x = x + attn_kv[0] #y for 0\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_kv[1] #hidden_states_i for k,v for 1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config, _) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        hidden_states = []\n",
        "\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x, hidden_state = block(x, hidden_states)\n",
        "\n",
        "            # level2: for router\n",
        "            if self.config.model_type == 'reflex':\n",
        "                hidden_states.append(hidden_state)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "eff35871-c2d5-410f-c6fb-327be0f361ab",
        "id": "jnDS-CBtAS7l"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.GPT"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-Nw8xYGAS7l"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'reflex' # or 'simple'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67921291-02b7-46c0-d916-eeabf790aec7",
        "id": "boNa-AQZAS7l"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_WzS4_2AS7m"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'reflexAttnGPT_with_router={config}1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNeWX60TAS7m"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/reglex_attn_GPT'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a288b7b1-c666-4055-90ea-b12850b0d3fd",
        "id": "wCSFnvjFAS7m"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 32,768\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Ozmd5lAS7n"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63645325-6438-4f5f-dd64-6dad252b0017",
        "id": "ctf5CSClAS7n"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc5bb8b-cadc-47c4-f2ab-855404e91f0d",
        "id": "jzdDEn5jAS7n"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 49, with 10,110 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "5d5d5cde-0966-4694-d0ba-d18f6e9f3c76",
        "id": "tHcyeDFKAS7n"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:lmgwg1fm) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">reflexAttnGPT_with_router=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/lmgwg1fm' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/lmgwg1fm</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241126_150806-lmgwg1fm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:lmgwg1fm). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241126_150834-j6fqr625</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/j6fqr625' target=\"_blank\">reflexAttnGPT_with_router=GPTConfig(batch_size=32, block_size=1024, vocab_size=50304, n_layer=6, n_head=6, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/j6fqr625' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/j6fqr625</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'eval'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c12f8252d313>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# evaluate the loss on train/val sets and write checkpoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaster_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwandb_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-c12f8252d313>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'eval'"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcBGeSrFAS7o"
      },
      "outputs": [],
      "source": [
        "# start = \"How to join a tbank-research?\"\n",
        "# start = \"How to join a tbank-research?\"\n",
        "start = \"It's snow.\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36bf217-1e21-40d1-fdd6-37c7d50535fb",
        "id": "J3GfveOuAS7o"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-9f0241acb2d4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfba90a-639a-4ae9-8fc8-6574ebfe788d",
        "id": "RzNaRCVuAS7o"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It's snow. It's still the same,\" she said.\n",
            "\n",
            "\"And I want it to lose.\"<|endoftext|>The Army soldiers who went to work in Afghanistan have been arrested on the site, including the last one for two years for the Veterans Affairs of the Army.\n",
            "\n",
            "By the time the Army will be charged, at least three thousand soldiers, the Defense Department says.\n",
            "\n",
            "The Army has been involved since March 31, which includes the new \"an\" system for troops in Afghanistan, Afghanistan, the Pentagon's Secretary of Defense, and the military's top commander.\n",
            "\n",
            "The more than 100 troops are expected to be in this year's first two years for the Pentagon and Lockheed Martin.\n",
            "\n",
            "The Pentagon has the second largest collection of detainees, which includes the 10,000 prisoners convicted of crimes in Afghanistan and Afghanistan, according to the National Centers for National Guard, the Defense Department said, according to the Pentagon's Office on the new report.\n",
            "\n",
            "The military has been in the ranks of military officers across the globe, with the highest level of job security since the Army is the top-ranked country's list of detainees.\n",
            "\n",
            "Story continues below advertisement\n",
            "\n",
            "The Army has more than 1,000 soldiers in Afghanistan, and more than 3,000 have been killed in Afghanistan, according to the National Institute for Civil Services.<|endoftext|>Image copyright EPA Image caption A new poll of 1,092 people had been killed on a mosque in the Khawramabad area by the police state of Bengaluru , a busing station in the capital, the capital of the country's capital, in the capital, the capital of the capital, the capital of the country's capital.\n",
            "\n",
            "The data collected by the police group showed that there were two types of homes in the capital, which are now around 8,000 people, most on the street in the capital, according to the figures, in the country of Bengaluru, a town in the capital, the capital, at the centre of the district.\n",
            "\n",
            "The toll group was at 0.2% in the first four years of the year, with the average time for the state's total population.\n",
            "\n",
            "Prime Minister Narendra Modi, he told reporters news footage about how his country's capital is now, that it wasn't the first time a police officer involved in the riots, but the prime minister for the next three years was just the result of his leadership in recent years.\n",
            "\n",
            "He said there were few people who\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"We have a good time.\"\n",
            "\n",
            "She said her team had a good day and another good Friday, which was on her way. She's now battling with her family, and she's so worried that the team has what it can do with it. \"It's a good day.\"\n",
            "\n",
            "She said the team was in shock and that it was a good start for a game. And just for the team and it was good news for her team and she's trying to get her own team.\n",
            "\n",
            "Her team's wife, Mark Sanchez, will be playing with the Houston Texans for the season.\n",
            "\n",
            "\"It's really good for the team,\" said Ross.\n",
            "\n",
            "The Bears also were in the midst of an argument about the team's progress.\n",
            "\n",
            "\"My team's been on track to win the game and I've never really got the support of my team and it's an argument that I think we can play with the team and our team. I was really excited about if it was up and we didn't have that opportunity to play against us in the playoffs and for the team.\"\n",
            "\n",
            "The Bears signed a $18 million deal, but the team never completed an offer.\n",
            "\n",
            "If the team had to have an offer, they could have been without a contract. So for now, the Bears have a lot more.\n",
            "\n",
            "Follow @DavidBatt<|endoftext|>\"The Great Britain\" redirects here. For more information, see the relevant links to the original version<|endoftext|>A new story of one of the most ridiculous stories on this planet in the history of the world.\n",
            "\n",
            "It may have been the first time (and it's a bit strange to see) but as you know it, the truth is that the world is struggling to grasp the facts of life in the first place. The story of the world goes on and on, and the experience of the future is how we move forward with those that we are at the center of the world. In the first week of Friday's 11th anniversary of the Great Britain period, the first world world where the United States was the world's first world world to see itself as the world's greatest nation to experience. Now, the first time the world has been to the realization that the world is changing, and what we know today is the world that the world is changing.\n",
            "\n",
            "It is the real world that we,\n",
            "\n",
            "The world is now, in fact, the world\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"It's the same place for the home, but no one else has to worry about,\" he said.\n",
            "\n",
            "\"You're being more, you're out of a path to where you have a good summer and a lot of things.\"\n",
            "\n",
            "As for the second quarter, players with their own team have been in their third quarter and their third quarter -- most of the first pair of games - which can't handle any team that can handle any team in the next three or seven years.\n",
            "\n",
            "Even six of the players in the group are competing in the second quarter, and that means they're ready to play the game.\n",
            "\n",
            "\"I think they're playing and in a way, they're in a way different place,\" said Alex \"to get their team up and compete for the next level.\"\n",
            "\n",
            "So it's possible they're playing with their own group and have to be prepared to play against them.\n",
            "\n",
            "\"There's no need for that,\" said Eric Upper. \"I think that's what we're doing, I think we'll be building a football game, and we're just going to be able to play like a football game.\"\n",
            "\n",
            "The first step was not for the third quarter but was to be released, with only a few of the players in the group.\n",
            "\n",
            "At the end of the fourth quarter, players played the first round, but only two were in the group.\n",
            "\n",
            "The second quarter continued to become the third quarter, with 12 players in the group with their own team.\n",
            "\n",
            "\"We're up against the puck. We're out of a strong team and we want to try to convince ourselves,\" said Upper, who now runs his players' game.\n",
            "\n",
            "It's unclear if the players in the group made more than three players in the group, and the other players would have to re-play this game.\n",
            "\n",
            "\"There's no question but then it's unfortunate,\" said Upper.\n",
            "\n",
            "The problem was that they thought they were going to change their mind against a team that they got the right fit for.\n",
            "\n",
            "\"When we're talking about this team, we want to find out how to play. We want to be with the same team.\"\n",
            "\n",
            "A similar situation has resulted in the club's second quarter.\n",
            "\n",
            "The first half of the game has not yet been finalized.\n",
            "\n",
            "That's because of the first quarter that the team had more than\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "His parents say they need to work when a car is needed to be able to take a hard hit, but they've tried to improve the driving.\n",
            "\n",
            "The car, under its control, is now on the roads.\n",
            "\n",
            "\"It's going to get to the bike because we're doing everything that goes through,\" he said.\n",
            "\n",
            "\"We're talking to San Francisco for a while and we're just looking at the bike so we can keep that bike safer on the road.\"\n",
            "\n",
            "\"It's going to make a lot of progress. But it's just trying to see what happens in San Francisco that happen, from the standpoint of being able to have a very hard time.\"\n",
            "\n",
            "The video from the San Francisco Chronicle reports that the bike's traffic had been delayed for a while, on the other end of the day, and that there were reports that the bike is safe, and that it wasn't clear what to expect as the bike's traffic was being delayed for about a month.\n",
            "\n",
            "\"We've got a lot of traffic,\" he said. \"If we get in the car, we'll start working with the bike.\"\n",
            "\n",
            "Sallyher is a longtime San Francisco resident and living in San Francisco, but he says he has the money to buy a bike or a bike.\n",
            "\n",
            "\"I think that's actually more than what we really want to do,\" he said. \"We're really looking for the bike and we actually are really trying to do that.\"\n",
            "\n",
            "The project works out to date that the bike is working on, but it's also not the only way to get it done, he said.\n",
            "\n",
            "Sallyher, who asked the city's chief executive of the Denver City Council, said she doesn't know about this, but she never saw it.\n",
            "\n",
            "\"I just wanted to keep that bike,\" she said. \"But we're really looking at the bike to make a difference, because we know what's going to have. We've got to keep the bike here and look at what this happened to our bike and that's a different project from the city.\"\n",
            "\n",
            "Sallyher has since moved from the city at the end of this year, and has a good track record of bike-friendly bike.\"\n",
            "\n",
            "Sallyher's bike has been slow in the last six years, and she has been the first to do that with the bike.<|endoftext|>I have always believed that the people\n",
            "---------------\n",
            "It's snow. Now, the first thing I've played in the past 15 years, I'm going to try to do a lot of things to do.\n",
            "\n",
            "So, as long as we've had those days, I used to it a little bit later and then they started working on our own. So I was in some kind of kind of playing on the phone and playing on the phone.\n",
            "\n",
            "I'm very proud of it so much, as long as I'd like to play the games, but you're like, \"Oh, you know what you do.\" And we both worked really hard on the iPhone.\n",
            "\n",
            "But I just don't care if I'm going to feel like I'm going to try but I mean I can't get them to play it on my iPhone, and I'm going to try and get them to play it on my iPhone and I'm going to be able to play it on my iPhone. And I think it's going to happen and I'm not going to try and just get me to play it on my iPhone and I'm going to try and get it on my iPhone, so I can try and get me to play it on my iPhone and I'm going to try and get myself back to the device.\n",
            "\n",
            "I'm really proud of it so much. I think I want to try and do whatever it takes to get in it. I'm hoping for the next few months and I'll be able to play the games in general, so I want to try and get me in the next couple weeks and months and I think it's going to be a huge fight with the new iPhone. I don't know if it's going to happen when I'm going to try and get it on my iPhone and then I'm going to try and get it on my iPhone.\n",
            "\n",
            "It's worth noting that the phone's iPhones are actually the result of the iPhones, so I'm not going to go through next week. But I think it's going to be a big fight for the iPhone. I'm also going to try and get the iPhone with it, to do it that's a great fight for the iPhone.\n",
            "\n",
            "If the iPhone was a great thing, I'd say back in the end and I think it's going to be a big fight for the iPhone, but I think it's a different story. I try to make this happen, and it's going to play it on the iPhone as a surprise.\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vGDOKHjAS7o"
      },
      "outputs": [],
      "source": [
        "start = \"How to join a tbank-research?\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fca2d9-2ce2-4b24-e454-339a61cc03bd",
        "id": "5ZBCdXBBAS7p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How to join a tbank-research?\n",
            "\n",
            "Yes, that is a big deal. It is worth the price. Where is it like the gold? There is no price for the bank and its debt is no one. But what does it do?\n",
            "\n",
            "Well, it is not a small amount of money on the banks. The amount of money they are being invested, which is an essential part of the asset. That means the money that the banks do not have to rely.\n",
            "\n",
            "But we have no confidence in our investments.\n",
            "\n",
            "But that said, the banks have no confidence in their investments, so they have to rely on the money. They have a lot of money and need to work. They have the money and money back from the money.\n",
            "\n",
            "But that is because many investors actually are doing a lot of money than money. I can say that while people have been doing a lot of money. I have a lot of cash.\n",
            "\n",
            "And they are getting the money they have.\n",
            "\n",
            "And they will be using them with a lot of money.\n",
            "\n",
            "And they are already spending their money. They have billions of dollars. And they have the money. They have trillions of dollars. They have to be able to look for the other way, and they have billions of dollars. So they have billions of dollars. And they have trillions of dollars.\n",
            "\n",
            "They are out of these things. It has to be them now.\n",
            "\n",
            "And they have millions of dollars.\n",
            "\n",
            "And they have billions of dollars every time they have, and they have billions of dollars over the next 10 years.\n",
            "\n",
            "And it is the one thing.\n",
            "\n",
            "So what is the bank doing?\n",
            "\n",
            "Well, your money has to be able to get the goods and services necessary. Of course, if you are in need of that, you go out there and take your money out and start doing it. You have to put everything in your place.\n",
            "\n",
            "And the money is going to make money. You have to invest in it. You have to put a tax on you.\n",
            "\n",
            "On top of all the money you want.\n",
            "\n",
            "And the money is going to be a big investment. How you can use the money.\n",
            "\n",
            "And you have to make money.\n",
            "\n",
            "And that is the kind of money you want to make money. You have to put all their money out of debt. You have to put all those funds in it, you have to put them\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "If you had a friend who, and who, as a journalist in a major economy manager and now a politician, I’d want to know. But I’d like to go to the D.E. to learn how to navigate the world, and learn how to do the job; they’d have to learn the lessons that I call “bail” to go to the N.C. to get through the D.E. and the D.E. as a journalist. The T.E. is a top-secret and senior staff writer, and I am very excited about the D.E.’s work.\n",
            "\n",
            "To receive a call, visit the D.E.’s workshop, follow or go to the N.E.’s conference, and find a friend who works on the project. He explains why he’s just the start.\n",
            "\n",
            "What about you, and how to make your team do it, and how to become a director?\n",
            "\n",
            "I’d love to talk to the general public about that, when I go to the N.C., and ask me to write a letter to David, for the first time. Here’s what we’re talking about:\n",
            "\n",
            "In the end, we’re going to be at least three people at a certain time. We’re going to be at least two people at that time and we’re going to be there for those around the world. And for everyone being a scientist, there’s a lot of people who are so great that we’re going to be in those offices and making sure we’re going to see the things we’re in and see how we’re going to be here. And it’s time to get started.\n",
            "\n",
            "What did you think of that?\n",
            "\n",
            "I’d like to talk to the people themselves. And I have a different attitude, I think. I’ve always been passionate about things, and it’s my job, and we’ve talked a lot and we’d like to think about the art, but I’d like to do a lot of work on that and I think it’s just fun. So, it’s kind of cool. It’s a way of doing that\n",
            "---------------\n",
            "How to join a tbank-research?\n",
            "\n",
            "The US economy is growing at a constant pace, with the unemployment rate as much as 12 percent as the economy increases. But it is seen that job growth is going up and the growth is still improving in the short term. But if we get to look to what is necessary, we’re still in the recession. And that’s a good thing, and this is a good thing.\n",
            "\n",
            "The US economy was in an economic crisis. For the first time since World War II, the US economy was rising for second purpose in 2009 and it’s still strong.\n",
            "\n",
            "The US economy was up 10 percent and the economy is still at a rate that the US economy is at a rate that has to be kept at a rate that is now going up and the jobs will rise. So the US economy is now the top priority for the United States. That’s not in the US economy that’s a bit higher than the US economy.\n",
            "\n",
            "“The reason I feel is that the US economy is going up, and the US economy is going up,” states the US economy is going up.\n",
            "\n",
            "A typical example of the US economy is not a single household.\n",
            "\n",
            "The US economy is very weak in the US economy, but with economic growth, the US economy is falling.\n",
            "\n",
            "It is a good thing to see what happened on the US economy. The US economy, which is very close to the US economy, is now rising and the US economy is already the world’s most vulnerable.\n",
            "\n",
            "The US economy is not just in the US, but in the last decade Europe is at its most vulnerable, as it has been for a very long time. And while the US economy is very weak in the US economy economy in the US, it is still not strong enough to make that economy worse.<|endoftext|>As the Philippines announced last week, the South Korean state of South Korea’s President Chaear Valley is preparing a state of emergency.\n",
            "\n",
            "The US, which is headed by the UN’s Ministry of Health, has decided to make plans to do so by sending letters to its citizens.\n",
            "\n",
            "The UN’s Office of National Health, which is headed by Prime Minister Shinzo Abe, is expected to speak to reporters when meeting a bilateral meeting between the two nations of South Korea and the US.\n",
            "\n",
            "The US and the US, China, the\n",
            "---------------\n",
            "How to join a tbank-research? Our best local data center is the largest economy, from the Central Bank and the Bank of America. It can provide more stable, more efficient, and better-developed products.\n",
            "\n",
            "You can check out our “Bitcoin Market” here.\n",
            "\n",
            "For more information about the market, click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for the table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click in that spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for an spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on the charts.\n",
            "\n",
            "Click there for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for it on a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click on here for a spreadsheet.\n",
            "\n",
            "Click here for a table.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click right for a calculator.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click to see a list.\n",
            "\n",
            "Click here for a visualization.\n",
            "\n",
            "Click here for a spreadsheet.\n",
            "\n",
            "Click here for a calculator.\n",
            "\n",
            "Click\n",
            "---------------\n",
            "How to join a tbank-research? Share: Send us a feed.\n",
            "\n",
            "About 2,000+ of our customers will receive about £7 to £4.50.\n",
            "\n",
            "Most of the over 50,000 customers will receive £4.50.\n",
            "\n",
            "“People who need a clean-up,” said Chris Harwinger, founder of the G2C board. “For example, they need to be able to do what they need to do.\n",
            "\n",
            "“When you got all the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money you want, the money they want, the money they want.\n",
            "\n",
            "“I think we get the money you want, the money you want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, and the money their money they want isn’t being invested in.\n",
            "\n",
            "“We are very much invested in the money they want, the money they want. They want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want, the money they want.”\n",
            "\n",
            "The cost of this scheme is a huge amount of money you can want.\n",
            "\n",
            "“I wanted so many people working in this small city. They are very wealthy people, I think, we can make a lot of money and it is a nice place to stay in the city and start with the money they want. But they are very wealthy people, they want.”\n",
            "\n",
            "Of the $1 million people in the city are from the public, so some of this is not a big deal but a lot of people have gotten so far.\n",
            "\n",
            "Over the past two years, we have seen only about 1,000 people living in the town of Qee. We have seen about 1,000 people working in the city, six million people working in the city, and 20 million people working in the city. We have seen over\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIwOQe_tAS7p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK_AV9EI4ROL"
      },
      "source": [
        "## GPT: Reflex-Router-Attention: unit-init router for attention tensors (9-10-11)\n",
        "\n",
        "1. **4layer with 2head**\n",
        "\n",
        "**2 layer: SA for 2 head**\n",
        "\n",
        "**2-4 layer: Reflex-Router-Attention for 2 head for all previous hidden states: fitting linear combination**\n",
        "\n",
        "---\n",
        "\n",
        "2. **6layer for 4 head**\n",
        "\n",
        "**2 layer: SA for 4 head**\n",
        "\n",
        "**2-6 layer: Reflex-Router-Attention for 4 head for all previous hidden states: fitting linear combination**\n",
        "\n",
        "---\n",
        "\n",
        "3. **8layer for 6 head**\n",
        "\n",
        "**2 layer: SA for 6 head**\n",
        "\n",
        "**2-8 layer: Reflex-Router-Attention for 6 head for all previous hidden states: fitting linear combination**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A226o6tG4ROM"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-6)\n",
        "\n",
        "class Router(nn.Module):\n",
        "    def __init__(self, n_head, i_layer):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.ones(n_head * i_layer))\n",
        "\n",
        "    def forward(self, lin_comb_i, head_i):\n",
        "        return torch.sum(self.weights[head_i].view(1, 1, 1, -1) * torch.stack(lin_comb_i, dim=0), dim=0)\n",
        "\n",
        "class ReflexRouterAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.router = Router(config.n_head, i_layer + 1)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head # n_heads = 8\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.model_type = config.model_type\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        #i_hidden_state:\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        #reflex!\n",
        "        if self.model_type == 'simple' or (self.model_type == 'reflex' and hidden_states.__len__() < 3):\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v,\n",
        "                                                                 attn_mask=None,\n",
        "                                                                 dropout_p=self.dropout if self.training else 0,\n",
        "                                                                 is_causal=True)\n",
        "        else:\n",
        "            attn = []\n",
        "            for i in range(self.n_head):\n",
        "              lin_comb_i = []\n",
        "              for k_hidden, v_hidden in (hidden_states + [(k, v)]):\n",
        "                lin_comb_i.append(torch.nn.functional.scaled_dot_product_attention(q[:, i,:, :], k_hidden[:, i, :, :], v_hidden[:, i, :, :], attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True))\n",
        "              attn.append(self.router(lin_comb_i, i))\n",
        "              del lin_comb_i\n",
        "            y = torch.cat(attn, dim=1)\n",
        "            del attn\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y, (k, v)\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config, i_layer):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = ReflexRouterAttention(config, i_layer)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x, hidden_states):\n",
        "        attn_kv = self.attn(self.ln_1(x), hidden_states)\n",
        "        x = x + attn_kv[0] #y for 0\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x, attn_kv[1] #hidden_states_i for k,v for 1\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config, _) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        hidden_states = []\n",
        "\n",
        "        for i, block in enumerate(self.transformer.h):\n",
        "            x, hidden_state = block(x, hidden_states)\n",
        "            if self.config.model_type == 'reflex':\n",
        "                hidden_states.append(hidden_state)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
        "        # first estimate the number of flops we do per iteration.\n",
        "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
        "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeSXiW1-4ROO"
      },
      "outputs": [],
      "source": [
        "data_dir = '/content'\n",
        "def get_batch(split):\n",
        "    batch_size = config.batch_size\n",
        "    block_size = config.block_size\n",
        "    # We recreate np.memmap every batch to avoid a memory leak, as per\n",
        "    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "iter_num = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "ec7c5648-05c1-4b32-a038-12014f98bfcc",
        "id": "g4CrsNPY4ROM"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.GPT"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>GPT</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>Base class for all neural network modules.\n",
              "\n",
              "Your models should also subclass this class.\n",
              "\n",
              "Modules can also contain other Modules, allowing to nest them in\n",
              "a tree structure. You can assign the submodules as regular attributes::\n",
              "\n",
              "    import torch.nn as nn\n",
              "    import torch.nn.functional as F\n",
              "\n",
              "    class Model(nn.Module):\n",
              "        def __init__(self) -&gt; None:\n",
              "            super().__init__()\n",
              "            self.conv1 = nn.Conv2d(1, 20, 5)\n",
              "            self.conv2 = nn.Conv2d(20, 20, 5)\n",
              "\n",
              "        def forward(self, x):\n",
              "            x = F.relu(self.conv1(x))\n",
              "            return F.relu(self.conv2(x))\n",
              "\n",
              "Submodules assigned in this way will be registered, and will have their\n",
              "parameters converted too when you call :meth:`to`, etc.\n",
              "\n",
              ".. note::\n",
              "    As per the example above, an ``__init__()`` call to the parent class\n",
              "    must be made before assignment on the child.\n",
              "\n",
              ":ivar training: Boolean represents whether this module is in training or\n",
              "                evaluation mode.\n",
              ":vartype training: bool</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA-Ds5_F4RON"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    batch_size: int = 32\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 4\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = False\n",
        "    model_type: str = 'reflex' # or 'simple'\n",
        "\n",
        "config = GPTConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e633b6cb-9954-4a2d-a50a-0257ae2b15f5",
        "id": "tUnOVdM-4RON"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTConfig(batch_size=16, block_size=1024, vocab_size=50304, n_layer=6, n_head=4, n_embd=768, dropout=0.0, bias=False, model_type='reflex')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOw3JU-d4RON"
      },
      "outputs": [],
      "source": [
        "wandb_log = True\n",
        "wandb_project = 'tbank-research'\n",
        "wandb_run_name = f'reflexAttnGPT_with_router={config}1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZE0HOUs4RON"
      },
      "outputs": [],
      "source": [
        "out_dir = '/content/reglex_attn_GPT'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True"
      ],
      "metadata": {
        "id": "YmtgHuGQE6H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "max_iters = 10000\n",
        "lr_decay_iters = 5000\n",
        "eval_interval = 300\n",
        "eval_iters = 200\n",
        "log_interval = 10\n",
        "gradient_accumulation_steps = 1"
      ],
      "metadata": {
        "id": "iO8TYqV4E0sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7307f121-3800-4442-fa9e-05999c4a776f",
        "id": "H0QQter_4ROO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens per iteration will be: 16,384\n"
          ]
        }
      ],
      "source": [
        "#scheduler lr, warmup\n",
        "lr_decay_iters = 5000\n",
        "min_lr = 1e-5\n",
        "warmup_iters = 400\n",
        "intercept = 'True' if config.bias else 'False'\n",
        "eval_only = False\n",
        "always_save_checkpoint = False\n",
        "\n",
        "weight_decay = 1e-1\n",
        "decay_lr = True\n",
        "beta1 = 0.9\n",
        "beta2 = 0.98\n",
        "grad_clip = 1.0\n",
        "# DDP settings\n",
        "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
        "# system\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
        "\n",
        "# various inits, derived attributes, I/O setup\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
        "if ddp:\n",
        "    init_process_group(backend=backend)\n",
        "    ddp_rank = int(os.environ['RANK'])\n",
        "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "    device = f'cuda:{ddp_local_rank}'\n",
        "    torch.cuda.set_device(device)\n",
        "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
        "    seed_offset = ddp_rank # each process gets a different seed\n",
        "    # world_size number of processes will be training simultaneously, so we can scale\n",
        "    # down the desired gradient accumulation iterations per process proportionally\n",
        "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
        "    gradient_accumulation_steps //= ddp_world_size\n",
        "else:\n",
        "    # if not ddp, we are running on a single gpu, and one process\n",
        "    master_process = True\n",
        "    seed_offset = 0\n",
        "    ddp_world_size = 1\n",
        "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * config.batch_size * config.block_size\n",
        "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
        "\n",
        "if master_process:\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "torch.manual_seed(1337 + seed_offset)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "# note: float16 data type will automatically use a GradScaler\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c3680d-0f41-447a-b48f-9f9a203024a8",
        "id": "1yptRstL4ROO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model from scratch\n",
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "# model init\n",
        "model_args = dict(n_layer=config.n_layer, n_head=config.n_head, n_embd=config.n_embd, block_size=config.block_size,\n",
        "                  bias=config.bias, vocab_size=config.vocab_size, dropout=config.dropout)\n",
        "\n",
        "# init a new model from scratch\n",
        "print(\"Initializing a new model from scratch\")\n",
        "model = GPT(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6491b521-c5cd-4fba-8b8b-81c59d2f9fe7",
        "id": "lFsMsSLS4ROO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 26, with 81,887,232 parameters\n",
            "num non-decayed parameter tensors: 19, with 10,068 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-0c2aead2cd98>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n",
        "checkpoint = None # free up memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb135f0-7a13-4303-a2e3-283d3db762dd",
        "id": "RzM-aU6X4ROO"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compiling the model... (takes a ~minute)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:tpgoo5in) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▆███▇▇▆▅▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mfu</td><td>▁█████████████████████████████████</td></tr><tr><td>train/loss</td><td>█▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/loss</td><td>█▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iter</td><td>9900</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>mfu</td><td>22.87551</td></tr><tr><td>train/loss</td><td>4.30068</td></tr><tr><td>val/loss</td><td>4.28447</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">reflexAttnGPT_with_router=GPTConfig(batch_size=16, block_size=1024, vocab_size=50304, n_layer=4, n_head=2, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</strong> at: <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/tpgoo5in' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/tpgoo5in</a><br/> View project at: <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241126_192837-tpgoo5in/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:tpgoo5in). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241126_195526-wup9rhu4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/206spv-central-university/tbank-research/runs/wup9rhu4' target=\"_blank\">reflexAttnGPT_with_router=GPTConfig(batch_size=16, block_size=1024, vocab_size=50304, n_layer=6, n_head=4, n_embd=768, dropout=0.0, bias=False, model_type='reflex')1</a></strong> to <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/206spv-central-university/tbank-research' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/206spv-central-university/tbank-research/runs/wup9rhu4' target=\"_blank\">https://wandb.ai/206spv-central-university/tbank-research/runs/wup9rhu4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-21-5d3c90e71337> line 235 \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:46.835000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-21-5d3c90e71337> line 182 \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.233000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-21-5d3c90e71337> line 17 \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.314000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-21-5d3c90e71337> line 128 \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1142, in compile_subgraph\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.519000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-21-5d3c90e71337> line 166 \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.695000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] WON'T CONVERT forward <ipython-input-21-5d3c90e71337> line 79 \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] due to: \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1446, in _call_user_compiler\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_dynamo.py\", line 129, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 2234, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1521, in compile_fx\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return aot_autograd(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/backends/common.py\", line 72, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1071, in aot_module_simplified\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 1056, in dispatch_and_compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1350, in fw_compiler_base\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 1421, in _fw_compiler_base\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return inner_compile(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 475, in compile_fx_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/repro/after_aot.py\", line 85, in debug_wrapper\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 661, in _compile_fx_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/codecache.py\", line 1334, in load\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 570, in codegen_and_compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/compile_fx.py\", line 878, in fx_codegen_and_compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1913, in compile_to_fn\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return self.compile_to_module().call\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1839, in compile_to_module\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return self._compile_to_module()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1845, in _compile_to_module\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/graph.py\", line 1780, in codegen\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.scheduler = Scheduler(self.operations)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1731, in __init__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self._init(nodes)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in _init\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1749, in <listcomp>\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.nodes = [self.create_scheduler_node(n) for n in nodes]\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 1856, in create_scheduler_node\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return SchedulerNode(self, node)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 833, in __init__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self._compute_attrs()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 846, in _compute_attrs\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     group_fn = self.scheduler.get_backend(self.node.get_device()).group_fn\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3360, in get_backend\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.backends[device] = self.create_backend(device)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_inductor/scheduler.py\", line 3352, in create_backend\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     raise RuntimeError(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] Traceback (most recent call last):\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 1064, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     result = self._inner_convert(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 526, in __call__\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 924, in _compile\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 666, in compile_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_utils_internal.py\", line 87, in wrapper_function\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return function(*args, **kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 699, in _compile_inner\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/bytecode_transformation.py\", line 1322, in transform_code_object\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     transformations(instructions, code_options)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 219, in _fn\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return fn(*args, **kwargs)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\", line 634, in transform\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     tracer.run()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2796, in run\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     super().run()\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 983, in run\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     while self.step():\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 895, in step\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self._return(inst)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/symbolic_convert.py\", line 2972, in _return\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.output.compile_subgraph(\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1117, in compile_subgraph\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1416, in call_user_compiler\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 1465, in _call_user_compiler\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] RuntimeError: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at https://github.com/openai/triton\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "W1126 19:55:47.885000 1991 torch/_dynamo/convert_frame.py:1125] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 10.9546, val loss 10.9550\n",
            "iter 0: loss 10.9605, time 48772.56ms, mfu -100.00%\n",
            "iter 10: loss 9.8901, time 219.81ms, mfu 12.98%\n",
            "iter 20: loss 9.1508, time 220.38ms, mfu 12.98%\n",
            "iter 30: loss 8.6824, time 220.66ms, mfu 12.97%\n",
            "iter 40: loss 8.0601, time 219.86ms, mfu 12.97%\n",
            "iter 50: loss 7.4125, time 219.82ms, mfu 12.97%\n",
            "iter 60: loss 7.1845, time 220.36ms, mfu 12.97%\n",
            "iter 70: loss 7.1745, time 221.20ms, mfu 12.96%\n",
            "iter 80: loss 6.9874, time 220.83ms, mfu 12.96%\n",
            "iter 90: loss 6.8488, time 219.70ms, mfu 12.96%\n",
            "iter 100: loss 7.1206, time 220.33ms, mfu 12.96%\n",
            "iter 110: loss 6.7756, time 220.61ms, mfu 12.96%\n",
            "iter 120: loss 6.7248, time 219.52ms, mfu 12.96%\n",
            "iter 130: loss 6.7034, time 220.12ms, mfu 12.96%\n",
            "iter 140: loss 6.5160, time 220.62ms, mfu 12.96%\n",
            "iter 150: loss 6.4889, time 220.25ms, mfu 12.96%\n",
            "iter 160: loss 6.4814, time 220.46ms, mfu 12.96%\n",
            "iter 170: loss 6.5565, time 220.68ms, mfu 12.95%\n",
            "iter 180: loss 6.2481, time 220.89ms, mfu 12.95%\n",
            "iter 190: loss 6.4183, time 220.42ms, mfu 12.95%\n",
            "iter 200: loss 6.2377, time 219.82ms, mfu 12.95%\n",
            "iter 210: loss 6.3780, time 220.20ms, mfu 12.95%\n",
            "iter 220: loss 6.3172, time 220.47ms, mfu 12.95%\n",
            "iter 230: loss 6.2472, time 220.25ms, mfu 12.95%\n",
            "iter 240: loss 6.4006, time 220.96ms, mfu 12.95%\n",
            "iter 250: loss 6.4956, time 221.09ms, mfu 12.94%\n",
            "iter 260: loss 6.1478, time 220.81ms, mfu 12.94%\n",
            "iter 270: loss 6.2538, time 220.52ms, mfu 12.94%\n",
            "iter 280: loss 6.4081, time 220.96ms, mfu 12.94%\n",
            "iter 290: loss 6.3048, time 220.13ms, mfu 12.94%\n",
            "step 300: train loss 6.2644, val loss 6.1964\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 300: loss 6.3102, time 36019.47ms, mfu 11.65%\n",
            "iter 310: loss 6.1959, time 220.44ms, mfu 11.78%\n",
            "iter 320: loss 6.2054, time 220.78ms, mfu 11.90%\n",
            "iter 330: loss 6.1945, time 220.64ms, mfu 12.00%\n",
            "iter 340: loss 6.2367, time 220.36ms, mfu 12.09%\n",
            "iter 350: loss 6.2849, time 220.50ms, mfu 12.18%\n",
            "iter 360: loss 5.9532, time 220.30ms, mfu 12.26%\n",
            "iter 370: loss 5.9251, time 220.68ms, mfu 12.32%\n",
            "iter 380: loss 6.2133, time 220.48ms, mfu 12.38%\n",
            "iter 390: loss 6.3958, time 220.94ms, mfu 12.44%\n",
            "iter 400: loss 6.1310, time 220.86ms, mfu 12.49%\n",
            "iter 410: loss 6.2280, time 220.19ms, mfu 12.53%\n",
            "iter 420: loss 6.0724, time 219.93ms, mfu 12.58%\n",
            "iter 430: loss 5.9762, time 220.52ms, mfu 12.61%\n",
            "iter 440: loss 6.1224, time 221.07ms, mfu 12.64%\n",
            "iter 450: loss 6.0310, time 219.80ms, mfu 12.68%\n",
            "iter 460: loss 6.4848, time 220.53ms, mfu 12.70%\n",
            "iter 470: loss 5.9395, time 220.73ms, mfu 12.72%\n",
            "iter 480: loss 6.0108, time 220.47ms, mfu 12.75%\n",
            "iter 490: loss 6.0104, time 220.68ms, mfu 12.76%\n",
            "iter 500: loss 5.9152, time 220.46ms, mfu 12.78%\n",
            "iter 510: loss 6.0253, time 220.76ms, mfu 12.80%\n",
            "iter 520: loss 6.1156, time 220.86ms, mfu 12.81%\n",
            "iter 530: loss 5.8175, time 220.41ms, mfu 12.82%\n",
            "iter 540: loss 6.1207, time 220.86ms, mfu 12.83%\n",
            "iter 550: loss 6.0832, time 220.56ms, mfu 12.84%\n",
            "iter 560: loss 6.0897, time 220.96ms, mfu 12.85%\n",
            "iter 570: loss 5.8084, time 220.09ms, mfu 12.86%\n",
            "iter 580: loss 6.0299, time 220.34ms, mfu 12.87%\n",
            "iter 590: loss 5.9376, time 220.44ms, mfu 12.88%\n",
            "step 600: train loss 5.9363, val loss 5.8864\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 600: loss 5.7261, time 36090.02ms, mfu 11.60%\n",
            "iter 610: loss 6.0484, time 220.48ms, mfu 11.73%\n",
            "iter 620: loss 5.9186, time 220.57ms, mfu 11.85%\n",
            "iter 630: loss 5.8373, time 220.75ms, mfu 11.96%\n",
            "iter 640: loss 5.9823, time 220.87ms, mfu 12.05%\n",
            "iter 650: loss 5.8585, time 220.60ms, mfu 12.14%\n",
            "iter 660: loss 5.9351, time 220.49ms, mfu 12.22%\n",
            "iter 670: loss 5.9866, time 220.82ms, mfu 12.29%\n",
            "iter 680: loss 5.7276, time 220.55ms, mfu 12.36%\n",
            "iter 690: loss 5.7128, time 220.72ms, mfu 12.41%\n",
            "iter 700: loss 5.7479, time 220.34ms, mfu 12.47%\n",
            "iter 710: loss 5.6327, time 220.54ms, mfu 12.51%\n",
            "iter 720: loss 5.8171, time 220.69ms, mfu 12.55%\n",
            "iter 730: loss 5.6137, time 220.48ms, mfu 12.59%\n",
            "iter 740: loss 5.5725, time 220.89ms, mfu 12.63%\n",
            "iter 750: loss 5.7900, time 220.64ms, mfu 12.66%\n",
            "iter 760: loss 6.0619, time 220.71ms, mfu 12.68%\n",
            "iter 770: loss 5.6589, time 220.91ms, mfu 12.71%\n",
            "iter 780: loss 5.5602, time 220.74ms, mfu 12.73%\n",
            "iter 790: loss 5.4641, time 221.46ms, mfu 12.74%\n",
            "iter 800: loss 5.7219, time 220.82ms, mfu 12.76%\n",
            "iter 810: loss 5.7943, time 220.66ms, mfu 12.78%\n",
            "iter 820: loss 5.5824, time 220.61ms, mfu 12.79%\n",
            "iter 830: loss 5.5197, time 220.77ms, mfu 12.81%\n",
            "iter 840: loss 5.4804, time 221.07ms, mfu 12.82%\n",
            "iter 850: loss 5.5307, time 220.46ms, mfu 12.83%\n",
            "iter 860: loss 5.6981, time 220.54ms, mfu 12.84%\n",
            "iter 870: loss 5.6364, time 220.91ms, mfu 12.85%\n",
            "iter 880: loss 5.5631, time 220.46ms, mfu 12.86%\n",
            "iter 890: loss 5.6030, time 220.38ms, mfu 12.87%\n",
            "step 900: train loss 5.5783, val loss 5.5115\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 900: loss 5.4541, time 36049.53ms, mfu 11.59%\n",
            "iter 910: loss 5.7812, time 219.78ms, mfu 11.73%\n",
            "iter 920: loss 5.2985, time 221.17ms, mfu 11.84%\n",
            "iter 930: loss 5.5063, time 220.27ms, mfu 11.95%\n",
            "iter 940: loss 5.5030, time 220.89ms, mfu 12.05%\n",
            "iter 950: loss 5.5616, time 220.67ms, mfu 12.14%\n",
            "iter 960: loss 5.5522, time 220.72ms, mfu 12.22%\n",
            "iter 970: loss 5.5057, time 220.47ms, mfu 12.29%\n",
            "iter 980: loss 5.5662, time 220.57ms, mfu 12.35%\n",
            "iter 990: loss 5.5474, time 220.67ms, mfu 12.41%\n",
            "iter 1000: loss 5.4531, time 220.73ms, mfu 12.46%\n",
            "iter 1010: loss 5.4816, time 220.58ms, mfu 12.51%\n",
            "iter 1020: loss 5.3235, time 220.51ms, mfu 12.55%\n",
            "iter 1030: loss 5.4703, time 220.65ms, mfu 12.59%\n",
            "iter 1040: loss 5.2858, time 220.87ms, mfu 12.62%\n",
            "iter 1050: loss 5.5852, time 220.60ms, mfu 12.65%\n",
            "iter 1060: loss 5.5307, time 220.21ms, mfu 12.68%\n",
            "iter 1070: loss 5.5295, time 220.64ms, mfu 12.71%\n",
            "iter 1080: loss 5.4696, time 220.99ms, mfu 12.73%\n",
            "iter 1090: loss 5.3557, time 220.54ms, mfu 12.75%\n",
            "iter 1100: loss 5.4897, time 220.79ms, mfu 12.77%\n",
            "iter 1110: loss 5.4456, time 220.98ms, mfu 12.78%\n",
            "iter 1120: loss 5.3164, time 220.65ms, mfu 12.80%\n",
            "iter 1130: loss 5.4606, time 220.47ms, mfu 12.81%\n",
            "iter 1140: loss 5.4275, time 220.49ms, mfu 12.82%\n",
            "iter 1150: loss 5.3038, time 220.91ms, mfu 12.83%\n",
            "iter 1160: loss 5.5293, time 220.83ms, mfu 12.84%\n",
            "iter 1170: loss 5.1733, time 220.22ms, mfu 12.85%\n",
            "iter 1180: loss 5.3780, time 220.26ms, mfu 12.86%\n",
            "iter 1190: loss 5.3201, time 220.91ms, mfu 12.87%\n",
            "step 1200: train loss 5.3357, val loss 5.2841\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1200: loss 5.1247, time 36105.81ms, mfu 11.59%\n",
            "iter 1210: loss 5.2091, time 220.96ms, mfu 11.72%\n",
            "iter 1220: loss 5.3913, time 220.78ms, mfu 11.84%\n",
            "iter 1230: loss 5.1464, time 220.80ms, mfu 11.95%\n",
            "iter 1240: loss 5.2625, time 220.10ms, mfu 12.05%\n",
            "iter 1250: loss 5.4006, time 220.87ms, mfu 12.14%\n",
            "iter 1260: loss 5.4329, time 220.88ms, mfu 12.22%\n",
            "iter 1270: loss 5.4691, time 219.87ms, mfu 12.29%\n",
            "iter 1280: loss 5.2576, time 220.06ms, mfu 12.36%\n",
            "iter 1290: loss 5.1900, time 220.85ms, mfu 12.41%\n",
            "iter 1300: loss 5.3876, time 220.29ms, mfu 12.47%\n",
            "iter 1310: loss 5.4275, time 220.86ms, mfu 12.51%\n",
            "iter 1320: loss 5.2668, time 220.95ms, mfu 12.55%\n",
            "iter 1330: loss 5.0574, time 220.46ms, mfu 12.59%\n",
            "iter 1340: loss 5.1911, time 220.49ms, mfu 12.63%\n",
            "iter 1350: loss 5.2079, time 220.26ms, mfu 12.66%\n",
            "iter 1360: loss 5.1053, time 220.71ms, mfu 12.69%\n",
            "iter 1370: loss 5.2387, time 220.93ms, mfu 12.71%\n",
            "iter 1380: loss 5.3546, time 220.79ms, mfu 12.73%\n",
            "iter 1390: loss 5.1567, time 220.47ms, mfu 12.75%\n",
            "iter 1400: loss 5.1028, time 220.98ms, mfu 12.77%\n",
            "iter 1410: loss 5.1637, time 220.71ms, mfu 12.78%\n",
            "iter 1420: loss 5.4025, time 220.68ms, mfu 12.80%\n",
            "iter 1430: loss 5.3453, time 220.07ms, mfu 12.81%\n",
            "iter 1440: loss 5.3056, time 220.85ms, mfu 12.82%\n",
            "iter 1450: loss 5.3582, time 220.54ms, mfu 12.84%\n",
            "iter 1460: loss 5.1911, time 220.66ms, mfu 12.84%\n",
            "iter 1470: loss 5.5658, time 220.39ms, mfu 12.85%\n",
            "iter 1480: loss 5.2624, time 220.40ms, mfu 12.86%\n",
            "iter 1490: loss 5.2017, time 220.43ms, mfu 12.87%\n",
            "step 1500: train loss 5.1932, val loss 5.1371\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1500: loss 5.4375, time 36253.42ms, mfu 11.59%\n",
            "iter 1510: loss 5.0585, time 220.80ms, mfu 11.73%\n",
            "iter 1520: loss 5.0263, time 220.67ms, mfu 11.85%\n",
            "iter 1530: loss 5.1290, time 219.98ms, mfu 11.96%\n",
            "iter 1540: loss 5.2331, time 220.31ms, mfu 12.06%\n",
            "iter 1550: loss 5.1427, time 220.04ms, mfu 12.15%\n",
            "iter 1560: loss 5.1972, time 221.21ms, mfu 12.22%\n",
            "iter 1570: loss 5.2325, time 220.24ms, mfu 12.30%\n",
            "iter 1580: loss 5.1641, time 220.58ms, mfu 12.36%\n",
            "iter 1590: loss 5.2264, time 220.46ms, mfu 12.42%\n",
            "iter 1600: loss 4.9982, time 221.17ms, mfu 12.47%\n",
            "iter 1610: loss 5.0133, time 220.59ms, mfu 12.51%\n",
            "iter 1620: loss 5.1542, time 221.17ms, mfu 12.55%\n",
            "iter 1630: loss 5.1807, time 221.04ms, mfu 12.59%\n",
            "iter 1640: loss 5.0587, time 220.44ms, mfu 12.62%\n",
            "iter 1650: loss 5.3031, time 219.66ms, mfu 12.66%\n",
            "iter 1660: loss 5.1101, time 220.80ms, mfu 12.69%\n",
            "iter 1670: loss 4.7354, time 221.69ms, mfu 12.70%\n",
            "iter 1680: loss 5.0862, time 220.41ms, mfu 12.73%\n",
            "iter 1690: loss 5.1484, time 220.30ms, mfu 12.75%\n",
            "iter 1700: loss 5.2001, time 220.63ms, mfu 12.77%\n",
            "iter 1710: loss 5.1516, time 220.90ms, mfu 12.78%\n",
            "iter 1720: loss 5.0237, time 219.03ms, mfu 12.81%\n",
            "iter 1730: loss 5.3581, time 220.75ms, mfu 12.82%\n",
            "iter 1740: loss 5.1430, time 220.34ms, mfu 12.83%\n",
            "iter 1750: loss 4.9557, time 221.22ms, mfu 12.84%\n",
            "iter 1760: loss 5.1311, time 220.66ms, mfu 12.85%\n",
            "iter 1770: loss 5.0448, time 220.41ms, mfu 12.86%\n",
            "iter 1780: loss 4.9471, time 220.65ms, mfu 12.86%\n",
            "iter 1790: loss 5.0983, time 222.10ms, mfu 12.86%\n",
            "step 1800: train loss 5.0492, val loss 5.0142\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 1800: loss 5.0281, time 36149.58ms, mfu 11.58%\n",
            "iter 1810: loss 5.1241, time 220.44ms, mfu 11.72%\n",
            "iter 1820: loss 5.2898, time 220.54ms, mfu 11.84%\n",
            "iter 1830: loss 4.9692, time 220.57ms, mfu 11.95%\n",
            "iter 1840: loss 5.0190, time 221.04ms, mfu 12.05%\n",
            "iter 1850: loss 5.2506, time 220.82ms, mfu 12.13%\n",
            "iter 1860: loss 4.9327, time 220.53ms, mfu 12.21%\n",
            "iter 1870: loss 5.1782, time 220.23ms, mfu 12.29%\n",
            "iter 1880: loss 5.0412, time 220.52ms, mfu 12.35%\n",
            "iter 1890: loss 4.8871, time 220.28ms, mfu 12.41%\n",
            "iter 1900: loss 5.3652, time 221.02ms, mfu 12.46%\n",
            "iter 1910: loss 4.9581, time 221.24ms, mfu 12.51%\n",
            "iter 1920: loss 5.1030, time 220.88ms, mfu 12.55%\n",
            "iter 1930: loss 5.0461, time 220.77ms, mfu 12.58%\n",
            "iter 1940: loss 4.9619, time 220.97ms, mfu 12.62%\n",
            "iter 1950: loss 4.8928, time 220.48ms, mfu 12.65%\n",
            "iter 1960: loss 4.8145, time 221.34ms, mfu 12.67%\n",
            "iter 1970: loss 5.2194, time 220.37ms, mfu 12.70%\n",
            "iter 1980: loss 4.6769, time 220.70ms, mfu 12.72%\n",
            "iter 1990: loss 4.9220, time 221.36ms, mfu 12.74%\n",
            "iter 2000: loss 4.9235, time 221.23ms, mfu 12.76%\n",
            "iter 2010: loss 4.9388, time 220.75ms, mfu 12.77%\n",
            "iter 2020: loss 4.9065, time 221.22ms, mfu 12.78%\n",
            "iter 2030: loss 5.0632, time 220.21ms, mfu 12.80%\n",
            "iter 2040: loss 4.9561, time 220.69ms, mfu 12.81%\n",
            "iter 2050: loss 4.8046, time 220.35ms, mfu 12.83%\n",
            "iter 2060: loss 5.0007, time 220.72ms, mfu 12.84%\n",
            "iter 2070: loss 5.1199, time 221.22ms, mfu 12.84%\n",
            "iter 2080: loss 5.0041, time 220.23ms, mfu 12.85%\n",
            "iter 2090: loss 5.1114, time 221.05ms, mfu 12.86%\n",
            "step 2100: train loss 4.9411, val loss 4.8984\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2100: loss 4.9328, time 35864.87ms, mfu 11.58%\n",
            "iter 2110: loss 4.8987, time 220.43ms, mfu 11.72%\n",
            "iter 2120: loss 4.8465, time 220.34ms, mfu 11.84%\n",
            "iter 2130: loss 5.2101, time 220.20ms, mfu 11.95%\n",
            "iter 2140: loss 4.8938, time 221.77ms, mfu 12.04%\n",
            "iter 2150: loss 4.8741, time 219.34ms, mfu 12.14%\n",
            "iter 2160: loss 4.8420, time 220.34ms, mfu 12.22%\n",
            "iter 2170: loss 4.7184, time 220.81ms, mfu 12.29%\n",
            "iter 2180: loss 4.8237, time 220.94ms, mfu 12.35%\n",
            "iter 2190: loss 4.8076, time 221.60ms, mfu 12.41%\n",
            "iter 2200: loss 4.9611, time 220.71ms, mfu 12.46%\n",
            "iter 2210: loss 5.0590, time 220.94ms, mfu 12.50%\n",
            "iter 2220: loss 4.8759, time 220.95ms, mfu 12.54%\n",
            "iter 2230: loss 4.7143, time 220.77ms, mfu 12.58%\n",
            "iter 2240: loss 4.8469, time 220.72ms, mfu 12.62%\n",
            "iter 2250: loss 4.6812, time 219.49ms, mfu 12.65%\n",
            "iter 2260: loss 4.8231, time 220.18ms, mfu 12.68%\n",
            "iter 2270: loss 4.8819, time 220.25ms, mfu 12.71%\n",
            "iter 2280: loss 4.8227, time 220.74ms, mfu 12.73%\n",
            "iter 2290: loss 4.8742, time 221.49ms, mfu 12.75%\n",
            "iter 2300: loss 5.0355, time 220.26ms, mfu 12.77%\n",
            "iter 2310: loss 4.7511, time 221.35ms, mfu 12.78%\n",
            "iter 2320: loss 4.9391, time 220.46ms, mfu 12.80%\n",
            "iter 2330: loss 4.7579, time 221.05ms, mfu 12.81%\n",
            "iter 2340: loss 4.9650, time 220.70ms, mfu 12.82%\n",
            "iter 2350: loss 4.8618, time 222.09ms, mfu 12.82%\n",
            "iter 2360: loss 4.7494, time 221.80ms, mfu 12.83%\n",
            "iter 2370: loss 4.7664, time 219.15ms, mfu 12.85%\n",
            "iter 2380: loss 5.0401, time 220.51ms, mfu 12.85%\n",
            "iter 2390: loss 4.8778, time 220.95ms, mfu 12.86%\n",
            "step 2400: train loss 4.7910, val loss 4.7600\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2400: loss 4.8095, time 36160.89ms, mfu 11.58%\n",
            "iter 2410: loss 4.7150, time 220.99ms, mfu 11.71%\n",
            "iter 2420: loss 4.8984, time 220.36ms, mfu 11.84%\n",
            "iter 2430: loss 4.7952, time 220.33ms, mfu 11.95%\n",
            "iter 2440: loss 4.6056, time 222.23ms, mfu 12.04%\n",
            "iter 2450: loss 4.8056, time 219.09ms, mfu 12.14%\n",
            "iter 2460: loss 4.7489, time 220.68ms, mfu 12.22%\n",
            "iter 2470: loss 4.8369, time 220.88ms, mfu 12.29%\n",
            "iter 2480: loss 4.6730, time 220.10ms, mfu 12.35%\n",
            "iter 2490: loss 4.8315, time 220.32ms, mfu 12.41%\n",
            "iter 2500: loss 4.6781, time 220.28ms, mfu 12.47%\n",
            "iter 2510: loss 4.8112, time 220.23ms, mfu 12.52%\n",
            "iter 2520: loss 4.5750, time 220.58ms, mfu 12.56%\n",
            "iter 2530: loss 4.6182, time 220.82ms, mfu 12.59%\n",
            "iter 2540: loss 4.6091, time 220.64ms, mfu 12.63%\n",
            "iter 2550: loss 4.7771, time 220.77ms, mfu 12.66%\n",
            "iter 2560: loss 4.6381, time 220.65ms, mfu 12.68%\n",
            "iter 2570: loss 4.5386, time 220.74ms, mfu 12.71%\n",
            "iter 2580: loss 4.5498, time 220.68ms, mfu 12.73%\n",
            "iter 2590: loss 4.8092, time 220.74ms, mfu 12.75%\n",
            "iter 2600: loss 4.6552, time 220.97ms, mfu 12.77%\n",
            "iter 2610: loss 4.6997, time 220.92ms, mfu 12.78%\n",
            "iter 2620: loss 4.6644, time 248.95ms, mfu 12.65%\n",
            "iter 2630: loss 4.6291, time 220.50ms, mfu 12.68%\n",
            "iter 2640: loss 4.6039, time 220.80ms, mfu 12.70%\n",
            "iter 2650: loss 4.6912, time 221.08ms, mfu 12.72%\n",
            "iter 2660: loss 4.6743, time 220.34ms, mfu 12.74%\n",
            "iter 2670: loss 4.6714, time 220.02ms, mfu 12.77%\n",
            "iter 2680: loss 4.6064, time 220.72ms, mfu 12.78%\n",
            "iter 2690: loss 4.6336, time 220.66ms, mfu 12.80%\n",
            "step 2700: train loss 4.6870, val loss 4.6616\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 2700: loss 4.6557, time 36121.81ms, mfu 11.53%\n",
            "iter 2710: loss 4.7395, time 229.43ms, mfu 11.62%\n",
            "iter 2720: loss 4.7316, time 220.47ms, mfu 11.75%\n",
            "iter 2730: loss 4.5535, time 220.19ms, mfu 11.87%\n",
            "iter 2740: loss 4.6412, time 220.49ms, mfu 11.98%\n",
            "iter 2750: loss 4.8686, time 220.19ms, mfu 12.07%\n",
            "iter 2760: loss 4.5981, time 219.33ms, mfu 12.17%\n",
            "iter 2770: loss 4.6185, time 220.62ms, mfu 12.24%\n",
            "iter 2780: loss 4.6138, time 220.67ms, mfu 12.31%\n",
            "iter 2790: loss 4.5035, time 220.21ms, mfu 12.38%\n",
            "iter 2800: loss 4.7776, time 220.49ms, mfu 12.43%\n",
            "iter 2810: loss 4.7763, time 220.72ms, mfu 12.48%\n",
            "iter 2820: loss 4.5398, time 221.15ms, mfu 12.52%\n",
            "iter 2830: loss 4.7312, time 220.61ms, mfu 12.57%\n",
            "iter 2840: loss 4.6611, time 220.42ms, mfu 12.60%\n",
            "iter 2850: loss 4.5601, time 220.86ms, mfu 12.63%\n",
            "iter 2860: loss 4.4835, time 220.88ms, mfu 12.66%\n",
            "iter 2870: loss 4.7040, time 220.53ms, mfu 12.69%\n",
            "iter 2880: loss 4.7182, time 220.30ms, mfu 12.72%\n",
            "iter 2890: loss 4.7968, time 220.36ms, mfu 12.74%\n",
            "iter 2900: loss 4.6454, time 220.66ms, mfu 12.76%\n",
            "iter 2910: loss 4.7135, time 220.40ms, mfu 12.78%\n",
            "iter 2920: loss 4.6830, time 220.85ms, mfu 12.79%\n",
            "iter 2930: loss 4.5349, time 220.33ms, mfu 12.81%\n",
            "iter 2940: loss 4.6504, time 220.54ms, mfu 12.82%\n",
            "iter 2950: loss 4.6317, time 220.73ms, mfu 12.83%\n",
            "iter 2960: loss 4.5720, time 220.59ms, mfu 12.84%\n",
            "iter 2970: loss 4.6101, time 220.40ms, mfu 12.85%\n",
            "iter 2980: loss 4.6173, time 220.29ms, mfu 12.86%\n",
            "iter 2990: loss 4.5650, time 220.41ms, mfu 12.87%\n",
            "step 3000: train loss 4.6005, val loss 4.5630\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3000: loss 4.5591, time 36099.48ms, mfu 11.59%\n",
            "iter 3010: loss 4.6849, time 220.65ms, mfu 11.72%\n",
            "iter 3020: loss 4.6787, time 220.58ms, mfu 11.85%\n",
            "iter 3030: loss 4.6227, time 219.93ms, mfu 11.96%\n",
            "iter 3040: loss 4.5052, time 220.53ms, mfu 12.06%\n",
            "iter 3050: loss 4.6143, time 220.58ms, mfu 12.14%\n",
            "iter 3060: loss 4.4898, time 220.48ms, mfu 12.22%\n",
            "iter 3070: loss 4.6059, time 220.75ms, mfu 12.29%\n",
            "iter 3080: loss 4.7448, time 220.61ms, mfu 12.36%\n",
            "iter 3090: loss 4.7259, time 220.39ms, mfu 12.42%\n",
            "iter 3100: loss 4.6133, time 220.71ms, mfu 12.47%\n",
            "iter 3110: loss 4.6118, time 220.42ms, mfu 12.51%\n",
            "iter 3120: loss 4.5119, time 220.96ms, mfu 12.55%\n",
            "iter 3130: loss 4.5850, time 219.97ms, mfu 12.60%\n",
            "iter 3140: loss 4.6112, time 220.72ms, mfu 12.63%\n",
            "iter 3150: loss 4.3671, time 220.04ms, mfu 12.66%\n",
            "iter 3160: loss 4.6099, time 220.67ms, mfu 12.69%\n",
            "iter 3170: loss 4.6182, time 221.02ms, mfu 12.71%\n",
            "iter 3180: loss 4.6107, time 221.09ms, mfu 12.73%\n",
            "iter 3190: loss 4.5168, time 220.21ms, mfu 12.75%\n",
            "iter 3200: loss 4.5070, time 220.80ms, mfu 12.77%\n",
            "iter 3210: loss 4.4281, time 221.35ms, mfu 12.78%\n",
            "iter 3220: loss 4.5009, time 220.93ms, mfu 12.79%\n",
            "iter 3230: loss 4.5387, time 220.40ms, mfu 12.81%\n",
            "iter 3240: loss 4.7361, time 220.59ms, mfu 12.82%\n",
            "iter 3250: loss 4.3867, time 220.81ms, mfu 12.83%\n",
            "iter 3260: loss 4.4593, time 220.62ms, mfu 12.84%\n",
            "iter 3270: loss 4.6924, time 220.59ms, mfu 12.85%\n",
            "iter 3280: loss 4.5940, time 220.79ms, mfu 12.86%\n",
            "iter 3290: loss 4.4648, time 220.42ms, mfu 12.87%\n",
            "step 3300: train loss 4.5303, val loss 4.5098\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3300: loss 4.7124, time 36110.19ms, mfu 11.59%\n",
            "iter 3310: loss 4.6869, time 220.29ms, mfu 11.72%\n",
            "iter 3320: loss 4.6382, time 220.23ms, mfu 11.85%\n",
            "iter 3330: loss 4.5703, time 220.84ms, mfu 11.95%\n",
            "iter 3340: loss 4.4477, time 220.80ms, mfu 12.05%\n",
            "iter 3350: loss 4.4240, time 220.63ms, mfu 12.14%\n",
            "iter 3360: loss 4.3327, time 220.62ms, mfu 12.22%\n",
            "iter 3370: loss 4.4954, time 220.77ms, mfu 12.29%\n",
            "iter 3380: loss 4.3436, time 220.55ms, mfu 12.35%\n",
            "iter 3390: loss 4.4576, time 220.70ms, mfu 12.41%\n",
            "iter 3400: loss 4.4875, time 220.45ms, mfu 12.46%\n",
            "iter 3410: loss 4.7088, time 220.15ms, mfu 12.51%\n",
            "iter 3420: loss 4.4062, time 220.02ms, mfu 12.56%\n",
            "iter 3430: loss 4.3961, time 221.00ms, mfu 12.59%\n",
            "iter 3440: loss 4.4849, time 220.98ms, mfu 12.63%\n",
            "iter 3450: loss 4.4005, time 221.19ms, mfu 12.65%\n",
            "iter 3460: loss 4.4481, time 220.20ms, mfu 12.68%\n",
            "iter 3470: loss 4.4074, time 221.13ms, mfu 12.70%\n",
            "iter 3480: loss 4.6500, time 221.03ms, mfu 12.73%\n",
            "iter 3490: loss 4.5824, time 220.58ms, mfu 12.75%\n",
            "iter 3500: loss 4.4161, time 220.83ms, mfu 12.76%\n",
            "iter 3510: loss 4.2451, time 220.90ms, mfu 12.78%\n",
            "iter 3520: loss 4.4631, time 221.06ms, mfu 12.79%\n",
            "iter 3530: loss 4.5264, time 220.34ms, mfu 12.81%\n",
            "iter 3540: loss 4.6001, time 220.29ms, mfu 12.82%\n",
            "iter 3550: loss 4.3240, time 220.43ms, mfu 12.83%\n",
            "iter 3560: loss 4.4572, time 221.05ms, mfu 12.84%\n",
            "iter 3570: loss 4.3865, time 220.49ms, mfu 12.85%\n",
            "iter 3580: loss 4.4149, time 220.27ms, mfu 12.86%\n",
            "iter 3590: loss 4.3846, time 220.87ms, mfu 12.87%\n",
            "step 3600: train loss 4.4570, val loss 4.4352\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3600: loss 4.6691, time 36081.17ms, mfu 11.59%\n",
            "iter 3610: loss 4.4599, time 220.84ms, mfu 11.72%\n",
            "iter 3620: loss 4.4149, time 220.83ms, mfu 11.84%\n",
            "iter 3630: loss 4.3305, time 220.84ms, mfu 11.95%\n",
            "iter 3640: loss 4.6023, time 220.26ms, mfu 12.05%\n",
            "iter 3650: loss 4.5819, time 220.54ms, mfu 12.14%\n",
            "iter 3660: loss 4.4731, time 221.11ms, mfu 12.21%\n",
            "iter 3670: loss 4.5542, time 220.67ms, mfu 12.29%\n",
            "iter 3680: loss 4.5019, time 220.37ms, mfu 12.35%\n",
            "iter 3690: loss 4.4255, time 220.71ms, mfu 12.41%\n",
            "iter 3700: loss 4.4249, time 220.70ms, mfu 12.46%\n",
            "iter 3710: loss 4.3813, time 220.51ms, mfu 12.51%\n",
            "iter 3720: loss 4.3662, time 220.54ms, mfu 12.55%\n",
            "iter 3730: loss 4.5148, time 220.39ms, mfu 12.59%\n",
            "iter 3740: loss 4.5214, time 220.16ms, mfu 12.63%\n",
            "iter 3750: loss 4.4763, time 220.55ms, mfu 12.66%\n",
            "iter 3760: loss 4.3838, time 220.70ms, mfu 12.69%\n",
            "iter 3770: loss 4.4352, time 220.61ms, mfu 12.71%\n",
            "iter 3780: loss 4.5976, time 220.49ms, mfu 12.73%\n",
            "iter 3790: loss 4.4270, time 220.66ms, mfu 12.75%\n",
            "iter 3800: loss 4.5435, time 221.13ms, mfu 12.77%\n",
            "iter 3810: loss 4.5520, time 221.35ms, mfu 12.78%\n",
            "iter 3820: loss 4.7220, time 220.62ms, mfu 12.79%\n",
            "iter 3830: loss 4.5713, time 220.63ms, mfu 12.81%\n",
            "iter 3840: loss 4.5600, time 220.74ms, mfu 12.82%\n",
            "iter 3850: loss 4.4715, time 220.70ms, mfu 12.83%\n",
            "iter 3860: loss 4.4884, time 219.25ms, mfu 12.85%\n",
            "iter 3870: loss 4.2899, time 220.67ms, mfu 12.86%\n",
            "iter 3880: loss 4.3968, time 220.56ms, mfu 12.86%\n",
            "iter 3890: loss 4.6336, time 220.26ms, mfu 12.87%\n",
            "step 3900: train loss 4.4133, val loss 4.4014\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 3900: loss 4.2349, time 36174.85ms, mfu 11.59%\n",
            "iter 3910: loss 4.5893, time 221.02ms, mfu 11.73%\n",
            "iter 3920: loss 4.6353, time 220.28ms, mfu 11.85%\n",
            "iter 3930: loss 4.2292, time 220.82ms, mfu 11.96%\n",
            "iter 3940: loss 4.4423, time 221.26ms, mfu 12.05%\n",
            "iter 3950: loss 4.2310, time 220.96ms, mfu 12.14%\n",
            "iter 3960: loss 4.3634, time 219.98ms, mfu 12.22%\n",
            "iter 3970: loss 4.4371, time 220.66ms, mfu 12.29%\n",
            "iter 3980: loss 4.4856, time 220.37ms, mfu 12.36%\n",
            "iter 3990: loss 4.4217, time 220.01ms, mfu 12.42%\n",
            "iter 4000: loss 4.5174, time 220.14ms, mfu 12.47%\n",
            "iter 4010: loss 4.2173, time 220.78ms, mfu 12.52%\n",
            "iter 4020: loss 4.5511, time 220.91ms, mfu 12.56%\n",
            "iter 4030: loss 4.3345, time 220.54ms, mfu 12.59%\n",
            "iter 4040: loss 4.2971, time 220.78ms, mfu 12.63%\n",
            "iter 4050: loss 4.5228, time 220.49ms, mfu 12.66%\n",
            "iter 4060: loss 4.2562, time 220.94ms, mfu 12.68%\n",
            "iter 4070: loss 4.2091, time 221.56ms, mfu 12.70%\n",
            "iter 4080: loss 4.1896, time 220.71ms, mfu 12.73%\n",
            "iter 4090: loss 4.4344, time 220.26ms, mfu 12.75%\n",
            "iter 4100: loss 4.4616, time 220.57ms, mfu 12.77%\n",
            "iter 4110: loss 4.5314, time 221.28ms, mfu 12.78%\n",
            "iter 4120: loss 4.2376, time 220.59ms, mfu 12.79%\n",
            "iter 4130: loss 4.3693, time 219.68ms, mfu 12.81%\n",
            "iter 4140: loss 4.5295, time 220.83ms, mfu 12.82%\n",
            "iter 4150: loss 4.4483, time 220.58ms, mfu 12.84%\n",
            "iter 4160: loss 4.3822, time 220.80ms, mfu 12.84%\n",
            "iter 4170: loss 4.4587, time 220.45ms, mfu 12.85%\n",
            "iter 4180: loss 4.4665, time 220.84ms, mfu 12.86%\n",
            "iter 4190: loss 4.3663, time 220.77ms, mfu 12.87%\n",
            "step 4200: train loss 4.3797, val loss 4.3811\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4200: loss 4.5033, time 36131.60ms, mfu 11.59%\n",
            "iter 4210: loss 4.3350, time 220.93ms, mfu 11.72%\n",
            "iter 4220: loss 4.2247, time 220.70ms, mfu 11.84%\n",
            "iter 4230: loss 4.4035, time 220.93ms, mfu 11.95%\n",
            "iter 4240: loss 4.4243, time 221.01ms, mfu 12.04%\n",
            "iter 4250: loss 4.4527, time 220.44ms, mfu 12.13%\n",
            "iter 4260: loss 4.4218, time 220.68ms, mfu 12.21%\n",
            "iter 4270: loss 4.3160, time 221.07ms, mfu 12.28%\n",
            "iter 4280: loss 4.2980, time 220.88ms, mfu 12.35%\n",
            "iter 4290: loss 4.3956, time 220.58ms, mfu 12.40%\n",
            "iter 4300: loss 4.4220, time 220.17ms, mfu 12.46%\n",
            "iter 4310: loss 4.4256, time 220.42ms, mfu 12.51%\n",
            "iter 4320: loss 4.4121, time 220.86ms, mfu 12.55%\n",
            "iter 4330: loss 4.3374, time 220.26ms, mfu 12.59%\n",
            "iter 4340: loss 4.4000, time 219.95ms, mfu 12.63%\n",
            "iter 4350: loss 4.5810, time 221.13ms, mfu 12.66%\n",
            "iter 4360: loss 4.4105, time 220.20ms, mfu 12.69%\n",
            "iter 4370: loss 4.3486, time 220.35ms, mfu 12.71%\n",
            "iter 4380: loss 4.4215, time 220.83ms, mfu 12.73%\n",
            "iter 4390: loss 4.2913, time 221.06ms, mfu 12.75%\n",
            "iter 4400: loss 4.4239, time 221.27ms, mfu 12.76%\n",
            "iter 4410: loss 4.2499, time 220.53ms, mfu 12.78%\n",
            "iter 4420: loss 4.4728, time 220.77ms, mfu 12.80%\n",
            "iter 4430: loss 4.5157, time 221.23ms, mfu 12.81%\n",
            "iter 4440: loss 4.3341, time 220.76ms, mfu 12.82%\n",
            "iter 4450: loss 4.4388, time 220.27ms, mfu 12.83%\n",
            "iter 4460: loss 4.4031, time 220.56ms, mfu 12.84%\n",
            "iter 4470: loss 4.2450, time 221.09ms, mfu 12.85%\n",
            "iter 4480: loss 4.1217, time 220.68ms, mfu 12.86%\n",
            "iter 4490: loss 4.3613, time 220.15ms, mfu 12.87%\n",
            "step 4500: train loss 4.3536, val loss 4.3492\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4500: loss 4.5025, time 35785.52ms, mfu 11.59%\n",
            "iter 4510: loss 4.2569, time 220.57ms, mfu 11.72%\n",
            "iter 4520: loss 4.1975, time 220.44ms, mfu 11.84%\n",
            "iter 4530: loss 4.3765, time 220.88ms, mfu 11.95%\n",
            "iter 4540: loss 4.5406, time 220.29ms, mfu 12.05%\n",
            "iter 4550: loss 4.2865, time 220.73ms, mfu 12.14%\n",
            "iter 4560: loss 4.2530, time 220.09ms, mfu 12.22%\n",
            "iter 4570: loss 4.3435, time 220.46ms, mfu 12.29%\n",
            "iter 4580: loss 4.4684, time 220.81ms, mfu 12.36%\n",
            "iter 4590: loss 4.2011, time 221.12ms, mfu 12.41%\n",
            "iter 4600: loss 4.2492, time 220.38ms, mfu 12.46%\n",
            "iter 4610: loss 4.3627, time 220.11ms, mfu 12.51%\n",
            "iter 4620: loss 4.5261, time 220.79ms, mfu 12.55%\n",
            "iter 4630: loss 4.3748, time 220.82ms, mfu 12.59%\n",
            "iter 4640: loss 4.3598, time 220.54ms, mfu 12.63%\n",
            "iter 4650: loss 4.1330, time 220.65ms, mfu 12.66%\n",
            "iter 4660: loss 4.4634, time 221.15ms, mfu 12.68%\n",
            "iter 4670: loss 4.4249, time 220.26ms, mfu 12.71%\n",
            "iter 4680: loss 4.3517, time 220.85ms, mfu 12.73%\n",
            "iter 4690: loss 4.4352, time 220.26ms, mfu 12.75%\n",
            "iter 4700: loss 4.4903, time 220.42ms, mfu 12.77%\n",
            "iter 4710: loss 4.3584, time 220.32ms, mfu 12.79%\n",
            "iter 4720: loss 4.4346, time 220.54ms, mfu 12.80%\n",
            "iter 4730: loss 4.5162, time 221.13ms, mfu 12.81%\n",
            "iter 4740: loss 4.5626, time 220.95ms, mfu 12.82%\n",
            "iter 4750: loss 4.4170, time 220.73ms, mfu 12.83%\n",
            "iter 4760: loss 4.3311, time 221.18ms, mfu 12.84%\n",
            "iter 4770: loss 4.3438, time 220.85ms, mfu 12.85%\n",
            "iter 4780: loss 4.2794, time 220.53ms, mfu 12.86%\n",
            "iter 4790: loss 4.3474, time 221.04ms, mfu 12.86%\n",
            "step 4800: train loss 4.3426, val loss 4.3260\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 4800: loss 4.2514, time 36163.30ms, mfu 11.58%\n",
            "iter 4810: loss 4.3674, time 220.87ms, mfu 11.72%\n",
            "iter 4820: loss 4.4180, time 221.04ms, mfu 11.84%\n",
            "iter 4830: loss 4.2479, time 220.63ms, mfu 11.95%\n",
            "iter 4840: loss 4.2178, time 220.31ms, mfu 12.05%\n",
            "iter 4850: loss 4.4183, time 220.62ms, mfu 12.13%\n",
            "iter 4860: loss 4.3598, time 220.41ms, mfu 12.22%\n",
            "iter 4870: loss 4.3519, time 220.59ms, mfu 12.29%\n",
            "iter 4880: loss 4.4402, time 220.89ms, mfu 12.35%\n",
            "iter 4890: loss 4.3037, time 220.32ms, mfu 12.41%\n",
            "iter 4900: loss 4.4625, time 220.81ms, mfu 12.46%\n",
            "iter 4910: loss 4.3249, time 220.22ms, mfu 12.51%\n",
            "iter 4920: loss 4.3152, time 220.27ms, mfu 12.55%\n",
            "iter 4930: loss 4.4153, time 220.64ms, mfu 12.59%\n",
            "iter 4940: loss 4.3561, time 220.27ms, mfu 12.63%\n",
            "iter 4950: loss 4.3013, time 220.95ms, mfu 12.66%\n",
            "iter 4960: loss 4.4164, time 220.81ms, mfu 12.68%\n",
            "iter 4970: loss 4.3341, time 220.41ms, mfu 12.71%\n",
            "iter 4980: loss 4.2367, time 220.35ms, mfu 12.73%\n",
            "iter 4990: loss 4.3413, time 220.95ms, mfu 12.75%\n",
            "iter 5000: loss 4.3785, time 220.76ms, mfu 12.77%\n",
            "iter 5010: loss 4.3823, time 220.57ms, mfu 12.78%\n",
            "iter 5020: loss 4.2890, time 220.63ms, mfu 12.80%\n",
            "iter 5030: loss 4.3300, time 220.92ms, mfu 12.81%\n",
            "iter 5040: loss 4.2562, time 220.69ms, mfu 12.82%\n",
            "iter 5050: loss 4.2426, time 221.02ms, mfu 12.83%\n",
            "iter 5060: loss 4.3646, time 219.74ms, mfu 12.85%\n",
            "iter 5070: loss 4.3606, time 220.58ms, mfu 12.86%\n",
            "iter 5080: loss 4.2901, time 220.44ms, mfu 12.86%\n",
            "iter 5090: loss 4.4212, time 220.73ms, mfu 12.87%\n",
            "step 5100: train loss 4.3480, val loss 4.3333\n",
            "iter 5100: loss 4.1510, time 34079.21ms, mfu 11.59%\n",
            "iter 5110: loss 4.3480, time 221.07ms, mfu 11.72%\n",
            "iter 5120: loss 4.4110, time 220.89ms, mfu 11.84%\n",
            "iter 5130: loss 4.2527, time 220.69ms, mfu 11.95%\n",
            "iter 5140: loss 4.3734, time 221.85ms, mfu 12.04%\n",
            "iter 5150: loss 4.3191, time 220.23ms, mfu 12.13%\n",
            "iter 5160: loss 4.3218, time 220.53ms, mfu 12.21%\n",
            "iter 5170: loss 4.1229, time 220.55ms, mfu 12.29%\n",
            "iter 5180: loss 4.2810, time 220.54ms, mfu 12.35%\n",
            "iter 5190: loss 4.2508, time 220.96ms, mfu 12.41%\n",
            "iter 5200: loss 4.2870, time 220.66ms, mfu 12.46%\n",
            "iter 5210: loss 4.3968, time 220.61ms, mfu 12.51%\n",
            "iter 5220: loss 4.5975, time 220.70ms, mfu 12.55%\n",
            "iter 5230: loss 4.2755, time 220.52ms, mfu 12.59%\n",
            "iter 5240: loss 4.4327, time 219.97ms, mfu 12.63%\n",
            "iter 5250: loss 4.3363, time 220.89ms, mfu 12.65%\n",
            "iter 5260: loss 4.3558, time 220.55ms, mfu 12.68%\n",
            "iter 5270: loss 4.3349, time 220.43ms, mfu 12.71%\n",
            "iter 5280: loss 4.3352, time 220.15ms, mfu 12.73%\n",
            "iter 5290: loss 4.4524, time 221.10ms, mfu 12.75%\n",
            "iter 5300: loss 4.3569, time 221.24ms, mfu 12.77%\n",
            "iter 5310: loss 4.3401, time 220.90ms, mfu 12.78%\n",
            "iter 5320: loss 4.3531, time 220.57ms, mfu 12.80%\n",
            "iter 5330: loss 4.3935, time 220.68ms, mfu 12.81%\n",
            "iter 5340: loss 4.2858, time 230.97ms, mfu 12.76%\n",
            "iter 5350: loss 4.3505, time 220.31ms, mfu 12.78%\n",
            "iter 5360: loss 4.3819, time 220.59ms, mfu 12.80%\n",
            "iter 5370: loss 4.2385, time 220.99ms, mfu 12.81%\n",
            "iter 5380: loss 4.5095, time 220.14ms, mfu 12.82%\n",
            "iter 5390: loss 4.1868, time 221.12ms, mfu 12.83%\n",
            "step 5400: train loss 4.3460, val loss 4.3377\n",
            "iter 5400: loss 4.2934, time 34010.73ms, mfu 11.56%\n",
            "iter 5410: loss 4.3040, time 220.53ms, mfu 11.69%\n",
            "iter 5420: loss 4.1688, time 220.71ms, mfu 11.82%\n",
            "iter 5430: loss 4.3828, time 220.75ms, mfu 11.93%\n",
            "iter 5440: loss 4.3947, time 220.77ms, mfu 12.03%\n",
            "iter 5450: loss 4.2156, time 221.17ms, mfu 12.11%\n",
            "iter 5460: loss 4.2330, time 220.68ms, mfu 12.20%\n",
            "iter 5470: loss 4.3700, time 221.07ms, mfu 12.27%\n",
            "iter 5480: loss 4.4334, time 220.51ms, mfu 12.33%\n",
            "iter 5490: loss 4.3319, time 220.49ms, mfu 12.39%\n",
            "iter 5500: loss 4.4180, time 220.59ms, mfu 12.45%\n",
            "iter 5510: loss 4.5528, time 220.55ms, mfu 12.50%\n",
            "iter 5520: loss 4.3392, time 220.85ms, mfu 12.54%\n",
            "iter 5530: loss 4.3829, time 220.50ms, mfu 12.58%\n",
            "iter 5540: loss 4.4449, time 220.64ms, mfu 12.61%\n",
            "iter 5550: loss 4.3739, time 220.34ms, mfu 12.65%\n",
            "iter 5560: loss 4.1188, time 221.33ms, mfu 12.67%\n",
            "iter 5570: loss 4.4302, time 220.86ms, mfu 12.70%\n",
            "iter 5580: loss 4.4306, time 220.46ms, mfu 12.72%\n",
            "iter 5590: loss 4.1582, time 221.25ms, mfu 12.74%\n",
            "iter 5600: loss 4.2668, time 221.35ms, mfu 12.75%\n",
            "iter 5610: loss 4.3673, time 220.75ms, mfu 12.77%\n",
            "iter 5620: loss 4.4742, time 220.68ms, mfu 12.79%\n",
            "iter 5630: loss 4.2676, time 220.63ms, mfu 12.80%\n",
            "iter 5640: loss 4.3318, time 220.85ms, mfu 12.81%\n",
            "iter 5650: loss 4.3621, time 220.48ms, mfu 12.83%\n",
            "iter 5660: loss 4.3118, time 220.45ms, mfu 12.84%\n",
            "iter 5670: loss 4.3495, time 220.54ms, mfu 12.85%\n",
            "iter 5680: loss 4.4273, time 220.62ms, mfu 12.86%\n",
            "iter 5690: loss 4.2570, time 221.20ms, mfu 12.86%\n",
            "step 5700: train loss 4.3415, val loss 4.3204\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 5700: loss 4.3059, time 38160.63ms, mfu 11.58%\n",
            "iter 5710: loss 4.2012, time 220.75ms, mfu 11.72%\n",
            "iter 5720: loss 4.2460, time 220.80ms, mfu 11.84%\n",
            "iter 5730: loss 4.3605, time 221.19ms, mfu 11.94%\n",
            "iter 5740: loss 4.3194, time 219.97ms, mfu 12.04%\n",
            "iter 5750: loss 4.3402, time 220.03ms, mfu 12.14%\n",
            "iter 5760: loss 4.5030, time 220.38ms, mfu 12.22%\n",
            "iter 5770: loss 4.4402, time 221.46ms, mfu 12.28%\n",
            "iter 5780: loss 4.3372, time 220.62ms, mfu 12.35%\n",
            "iter 5790: loss 4.3553, time 220.93ms, mfu 12.41%\n",
            "iter 5800: loss 4.3546, time 220.72ms, mfu 12.46%\n",
            "iter 5810: loss 4.3352, time 220.60ms, mfu 12.51%\n",
            "iter 5820: loss 4.4391, time 221.17ms, mfu 12.54%\n",
            "iter 5830: loss 4.0496, time 220.62ms, mfu 12.58%\n",
            "iter 5840: loss 4.3694, time 220.19ms, mfu 12.62%\n",
            "iter 5850: loss 4.4084, time 196.45ms, mfu 12.81%\n",
            "iter 5860: loss 4.3871, time 220.16ms, mfu 12.83%\n",
            "iter 5870: loss 4.2736, time 219.94ms, mfu 12.84%\n",
            "iter 5880: loss 4.3566, time 220.74ms, mfu 12.85%\n",
            "iter 5890: loss 4.3560, time 221.03ms, mfu 12.85%\n",
            "iter 5900: loss 4.4495, time 221.31ms, mfu 12.86%\n",
            "iter 5910: loss 4.3555, time 220.49ms, mfu 12.87%\n",
            "iter 5920: loss 4.2840, time 220.64ms, mfu 12.87%\n",
            "iter 5930: loss 4.4757, time 220.93ms, mfu 12.88%\n",
            "iter 5940: loss 4.3972, time 220.94ms, mfu 12.88%\n",
            "iter 5950: loss 4.2722, time 220.39ms, mfu 12.89%\n",
            "iter 5960: loss 4.2900, time 220.32ms, mfu 12.89%\n",
            "iter 5970: loss 4.3054, time 220.54ms, mfu 12.90%\n",
            "iter 5980: loss 4.4088, time 220.85ms, mfu 12.90%\n",
            "iter 5990: loss 4.3013, time 220.66ms, mfu 12.90%\n",
            "step 6000: train loss 4.3266, val loss 4.3236\n",
            "iter 6000: loss 4.3450, time 34053.48ms, mfu 11.62%\n",
            "iter 6010: loss 4.2544, time 220.37ms, mfu 11.75%\n",
            "iter 6020: loss 4.2999, time 220.32ms, mfu 11.87%\n",
            "iter 6030: loss 4.4096, time 220.21ms, mfu 11.98%\n",
            "iter 6040: loss 4.4081, time 221.37ms, mfu 12.07%\n",
            "iter 6050: loss 4.4066, time 220.61ms, mfu 12.16%\n",
            "iter 6060: loss 4.3485, time 220.10ms, mfu 12.24%\n",
            "iter 6070: loss 4.3281, time 220.49ms, mfu 12.31%\n",
            "iter 6080: loss 4.2455, time 221.18ms, mfu 12.37%\n",
            "iter 6090: loss 4.2653, time 220.31ms, mfu 12.43%\n",
            "iter 6100: loss 4.4417, time 220.59ms, mfu 12.48%\n",
            "iter 6110: loss 4.4017, time 221.46ms, mfu 12.52%\n",
            "iter 6120: loss 4.4567, time 220.88ms, mfu 12.56%\n",
            "iter 6130: loss 4.4192, time 220.79ms, mfu 12.59%\n",
            "iter 6140: loss 4.2162, time 219.96ms, mfu 12.63%\n",
            "iter 6150: loss 4.2514, time 219.88ms, mfu 12.67%\n",
            "iter 6160: loss 4.3508, time 221.03ms, mfu 12.69%\n",
            "iter 6170: loss 4.4891, time 221.06ms, mfu 12.71%\n",
            "iter 6180: loss 4.4475, time 220.52ms, mfu 12.73%\n",
            "iter 6190: loss 4.2874, time 220.55ms, mfu 12.75%\n",
            "iter 6200: loss 4.3947, time 220.86ms, mfu 12.77%\n",
            "iter 6210: loss 4.2313, time 220.72ms, mfu 12.79%\n",
            "iter 6220: loss 4.4159, time 220.61ms, mfu 12.80%\n",
            "iter 6230: loss 4.3133, time 220.67ms, mfu 12.81%\n",
            "iter 6240: loss 4.3433, time 220.86ms, mfu 12.82%\n",
            "iter 6250: loss 4.4765, time 221.06ms, mfu 12.83%\n",
            "iter 6260: loss 4.3533, time 220.41ms, mfu 12.84%\n",
            "iter 6270: loss 4.2914, time 221.05ms, mfu 12.85%\n",
            "iter 6280: loss 4.4439, time 220.90ms, mfu 12.86%\n",
            "iter 6290: loss 4.3171, time 220.66ms, mfu 12.86%\n",
            "step 6300: train loss 4.3285, val loss 4.3149\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 6300: loss 4.1875, time 36127.09ms, mfu 11.58%\n",
            "iter 6310: loss 4.3457, time 220.45ms, mfu 11.72%\n",
            "iter 6320: loss 4.3181, time 221.04ms, mfu 11.84%\n",
            "iter 6330: loss 4.3224, time 220.59ms, mfu 11.95%\n",
            "iter 6340: loss 4.4019, time 220.87ms, mfu 12.05%\n",
            "iter 6350: loss 4.3630, time 220.70ms, mfu 12.13%\n",
            "iter 6360: loss 4.5109, time 220.86ms, mfu 12.21%\n",
            "iter 6370: loss 4.2851, time 219.84ms, mfu 12.29%\n",
            "iter 6380: loss 4.3126, time 220.42ms, mfu 12.35%\n",
            "iter 6390: loss 4.2441, time 220.16ms, mfu 12.41%\n",
            "iter 6400: loss 4.3193, time 220.44ms, mfu 12.47%\n",
            "iter 6410: loss 4.2972, time 220.71ms, mfu 12.51%\n",
            "iter 6420: loss 4.3368, time 221.31ms, mfu 12.55%\n",
            "iter 6430: loss 4.3497, time 220.57ms, mfu 12.59%\n",
            "iter 6440: loss 4.3272, time 220.56ms, mfu 12.62%\n",
            "iter 6450: loss 4.3541, time 220.74ms, mfu 12.65%\n",
            "iter 6460: loss 4.3365, time 221.06ms, mfu 12.68%\n",
            "iter 6470: loss 4.3918, time 220.99ms, mfu 12.70%\n",
            "iter 6480: loss 4.2659, time 219.81ms, mfu 12.73%\n",
            "iter 6490: loss 4.1836, time 220.35ms, mfu 12.75%\n",
            "iter 6500: loss 4.3655, time 220.62ms, mfu 12.77%\n",
            "iter 6510: loss 4.3807, time 220.76ms, mfu 12.79%\n",
            "iter 6520: loss 4.3389, time 220.52ms, mfu 12.80%\n",
            "iter 6530: loss 4.1967, time 220.82ms, mfu 12.81%\n",
            "iter 6540: loss 4.1984, time 220.00ms, mfu 12.83%\n",
            "iter 6550: loss 4.2247, time 220.63ms, mfu 12.84%\n",
            "iter 6560: loss 4.3822, time 221.01ms, mfu 12.85%\n",
            "iter 6570: loss 4.1362, time 219.97ms, mfu 12.86%\n",
            "iter 6580: loss 4.1808, time 220.89ms, mfu 12.86%\n",
            "iter 6590: loss 4.3369, time 220.66ms, mfu 12.87%\n",
            "step 6600: train loss 4.3253, val loss 4.3227\n",
            "iter 6600: loss 4.5006, time 34045.87ms, mfu 11.59%\n",
            "iter 6610: loss 4.3758, time 221.00ms, mfu 11.72%\n",
            "iter 6620: loss 4.1791, time 221.20ms, mfu 11.84%\n",
            "iter 6630: loss 4.3052, time 220.38ms, mfu 11.95%\n",
            "iter 6640: loss 4.2558, time 220.75ms, mfu 12.05%\n",
            "iter 6650: loss 4.4453, time 220.59ms, mfu 12.14%\n",
            "iter 6660: loss 4.4037, time 220.63ms, mfu 12.22%\n",
            "iter 6670: loss 4.2965, time 221.57ms, mfu 12.28%\n",
            "iter 6680: loss 4.3680, time 220.76ms, mfu 12.35%\n",
            "iter 6690: loss 4.2010, time 220.57ms, mfu 12.41%\n",
            "iter 6700: loss 4.2338, time 220.84ms, mfu 12.46%\n",
            "iter 6710: loss 4.0015, time 220.88ms, mfu 12.50%\n",
            "iter 6720: loss 4.4486, time 220.73ms, mfu 12.54%\n",
            "iter 6730: loss 4.3994, time 220.83ms, mfu 12.58%\n",
            "iter 6740: loss 4.1754, time 220.54ms, mfu 12.62%\n",
            "iter 6750: loss 4.3229, time 221.00ms, mfu 12.65%\n",
            "iter 6760: loss 4.3628, time 221.22ms, mfu 12.67%\n",
            "iter 6770: loss 4.2624, time 220.77ms, mfu 12.70%\n",
            "iter 6780: loss 4.3346, time 220.04ms, mfu 12.72%\n",
            "iter 6790: loss 4.3045, time 220.50ms, mfu 12.75%\n",
            "iter 6800: loss 4.2770, time 220.92ms, mfu 12.76%\n",
            "iter 6810: loss 4.3740, time 221.07ms, mfu 12.78%\n",
            "iter 6820: loss 4.3238, time 220.66ms, mfu 12.79%\n",
            "iter 6830: loss 4.3731, time 220.64ms, mfu 12.81%\n",
            "iter 6840: loss 4.3391, time 220.91ms, mfu 12.82%\n",
            "iter 6850: loss 4.2612, time 220.94ms, mfu 12.83%\n",
            "iter 6860: loss 4.3675, time 221.16ms, mfu 12.83%\n",
            "iter 6870: loss 4.3699, time 220.33ms, mfu 12.85%\n",
            "iter 6880: loss 4.3329, time 220.35ms, mfu 12.86%\n",
            "iter 6890: loss 4.4083, time 221.17ms, mfu 12.86%\n",
            "step 6900: train loss 4.3404, val loss 4.3164\n",
            "iter 6900: loss 4.2456, time 34017.27ms, mfu 11.58%\n",
            "iter 6910: loss 4.2085, time 220.65ms, mfu 11.72%\n",
            "iter 6920: loss 4.4619, time 220.70ms, mfu 11.84%\n",
            "iter 6930: loss 4.4461, time 220.87ms, mfu 11.95%\n",
            "iter 6940: loss 4.3941, time 220.64ms, mfu 12.04%\n",
            "iter 6950: loss 4.2459, time 220.67ms, mfu 12.13%\n",
            "iter 6960: loss 4.1999, time 220.99ms, mfu 12.21%\n",
            "iter 6970: loss 4.2297, time 220.60ms, mfu 12.28%\n",
            "iter 6980: loss 4.3904, time 220.34ms, mfu 12.35%\n",
            "iter 6990: loss 4.2765, time 220.97ms, mfu 12.41%\n",
            "iter 7000: loss 4.2258, time 220.75ms, mfu 12.46%\n",
            "iter 7010: loss 4.4170, time 220.76ms, mfu 12.50%\n",
            "iter 7020: loss 4.3323, time 220.19ms, mfu 12.55%\n",
            "iter 7030: loss 4.2121, time 220.25ms, mfu 12.59%\n",
            "iter 7040: loss 4.3975, time 221.09ms, mfu 12.62%\n",
            "iter 7050: loss 4.3803, time 220.98ms, mfu 12.65%\n",
            "iter 7060: loss 4.4377, time 220.73ms, mfu 12.68%\n",
            "iter 7070: loss 4.2556, time 220.95ms, mfu 12.70%\n",
            "iter 7080: loss 4.2511, time 221.18ms, mfu 12.72%\n",
            "iter 7090: loss 4.2439, time 220.62ms, mfu 12.74%\n",
            "iter 7100: loss 4.3026, time 220.25ms, mfu 12.76%\n",
            "iter 7110: loss 4.6244, time 220.92ms, mfu 12.78%\n",
            "iter 7120: loss 4.4147, time 220.79ms, mfu 12.79%\n",
            "iter 7130: loss 4.4696, time 220.82ms, mfu 12.81%\n",
            "iter 7140: loss 4.4939, time 221.24ms, mfu 12.81%\n",
            "iter 7150: loss 4.3433, time 219.99ms, mfu 12.83%\n",
            "iter 7160: loss 4.2280, time 220.73ms, mfu 12.84%\n",
            "iter 7170: loss 4.3128, time 220.85ms, mfu 12.85%\n",
            "iter 7180: loss 4.1951, time 221.46ms, mfu 12.85%\n",
            "iter 7190: loss 4.3858, time 220.75ms, mfu 12.86%\n",
            "step 7200: train loss 4.3191, val loss 4.3007\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 7200: loss 4.2656, time 36127.49ms, mfu 11.58%\n",
            "iter 7210: loss 4.3619, time 220.48ms, mfu 11.72%\n",
            "iter 7220: loss 4.3436, time 220.29ms, mfu 11.84%\n",
            "iter 7230: loss 4.2519, time 220.90ms, mfu 11.95%\n",
            "iter 7240: loss 4.1201, time 220.91ms, mfu 12.04%\n",
            "iter 7250: loss 4.5847, time 220.60ms, mfu 12.13%\n",
            "iter 7260: loss 4.3656, time 220.88ms, mfu 12.21%\n",
            "iter 7270: loss 4.2355, time 220.81ms, mfu 12.28%\n",
            "iter 7280: loss 4.1506, time 221.19ms, mfu 12.34%\n",
            "iter 7290: loss 4.4233, time 221.57ms, mfu 12.40%\n",
            "iter 7300: loss 4.4044, time 220.91ms, mfu 12.45%\n",
            "iter 7310: loss 4.2976, time 220.74ms, mfu 12.50%\n",
            "iter 7320: loss 4.1425, time 220.29ms, mfu 12.54%\n",
            "iter 7330: loss 4.3238, time 220.62ms, mfu 12.58%\n",
            "iter 7340: loss 4.3593, time 220.20ms, mfu 12.62%\n",
            "iter 7350: loss 4.2805, time 220.63ms, mfu 12.65%\n",
            "iter 7360: loss 4.4724, time 220.71ms, mfu 12.68%\n",
            "iter 7370: loss 4.2408, time 221.36ms, mfu 12.70%\n",
            "iter 7380: loss 4.4891, time 220.37ms, mfu 12.72%\n",
            "iter 7390: loss 4.3076, time 220.76ms, mfu 12.74%\n",
            "iter 7400: loss 4.0910, time 221.01ms, mfu 12.76%\n",
            "iter 7410: loss 4.3472, time 221.05ms, mfu 12.77%\n",
            "iter 7420: loss 4.1217, time 220.65ms, mfu 12.79%\n",
            "iter 7430: loss 4.3220, time 220.44ms, mfu 12.81%\n",
            "iter 7440: loss 4.1729, time 221.12ms, mfu 12.81%\n",
            "iter 7450: loss 4.3892, time 220.46ms, mfu 12.83%\n",
            "iter 7460: loss 4.4561, time 220.62ms, mfu 12.84%\n",
            "iter 7470: loss 4.2302, time 220.96ms, mfu 12.85%\n",
            "iter 7480: loss 4.2419, time 220.54ms, mfu 12.85%\n",
            "iter 7490: loss 4.4257, time 220.84ms, mfu 12.86%\n",
            "step 7500: train loss 4.3324, val loss 4.3112\n",
            "iter 7500: loss 4.4119, time 34069.65ms, mfu 11.58%\n",
            "iter 7510: loss 4.1100, time 220.78ms, mfu 11.72%\n",
            "iter 7520: loss 4.1683, time 220.75ms, mfu 11.84%\n",
            "iter 7530: loss 4.3608, time 220.35ms, mfu 11.95%\n",
            "iter 7540: loss 4.2896, time 220.67ms, mfu 12.05%\n",
            "iter 7550: loss 4.3510, time 221.14ms, mfu 12.13%\n",
            "iter 7560: loss 4.2740, time 220.99ms, mfu 12.21%\n",
            "iter 7570: loss 4.3097, time 221.09ms, mfu 12.28%\n",
            "iter 7580: loss 4.3818, time 220.85ms, mfu 12.34%\n",
            "iter 7590: loss 4.2583, time 221.22ms, mfu 12.40%\n",
            "iter 7600: loss 4.4378, time 221.19ms, mfu 12.45%\n",
            "iter 7610: loss 4.3129, time 220.04ms, mfu 12.50%\n",
            "iter 7620: loss 4.4412, time 220.70ms, mfu 12.54%\n",
            "iter 7630: loss 4.1910, time 221.18ms, mfu 12.58%\n",
            "iter 7640: loss 4.1921, time 220.91ms, mfu 12.61%\n",
            "iter 7650: loss 4.2316, time 220.84ms, mfu 12.64%\n",
            "iter 7660: loss 4.2707, time 219.92ms, mfu 12.68%\n",
            "iter 7670: loss 4.3349, time 220.26ms, mfu 12.70%\n",
            "iter 7680: loss 4.3786, time 220.94ms, mfu 12.72%\n",
            "iter 7690: loss 4.1485, time 220.76ms, mfu 12.74%\n",
            "iter 7700: loss 4.3031, time 220.24ms, mfu 12.77%\n",
            "iter 7710: loss 4.5342, time 220.37ms, mfu 12.78%\n",
            "iter 7720: loss 4.3518, time 220.97ms, mfu 12.80%\n",
            "iter 7730: loss 4.3934, time 220.30ms, mfu 12.81%\n",
            "iter 7740: loss 4.2041, time 221.69ms, mfu 12.82%\n",
            "iter 7750: loss 4.2661, time 220.23ms, mfu 12.83%\n",
            "iter 7760: loss 4.4412, time 221.19ms, mfu 12.84%\n",
            "iter 7770: loss 4.2510, time 219.81ms, mfu 12.85%\n",
            "iter 7780: loss 4.3912, time 220.52ms, mfu 12.86%\n",
            "iter 7790: loss 4.2703, time 221.07ms, mfu 12.86%\n",
            "step 7800: train loss 4.3248, val loss 4.3138\n",
            "iter 7800: loss 4.1583, time 34031.54ms, mfu 11.59%\n",
            "iter 7810: loss 4.2937, time 220.88ms, mfu 11.72%\n",
            "iter 7820: loss 4.2786, time 222.29ms, mfu 11.83%\n",
            "iter 7830: loss 4.1869, time 220.84ms, mfu 11.94%\n",
            "iter 7840: loss 4.2768, time 220.52ms, mfu 12.04%\n",
            "iter 7850: loss 4.2544, time 220.37ms, mfu 12.13%\n",
            "iter 7860: loss 4.4014, time 220.29ms, mfu 12.21%\n",
            "iter 7870: loss 4.4431, time 220.71ms, mfu 12.28%\n",
            "iter 7880: loss 4.3813, time 220.98ms, mfu 12.35%\n",
            "iter 7890: loss 4.4614, time 219.36ms, mfu 12.41%\n",
            "iter 7900: loss 4.3508, time 220.72ms, mfu 12.46%\n",
            "iter 7910: loss 4.1814, time 220.64ms, mfu 12.51%\n",
            "iter 7920: loss 4.4132, time 222.28ms, mfu 12.54%\n",
            "iter 7930: loss 4.4667, time 219.88ms, mfu 12.59%\n",
            "iter 7940: loss 4.2940, time 221.10ms, mfu 12.62%\n",
            "iter 7950: loss 4.4115, time 220.66ms, mfu 12.65%\n",
            "iter 7960: loss 4.3772, time 220.52ms, mfu 12.68%\n",
            "iter 7970: loss 4.4515, time 220.59ms, mfu 12.70%\n",
            "iter 7980: loss 4.2487, time 220.56ms, mfu 12.73%\n",
            "iter 7990: loss 4.1984, time 220.69ms, mfu 12.75%\n",
            "iter 8000: loss 4.3002, time 220.65ms, mfu 12.76%\n",
            "iter 8010: loss 4.2774, time 220.82ms, mfu 12.78%\n",
            "iter 8020: loss 4.4305, time 221.15ms, mfu 12.79%\n",
            "iter 8030: loss 4.3323, time 220.91ms, mfu 12.80%\n",
            "iter 8040: loss 4.2863, time 221.63ms, mfu 12.81%\n",
            "iter 8050: loss 4.3746, time 219.90ms, mfu 12.83%\n",
            "iter 8060: loss 4.3126, time 220.76ms, mfu 12.84%\n",
            "iter 8070: loss 4.1330, time 220.76ms, mfu 12.85%\n",
            "iter 8080: loss 4.2636, time 220.81ms, mfu 12.85%\n",
            "iter 8090: loss 4.2016, time 221.03ms, mfu 12.86%\n",
            "step 8100: train loss 4.3235, val loss 4.3021\n",
            "iter 8100: loss 4.3063, time 34050.10ms, mfu 11.58%\n",
            "iter 8110: loss 4.2697, time 221.00ms, mfu 11.71%\n",
            "iter 8120: loss 4.2141, time 220.66ms, mfu 11.84%\n",
            "iter 8130: loss 4.2646, time 219.63ms, mfu 11.95%\n",
            "iter 8140: loss 4.3619, time 220.08ms, mfu 12.05%\n",
            "iter 8150: loss 4.3615, time 220.55ms, mfu 12.14%\n",
            "iter 8160: loss 4.4479, time 220.50ms, mfu 12.22%\n",
            "iter 8170: loss 4.2159, time 220.76ms, mfu 12.29%\n",
            "iter 8180: loss 4.3721, time 221.07ms, mfu 12.35%\n",
            "iter 8190: loss 4.3241, time 220.77ms, mfu 12.41%\n",
            "iter 8200: loss 4.2468, time 220.57ms, mfu 12.46%\n",
            "iter 8210: loss 4.3309, time 220.73ms, mfu 12.51%\n",
            "iter 8220: loss 4.3342, time 220.50ms, mfu 12.55%\n",
            "iter 8230: loss 4.4752, time 220.58ms, mfu 12.59%\n",
            "iter 8240: loss 4.3962, time 220.27ms, mfu 12.63%\n",
            "iter 8250: loss 4.0868, time 220.56ms, mfu 12.66%\n",
            "iter 8260: loss 4.3435, time 220.43ms, mfu 12.69%\n",
            "iter 8270: loss 4.4351, time 221.02ms, mfu 12.71%\n",
            "iter 8280: loss 4.2375, time 220.63ms, mfu 12.73%\n",
            "iter 8290: loss 4.2895, time 220.58ms, mfu 12.75%\n",
            "iter 8300: loss 4.3885, time 220.70ms, mfu 12.77%\n",
            "iter 8310: loss 4.3110, time 220.55ms, mfu 12.78%\n",
            "iter 8320: loss 4.2980, time 220.43ms, mfu 12.80%\n",
            "iter 8330: loss 4.3511, time 220.65ms, mfu 12.81%\n",
            "iter 8340: loss 4.5540, time 220.96ms, mfu 12.82%\n",
            "iter 8350: loss 4.3070, time 220.67ms, mfu 12.83%\n",
            "iter 8360: loss 4.3036, time 220.84ms, mfu 12.84%\n",
            "iter 8370: loss 4.1725, time 221.30ms, mfu 12.85%\n",
            "iter 8380: loss 4.2874, time 221.32ms, mfu 12.85%\n",
            "iter 8390: loss 4.3160, time 220.09ms, mfu 12.86%\n",
            "step 8400: train loss 4.3232, val loss 4.2964\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 8400: loss 4.3667, time 36097.90ms, mfu 11.58%\n",
            "iter 8410: loss 4.1834, time 220.56ms, mfu 11.72%\n",
            "iter 8420: loss 4.2295, time 220.80ms, mfu 11.84%\n",
            "iter 8430: loss 4.5703, time 220.78ms, mfu 11.95%\n",
            "iter 8440: loss 4.4096, time 220.34ms, mfu 12.05%\n",
            "iter 8450: loss 4.4565, time 220.91ms, mfu 12.13%\n",
            "iter 8460: loss 4.3574, time 220.75ms, mfu 12.21%\n",
            "iter 8470: loss 4.3315, time 220.79ms, mfu 12.28%\n",
            "iter 8480: loss 4.3669, time 220.89ms, mfu 12.35%\n",
            "iter 8490: loss 4.5386, time 220.51ms, mfu 12.41%\n",
            "iter 8500: loss 4.3267, time 220.47ms, mfu 12.46%\n",
            "iter 8510: loss 4.2037, time 220.69ms, mfu 12.51%\n",
            "iter 8520: loss 4.3259, time 221.02ms, mfu 12.55%\n",
            "iter 8530: loss 4.2495, time 220.55ms, mfu 12.59%\n",
            "iter 8540: loss 4.3719, time 220.99ms, mfu 12.62%\n",
            "iter 8550: loss 4.2747, time 221.06ms, mfu 12.65%\n",
            "iter 8560: loss 4.4381, time 220.42ms, mfu 12.68%\n",
            "iter 8570: loss 4.4340, time 220.58ms, mfu 12.70%\n",
            "iter 8580: loss 4.3160, time 220.22ms, mfu 12.73%\n",
            "iter 8590: loss 4.4520, time 221.27ms, mfu 12.74%\n",
            "iter 8600: loss 4.4909, time 220.96ms, mfu 12.76%\n",
            "iter 8610: loss 4.2964, time 220.40ms, mfu 12.78%\n",
            "iter 8620: loss 4.2646, time 220.04ms, mfu 12.80%\n",
            "iter 8630: loss 4.1102, time 220.77ms, mfu 12.81%\n",
            "iter 8640: loss 4.1651, time 220.44ms, mfu 12.82%\n",
            "iter 8650: loss 4.3936, time 220.83ms, mfu 12.83%\n",
            "iter 8660: loss 4.0623, time 220.86ms, mfu 12.84%\n",
            "iter 8670: loss 4.3879, time 221.02ms, mfu 12.85%\n",
            "iter 8680: loss 4.4858, time 220.59ms, mfu 12.86%\n",
            "iter 8690: loss 4.1764, time 220.53ms, mfu 12.86%\n",
            "step 8700: train loss 4.3132, val loss 4.3009\n",
            "iter 8700: loss 4.4992, time 34008.77ms, mfu 11.59%\n",
            "iter 8710: loss 4.3189, time 221.13ms, mfu 11.72%\n",
            "iter 8720: loss 4.2956, time 220.98ms, mfu 11.84%\n",
            "iter 8730: loss 4.1973, time 220.80ms, mfu 11.95%\n",
            "iter 8740: loss 4.3681, time 231.38ms, mfu 11.98%\n",
            "iter 8750: loss 4.3498, time 220.82ms, mfu 12.08%\n",
            "iter 8760: loss 4.4089, time 220.80ms, mfu 12.16%\n",
            "iter 8770: loss 4.3402, time 220.37ms, mfu 12.24%\n",
            "iter 8780: loss 4.3601, time 220.77ms, mfu 12.31%\n",
            "iter 8790: loss 4.3349, time 221.12ms, mfu 12.37%\n",
            "iter 8800: loss 4.0618, time 220.76ms, mfu 12.42%\n",
            "iter 8810: loss 4.1927, time 221.49ms, mfu 12.47%\n",
            "iter 8820: loss 4.2908, time 220.35ms, mfu 12.52%\n",
            "iter 8830: loss 4.2938, time 220.25ms, mfu 12.56%\n",
            "iter 8840: loss 4.2989, time 220.41ms, mfu 12.60%\n",
            "iter 8850: loss 4.4264, time 220.88ms, mfu 12.63%\n",
            "iter 8860: loss 4.1908, time 220.43ms, mfu 12.66%\n",
            "iter 8870: loss 4.2674, time 220.68ms, mfu 12.69%\n",
            "iter 8880: loss 4.3528, time 220.32ms, mfu 12.71%\n",
            "iter 8890: loss 4.3781, time 220.59ms, mfu 12.74%\n",
            "iter 8900: loss 4.2726, time 220.95ms, mfu 12.75%\n",
            "iter 8910: loss 4.3470, time 220.95ms, mfu 12.77%\n",
            "iter 8920: loss 4.2281, time 220.69ms, mfu 12.79%\n",
            "iter 8930: loss 4.2844, time 220.62ms, mfu 12.80%\n",
            "iter 8940: loss 4.3642, time 220.80ms, mfu 12.81%\n",
            "iter 8950: loss 4.1773, time 220.55ms, mfu 12.82%\n",
            "iter 8960: loss 4.1579, time 220.25ms, mfu 12.84%\n",
            "iter 8970: loss 4.4245, time 220.55ms, mfu 12.85%\n",
            "iter 8980: loss 4.2865, time 220.66ms, mfu 12.86%\n",
            "iter 8990: loss 4.3267, time 220.51ms, mfu 12.86%\n",
            "step 9000: train loss 4.3208, val loss 4.2952\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9000: loss 4.3010, time 36117.34ms, mfu 11.59%\n",
            "iter 9010: loss 4.3280, time 220.65ms, mfu 11.72%\n",
            "iter 9020: loss 4.4617, time 220.40ms, mfu 11.84%\n",
            "iter 9030: loss 4.2041, time 220.36ms, mfu 11.95%\n",
            "iter 9040: loss 4.1459, time 220.32ms, mfu 12.05%\n",
            "iter 9050: loss 4.1911, time 220.73ms, mfu 12.14%\n",
            "iter 9060: loss 4.2911, time 220.36ms, mfu 12.22%\n",
            "iter 9070: loss 4.1699, time 220.33ms, mfu 12.29%\n",
            "iter 9080: loss 4.2811, time 220.12ms, mfu 12.36%\n",
            "iter 9090: loss 4.2475, time 220.38ms, mfu 12.42%\n",
            "iter 9100: loss 4.1931, time 220.65ms, mfu 12.47%\n",
            "iter 9110: loss 4.3728, time 220.48ms, mfu 12.52%\n",
            "iter 9120: loss 4.1592, time 220.45ms, mfu 12.56%\n",
            "iter 9130: loss 4.2656, time 220.58ms, mfu 12.60%\n",
            "iter 9140: loss 4.2504, time 220.62ms, mfu 12.63%\n",
            "iter 9150: loss 4.1453, time 220.88ms, mfu 12.66%\n",
            "iter 9160: loss 4.4082, time 220.74ms, mfu 12.69%\n",
            "iter 9170: loss 4.4612, time 220.40ms, mfu 12.71%\n",
            "iter 9180: loss 4.3553, time 221.16ms, mfu 12.73%\n",
            "iter 9190: loss 4.2298, time 220.63ms, mfu 12.75%\n",
            "iter 9200: loss 4.1119, time 220.24ms, mfu 12.77%\n",
            "iter 9210: loss 4.3709, time 220.55ms, mfu 12.79%\n",
            "iter 9220: loss 4.3572, time 222.18ms, mfu 12.79%\n",
            "iter 9230: loss 4.4060, time 220.24ms, mfu 12.81%\n",
            "iter 9240: loss 4.4092, time 220.24ms, mfu 12.82%\n",
            "iter 9250: loss 4.4430, time 220.82ms, mfu 12.83%\n",
            "iter 9260: loss 4.3242, time 220.11ms, mfu 12.85%\n",
            "iter 9270: loss 4.2862, time 220.98ms, mfu 12.85%\n",
            "iter 9280: loss 4.2161, time 221.05ms, mfu 12.86%\n",
            "iter 9290: loss 4.2084, time 220.66ms, mfu 12.86%\n",
            "step 9300: train loss 4.3079, val loss 4.3066\n",
            "iter 9300: loss 4.4859, time 34078.15ms, mfu 11.59%\n",
            "iter 9310: loss 4.2418, time 220.84ms, mfu 11.72%\n",
            "iter 9320: loss 4.2741, time 220.63ms, mfu 11.84%\n",
            "iter 9330: loss 4.4601, time 220.68ms, mfu 11.95%\n",
            "iter 9340: loss 4.2824, time 220.72ms, mfu 12.05%\n",
            "iter 9350: loss 4.1882, time 220.46ms, mfu 12.14%\n",
            "iter 9360: loss 4.1085, time 220.64ms, mfu 12.22%\n",
            "iter 9370: loss 4.3603, time 220.60ms, mfu 12.29%\n",
            "iter 9380: loss 4.3009, time 220.54ms, mfu 12.35%\n",
            "iter 9390: loss 4.2828, time 221.24ms, mfu 12.41%\n",
            "iter 9400: loss 4.4505, time 221.16ms, mfu 12.46%\n",
            "iter 9410: loss 4.1841, time 220.65ms, mfu 12.50%\n",
            "iter 9420: loss 4.3856, time 220.60ms, mfu 12.55%\n",
            "iter 9430: loss 4.2666, time 221.25ms, mfu 12.58%\n",
            "iter 9440: loss 4.4170, time 220.86ms, mfu 12.61%\n",
            "iter 9450: loss 4.3058, time 220.10ms, mfu 12.65%\n",
            "iter 9460: loss 4.3725, time 220.85ms, mfu 12.68%\n",
            "iter 9470: loss 4.2395, time 220.58ms, mfu 12.70%\n",
            "iter 9480: loss 4.5089, time 220.60ms, mfu 12.73%\n",
            "iter 9490: loss 4.0714, time 220.78ms, mfu 12.74%\n",
            "iter 9500: loss 4.3819, time 220.75ms, mfu 12.76%\n",
            "iter 9510: loss 4.3032, time 220.92ms, mfu 12.78%\n",
            "iter 9520: loss 4.1122, time 220.58ms, mfu 12.79%\n",
            "iter 9530: loss 4.3492, time 220.43ms, mfu 12.81%\n",
            "iter 9540: loss 4.3493, time 220.06ms, mfu 12.82%\n",
            "iter 9550: loss 4.3187, time 221.06ms, mfu 12.83%\n",
            "iter 9560: loss 4.2536, time 220.31ms, mfu 12.84%\n",
            "iter 9570: loss 4.2533, time 221.15ms, mfu 12.85%\n",
            "iter 9580: loss 4.3547, time 220.20ms, mfu 12.86%\n",
            "iter 9590: loss 4.3773, time 220.63ms, mfu 12.87%\n",
            "step 9600: train loss 4.3086, val loss 4.3046\n",
            "iter 9600: loss 4.4635, time 34035.55ms, mfu 11.59%\n",
            "iter 9610: loss 4.4299, time 219.83ms, mfu 11.73%\n",
            "iter 9620: loss 4.2226, time 220.35ms, mfu 11.85%\n",
            "iter 9630: loss 4.1720, time 220.31ms, mfu 11.96%\n",
            "iter 9640: loss 4.2288, time 220.94ms, mfu 12.06%\n",
            "iter 9650: loss 4.1927, time 220.38ms, mfu 12.14%\n",
            "iter 9660: loss 4.1811, time 221.28ms, mfu 12.22%\n",
            "iter 9670: loss 4.1105, time 220.57ms, mfu 12.29%\n",
            "iter 9680: loss 4.3596, time 220.53ms, mfu 12.36%\n",
            "iter 9690: loss 4.2967, time 221.17ms, mfu 12.41%\n",
            "iter 9700: loss 4.1678, time 221.65ms, mfu 12.46%\n",
            "iter 9710: loss 4.2408, time 220.17ms, mfu 12.51%\n",
            "iter 9720: loss 4.1735, time 221.40ms, mfu 12.54%\n",
            "iter 9730: loss 4.2517, time 221.08ms, mfu 12.58%\n",
            "iter 9740: loss 4.0549, time 221.03ms, mfu 12.61%\n",
            "iter 9750: loss 4.2435, time 221.06ms, mfu 12.64%\n",
            "iter 9760: loss 4.3701, time 221.25ms, mfu 12.67%\n",
            "iter 9770: loss 4.4534, time 220.60ms, mfu 12.69%\n",
            "iter 9780: loss 4.5186, time 220.55ms, mfu 12.72%\n",
            "iter 9790: loss 4.3156, time 221.88ms, mfu 12.73%\n",
            "iter 9800: loss 4.2634, time 220.79ms, mfu 12.75%\n",
            "iter 9810: loss 4.2863, time 220.39ms, mfu 12.77%\n",
            "iter 9820: loss 4.3376, time 220.41ms, mfu 12.79%\n",
            "iter 9830: loss 4.3527, time 220.88ms, mfu 12.80%\n",
            "iter 9840: loss 4.2651, time 221.03ms, mfu 12.81%\n",
            "iter 9850: loss 4.3138, time 220.97ms, mfu 12.82%\n",
            "iter 9860: loss 4.3908, time 220.46ms, mfu 12.83%\n",
            "iter 9870: loss 4.3591, time 220.60ms, mfu 12.84%\n",
            "iter 9880: loss 4.0543, time 220.75ms, mfu 12.85%\n",
            "iter 9890: loss 4.1527, time 220.37ms, mfu 12.86%\n",
            "step 9900: train loss 4.3163, val loss 4.2899\n",
            "saving checkpoint to /content/reglex_attn_GPT\n",
            "iter 9900: loss 4.3673, time 36182.03ms, mfu 11.58%\n",
            "iter 9910: loss 4.4540, time 220.97ms, mfu 11.72%\n",
            "iter 9920: loss 4.3747, time 220.71ms, mfu 11.84%\n",
            "iter 9930: loss 4.3290, time 221.02ms, mfu 11.94%\n",
            "iter 9940: loss 4.2498, time 220.46ms, mfu 12.04%\n",
            "iter 9950: loss 4.2159, time 220.47ms, mfu 12.13%\n",
            "iter 9960: loss 4.4071, time 220.69ms, mfu 12.21%\n",
            "iter 9970: loss 4.2023, time 221.13ms, mfu 12.28%\n",
            "iter 9980: loss 4.2526, time 220.79ms, mfu 12.35%\n",
            "iter 9990: loss 4.2048, time 220.74ms, mfu 12.40%\n",
            "iter 10000: loss 4.3408, time 220.91ms, mfu 12.45%\n"
          ]
        }
      ],
      "source": [
        "# compile the model\n",
        "if compile:\n",
        "    print(\"compiling the model... (takes a ~minute)\")\n",
        "    unoptimized_model = model\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "\n",
        "# wrap model into DDP container\n",
        "if ddp:\n",
        "    model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# learning rate decay scheduler (cosine with warmup)\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)\n",
        "\n",
        "# logging\n",
        "if wandb_log and master_process:\n",
        "    import wandb\n",
        "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n",
        "# training loop\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "X, Y = get_batch('train') # fetch the very first batch\n",
        "t0 = time.time()\n",
        "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
        "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
        "running_mfu = -1.0\n",
        "while True:\n",
        "\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluate the loss on train/val sets and write checkpoints\n",
        "    if iter_num % eval_interval == 0 and master_process:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if wandb_log:\n",
        "            wandb.log({\n",
        "                \"iter\": iter_num,\n",
        "                \"train/loss\": losses['train'],\n",
        "                \"val/loss\": losses['val'],\n",
        "                \"lr\": lr,\n",
        "                \"mfu\": running_mfu*100, # convert to percentage\n",
        "            })\n",
        "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                checkpoint = {\n",
        "                    'model': raw_model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': iter_num,\n",
        "                    'best_val_loss': best_val_loss,\n",
        "                    'config': config,\n",
        "                }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "    if iter_num == 0 and eval_only:\n",
        "        break\n",
        "\n",
        "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
        "    # and using the GradScaler if data type is float16\n",
        "    for micro_step in range(gradient_accumulation_steps):\n",
        "        if ddp:\n",
        "            # in DDP training we only need to sync gradients at the last micro step.\n",
        "            # the official way to do this is with model.no_sync() context manager, but\n",
        "            # I really dislike that this bloats the code and forces us to repeat code\n",
        "            # looking at the source of that context manager, it just toggles this variable\n",
        "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
        "        with ctx:\n",
        "            logits, loss = model(X, Y)\n",
        "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
        "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
        "        X, Y = get_batch('train')\n",
        "        # backward pass, with gradient scaling if training in fp16\n",
        "        scaler.scale(loss).backward()\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    # flush the gradients as soon as we can, no need for this memory anymore\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0 and master_process:\n",
        "        lossf = loss.item() * gradient_accumulation_steps\n",
        "        if local_iter_num >= 5: # let the training loop settle a bit\n",
        "            mfu = raw_model.estimate_mfu(config.batch_size * gradient_accumulation_steps, dt)\n",
        "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # termination conditions\n",
        "    if iter_num > max_iters:\n",
        "        break\n",
        "\n",
        "if ddp:\n",
        "    destroy_process_group()\n",
        "\n",
        "#/content/train.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq-9EHt44ROP"
      },
      "outputs": [],
      "source": [
        "# start = \"How to join a tbank-research?\"\n",
        "# start = \"How to join a tbank-research?\"\n",
        "start = \"It's snow.\"\n",
        "num_samples = 5 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 100 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0afa38-c866-4388-fd36-0c3e15a4e591",
        "id": "Y6XkVsq54ROP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-9f0241acb2d4>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 81.11M\n"
          ]
        }
      ],
      "source": [
        "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: enc.decode(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d7e884-45dc-4c56-8750-c385de2baea9",
        "id": "KvhdB-E44ROP"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It's snow.\n",
            "\n",
            "One of the most important things we need for so many people in the world are in general.\n",
            "\n",
            "\"An area is not just a lot more severe, but it's not coming off for two years,\" said Matt Choeche, a fellow local who was playing games in the World of Australia at the University of England. \"We believe it is going to be different from the world.\"\n",
            "\n",
            "\"One of the things you've been with is growing in Australia right now. Many of the people who are in Europe say they're in Europe who are in Europe more closely.\n",
            "\n",
            "\"These are probably the most dangerous areas of Europe because of Australia and Spain.\"\n",
            "\n",
            "It was the most common case now.\n",
            "\n",
            "\"The most common cases for all races are in Europe,\" said Choeche.\n",
            "\n",
            "\"The difference between them, it's not like, but the British has a lot more to say.<|endoftext|>A man who lived in the U.S. military in the U.S. Air Force was targeted for a job in Afghanistan is the latest incident involving some 40,000 U.S. air strikes, according to the Pentagon, which saw a surge of troops.\n",
            "\n",
            "The war of Afghanistan did not come the day when the Taliban fell in their military vehicles.\n",
            "\n",
            "The attack on the ground was the result of the Bush administration. And the US intervention had gone into effect on the Islamic State.\n",
            "\n",
            "\"The problem of the Iraq war on the ground has been, in fact, there is no time for the U.S.-led coalition,\" said Mark Aufovin, who has worked for the Taliban.\n",
            "\n",
            "When a military presence were made, it was supposed to be — the answer to the question about whether the militants were going to have been going to be on the ground in Afghanistan.\n",
            "\n",
            "\"It's not happening now, but it's going to be a war of terrorism,\" said Aufovin. \"Now it's not in a war of terrorism.\"\n",
            "\n",
            "Story continues below advertisement\n",
            "\n",
            "The U.S. military has since begun the war.\n",
            "\n",
            "But the Taliban will be unable to meet that, but they will be able to deal with their own hands, while on where they are to keep a base with them.\n",
            "\n",
            "Two days after the U.S. air strikes in Iraq and the U.S.-led coalition, the U.S.-led coalition, U\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"We have a way of the game, at a certain rate, and we cannot find another place we're going to start there. We've been in a lifetime, so we could do what we did to do. We don't have a very strong team to have a great ability to create. The game is a very strong, dynamic and important. We're going to have a great chance of just what the game doesn't happen with it is to be and we're going to be and we're going to have a stronger team on the battlefield. We will be playing with the best players in the world today.\n",
            "\n",
            "\"It must be a very good team. It's great and there's a great chance we're going to have to be here before it's we can win that. We want to fight again. We have to go out there with the best guys in the world and then we will make it that year.\n",
            "\n",
            "\"We have a great chance. We have a great chance if we want it and we'll be playing against them. We've got a good chance we'll definitely play against it, but we'll have to do it. We'll do the best. We'll have to do it. We'll do it. We'll win that. It will be a great chance for now. Our coach and we've made the best we can do it.<|endoftext|>The President has officially released the “1” piece, but the president has decided to release a statement made by Obama in January he discussed his position of the president’s speech.\n",
            "\n",
            "“At the end of the day, the president has done it,” he said in a tweet.\n",
            "\n",
            "“It’s a big step forward,” he added. “We believe that this is a good thing. And I’m certainly not going to be in the position of President George W. Bush. But the president is back in December.”\n",
            "\n",
            "He added that Trump would not comment on the president’s speech last week when he made the statement.\n",
            "\n",
            "“The president has made many of the people in the United States. The president is the only one who can take the opportunity to come forward and work on something and grow,” he said.\n",
            "\n",
            "Trump has also repeatedly expressed his interest in understanding how to deal, and his approach is not just about what he looks like.\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "\"It's not going to have to go back to the water, but there's a lot of good folks in the world. So we've got to make the money. We've got to have a lot of money,\" he said in an interview.\n",
            "\n",
            "\"If you've been, that guy will make the money, but it's a good thing -- they're so good.\n",
            "\n",
            "\"It's a big thing to do. We've got to pay their bills and they've been pushing this up and the money is going to take care of it very well.\n",
            "\n",
            "\"It's a really tough situation.\"\n",
            "\n",
            "\"We've got to have a fair and hard trade. That's a big thing.\n",
            "\n",
            "\"We've got to get this as long as we are.\n",
            "\n",
            "\"We've got to have a tax. And we don't lose the money.\n",
            "\n",
            "\" So we've got to make the money. We need to deal with this. My job was to get involved.\n",
            "\n",
            "\"So I've got to get it right I've gotten to the debt. We've got to get it right.\"\n",
            "\n",
            "He and his family are all at the time.\n",
            "\n",
            "\"I know the pain of my own.\"\n",
            "\n",
            "With my father, I've got to the financial assistance there to support the economy.\n",
            "\n",
            "\"I have things like that, as long as I leave, is my job, I've got to deal with it all through the budget and the money is done.\n",
            "\n",
            "\"I want to be up to the debt.\n",
            "\n",
            "\"I want to have done. I want to work for another person, I want to have a job.\"\n",
            "\n",
            "\"I want to be here to help get a job.\"\n",
            "\n",
            "He said that he would work on the debt.\n",
            "\n",
            "He said the debt is a great deal.\n",
            "\n",
            "\"I need to have a plan to go down, come up with the debt.\"\n",
            "\n",
            "\"I want to have a job. It's not the least going to be better than that,\" he said.\n",
            "\n",
            "He said he's able to build a very large debt and I think it's going to be going to work with the debt.\"\n",
            "\n",
            "A spokesman for the Bank of England has asked the debt was a \"full time\" for the debt.\n",
            "\n",
            "\"I think it's a good thing to do.\"\n",
            "\n",
            "He said the debt is\n",
            "---------------\n",
            "It's snow.\n",
            "\n",
            "During the video, the camera took a little bit of work in the direction of the camera for its air, so this is the first time we're talking about all the time.\n",
            "\n",
            "The pilot shows off the camera that's going to be the first person to find the pilot.\n",
            "\n",
            "The video that goes through the camera, there was a \"flose\" and \"cracker.\"\n",
            "\n",
            "The video was mixed with the camera so that the body is sitting in on the rear of the camera.\n",
            "\n",
            "The video was mixed with the video, and it was still being seen outside the camera.\n",
            "\n",
            "The video ran from the video, and it was filmed on the video.\n",
            "\n",
            "When the video was released, the video was mixed with the video.\n",
            "\n",
            "The video was filled with a camera shot, and with an explosion at a gas station at the explosion.\n",
            "\n",
            "The video was released following the video shot in the video for the video.\n",
            "\n",
            "The video was carried at the scene on the video.\n",
            "\n",
            "The video was carried by the camera.\n",
            "\n",
            "The video was taken in December of 2010 by the video.\n",
            "\n",
            "The video was taken in December of 2011.\n",
            "\n",
            "The video was taken from the camera in 2009, with the video.\n",
            "\n",
            "The video was taken from the video.\n",
            "\n",
            "The video was taken from the video.<|endoftext|>A federal judge to review a lawsuit filed Monday.\n",
            "\n",
            "On Tuesday, the Court of Appeals and Criminal Court of Appeals have asked the Supreme Court to review that decision.\n",
            "\n",
            "The judge said it’s the only way the Court had to argue against that decision.\n",
            "\n",
            "This is the first piece that has been filed at the Justice Department for a judge to review the motion.\n",
            "\n",
            "“I never saw another conviction,” said the judge. “My view is that the Court is aware of the principle that the jury to review a lawsuit is reasonable, but I don’t think it’s clear the court's decision and that the judge is very important and that Judge James P. Kennedy from the Constitution shall have them put into place in the courts and that the judge has to appeal.”<|endoftext|>by and of many years of research have been told, our findings show.\n",
            "\n",
            "Most students of all ages have become increasingly difficult to see in New York, a state where they spend 20 years in the early 1970s, with the best\n",
            "---------------\n",
            "It's snow. After the snow that day, he was in the morning before noon, and he told the crowd of the sky. \"They don't really want that day, but it's just an hour later.\"\n",
            "\n",
            "The snow on the lake has moved from the foot of the river's surface. When it comes to the road, it's so much more like it.\n",
            "\n",
            "\"It's beautiful,\" said Denninsburg, who has been trying to figure out how his truck has been. \"I told him all the times that he's going to be.\"\n",
            "\n",
            "\"I actually thought it's a great chance to get back out of here. I don't see much about it, so I guess I'm just not going to see it. It's a tough way to get out of here. And I don't see it. I don't know I'm going to shoot me out of here,\" said Denninsburg, who was at the time of the storm in 2011, and they don't know.\n",
            "\n",
            "\"They have been there, but don't know how to get back on the beach so they can see it. Their fire is a big barrier,\" said Ivani, who is the one that has gone through. \"We had to wait and see what happens.\"\n",
            "\n",
            "A: They have been a very good ride there at the lake, but there is a very good ride. There are two people who have been there to make a lot of people on the road now.<|endoftext|>A large white man is standing on a grass, but it's an annual average of 400.\n",
            "\n",
            "But the man is standing in the middle of this blue line on when he walks on at the beach, and then sees the car's head.\n",
            "\n",
            "\"The fish is standing on the surface,\" the doctor told NBC News.\n",
            "\n",
            "He said it's the result, and he said the people he has to take in.\n",
            "\n",
            "\"We love to give him the impression of that and he doesn't want to give them the impression of a thing that could be used as a small white man that's a great guy.\"\n",
            "\n",
            "\"He was thinking in the eye of the night,\" the doctor said. \"He's giving up every single day and I think of your family.\"\n",
            "\n",
            "\"There's only so much pain as to why it is, it is a very good thing to do,\" the doctor said, in the paper.\n",
            "\n",
            "He\n",
            "---------------\n"
          ]
        }
      ],
      "source": [
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uplvobSg4ROQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "7YymNa9v6BOy",
        "08DYDtJEYKbt",
        "D7C6D6nX6Q-9",
        "_r1yj6Kygjwd",
        "HsfrhQkpUCYN",
        "JjURIxWLcC45",
        "_Vq0c3N68LhO",
        "rQlU0qVoAS7j",
        "zK_AV9EI4ROL"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}